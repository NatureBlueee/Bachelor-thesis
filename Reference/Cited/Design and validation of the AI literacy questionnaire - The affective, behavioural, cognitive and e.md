

## ORIGINAL ARTICLE

# Design and validation of the AI literacy questionnaire: The affective, behavioural, cognitive and ethical approach

Davy Tsz Kit Ng<sup>1</sup> <sup>b</sup> | Wenjie Wu<sup>2</sup> | Jac Ka Lok Leung<sup>3</sup> <sup>b</sup> | Thomas Kin Fung Chu<sup>2</sup> <sup>b</sup> | Samuel Kai Wah Chu<sup>4</sup> <sup>b</sup><sup>1</sup>Faculty of Education, The University of Hong Kong, Hong Kong SAR, China<sup>2</sup>Faculty of Education, The Chinese University of Hong Kong, Hong Kong SAR, China<sup>3</sup>Division of Integrative Systems and Design, The Hong Kong University of Science and Technology, Hong Kong SAR, China<sup>4</sup>School of Nursing and Health Studies, Hong Kong Metropolitan University, Hong Kong SAR, China

## Correspondence

Davy Tsz Kit Ng, Faculty of Education, The University of Hong Kong, Hong Kong SAR, China.

Email: [davyngtk@connect.hku.hk](mailto:davyngtk@connect.hku.hk)

Artificial intelligence (AI) literacy is at the top of the agenda for education today in developing learners' AI knowledge, skills, attitudes and values in the 21st century. However, there are few validated research instruments for educators to examine how secondary students develop and perceive their learning outcomes. After reviewing the literature on AI literacy questionnaires, we categorized the identified competencies in four dimensions: (1) affective learning (intrinsic motivation and self-efficacy/confidence), (2) behavioural learning (behavioural commitment and collaboration), (3) cognitive learning (know and understand; apply, evaluate and create) and (4) ethical learning. Then, a 32-item self-reported questionnaire on AI literacy (AILQ) was developed and validated to measure students' literacy development in the four dimensions. The design and validation of AILQ were examined through theoretical review, expert judgement, interview, pilot study and first- and second-order confirmatory factor analysis. This article reports the findings of a pilot study using a preliminary version of the AILQ among 363 secondary school students in Hong Kong to analyse the psychometric properties of the instrument. Results indicated a four-factor structure of the AILQ and revealed good reliability and validity. The AILQ is recommended as a reliable measurement scale for assessing how secondary students foster their AI literacy and inform better instructional

This is an open access article under the terms of the [Creative Commons Attribution-NonCommercial License](#), which permits use, distribution and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes.

© 2023 The Authors. *British Journal of Educational Technology* published by John Wiley & Sons Ltd on behalf of British Educational Research Association.

design based on the proposed affective, behavioural, cognitive and ethical (ABCE) learning framework.

## KEY WORDS

AI education, AI literacy, AI literacy questionnaire (AILQ), artificial intelligence, questionnaire validation

### Practitioner notes

What is already known about this topic

- AI literacy has drawn increasing attention in recent years and has been identified as an important digital literacy.
- Schools and universities around the world started to incorporate AI into their curriculum to foster young learners' AI literacy.
- Some studies have worked to design suitable measurement tools, especially questionnaires, to examine students' learning outcomes in AI learning programmes.

What this paper adds

- Develops an AI literacy questionnaire (AILQ) to evaluate students' literacy development in terms of affective, behavioural, cognitive and ethical (ABCE) dimensions.
- Proposes a parsimonious model based on the ABCE framework and addresses a skill set of AI literacy.

Implications for practice and/or policy

- Researchers are able to use the AILQ as a guide to measure students' AI literacy.
- Practitioners are able to use the AILQ to assess students' AI literacy development.

## INTRODUCTION

Artificial intelligence (AI) is changing the way people live, learn and work in our digital society. In education, AI literacy has drawn the attention of researchers and educators to prepare students and all citizens to live and work effectively and safely with AI (eg, Kandlhofer et al., 2016; Ng et al., 2021a, 2021b; Long & Magerko, 2020; UNESCO, 2022). As schools have started to implement curricula to foster students' AI literacy, recent scholars have developed assessment tools to understand students' learning in AI courses (eg, Carolus et al., 2023; Chai, Lin, et al., 2020; Chiu et al., 2021; Dai et al., 2020; Kong et al., 2023; Laupichler et al., 2022; Ng, Wu, et al., 2023; Pinski & Benlian 2023; Wang et al., 2022). These studies are some of the pioneers in developing test instruments for student AI learning and shed light on further research. As an emerging field of interest, a rigorous instrument to assess students' learning during their AI literacy development phase is much needed, and its validity, reliability and framework support should be carefully examined.

Some questionnaires are designed to examine students' current level of AI literacy and predict whether they can adapt to AI technologies. For example, Wang et al. (2022) introduced a 12-item scale to measure AI literacy across four dimensions (awareness, usage,

evaluation and ethics), which was validated using factor analysis, for high school learners. Carolus et al. (2023) conducted confirmatory factor analyses and created a 12-item scale to specifically measure AI readiness among adult learners through self-report assessment across four domains: 'Use and apply AI', 'Understand AI', 'Detect AI' and 'Ethics', based on the four cognitive domains proposed by Ng et al., (2021b). Laupichler et al. (2022) used an iterative Delphi expert method to develop a 47-item AI literacy scale to examine people's AI understanding, without performing factor analysis.

However, these publications on measuring instruments focus primarily on cognitive and ethical domains and do not take into account students' attitudinal and behavioural characteristics to learn AI (Carolus et al., 2023; Wang et al., 2022). Second, these questionnaires were designed for individuals of various age ranges (Chai, Wang, et al., 2020). It is crucial to develop age-appropriate questionnaires, particularly for young learners, to accurately assess their literacy development due to differences in developmental stages and experiences. Third, as questionnaires are often used to assess students' development of a particular concept or skill, curriculum design should be implemented to ensure that students have the necessary knowledge and skills to examine if the instructions are successful (Dai et al., 2020; Lin et al., 2021). This study aims to address these research gaps by taking a broader perspective and proposing that other domains be considered as important factors when developing AI literacy through an intervention programme for secondary students.

In school settings, Chai, Wang, et al. (2020) validated a 37-item questionnaire that evaluates primary students' learning outcomes throughout an AI curriculum. The questionnaire was retested to ensure reliability in other school settings (Chiu et al., 2021; Dai et al., 2020; Lin et al., 2021). Chiu et al. (2021) adopted the questionnaire by using five of its scales: (1) perceived knowledge, (2) readiness, (3) confidence, (4) relevance of learning AI and (5) intrinsic motivation to evaluate a pre-tertiary AI curriculum among 335 students. Xia et al. (2022) further modified the scale to examine secondary students' AI learning outcomes in four dimensions across gender and academic backgrounds: (1) confidence in learning AI content, (2) anxiety level towards learning AI, (3) attitude towards AI and (4) learning motivation. However, the primary focus of these studies was measuring students' learning outcomes of AI curricula, rather than measuring their literacy development. These studies viewed AI as a part of computer proficiency to develop students' basic AI knowledge, skills and attitudes, without explicitly measuring students' 'AI literacy' (Ng & Chu, 2021; Chai, Wang, et al., 2020; Dai et al., 2020).

Among these studies, only Chai, Wang, et al. (2020)'s study used the concept of 'AI literacy' as a measurement variable for scale development; it focused on technological knowledge and skill acquisition. Borrowing the idea of digital literacy, when identifying key competencies, possessing technological proficiency (eg, using digital devices smartly and knowing the knowledge behind) is arguably not the ultimate goal of digital literacy education (Audrin & Audrin, 2022; CEPS, 2015; Proulx, 1994) as there are other important competencies such as socio-emotional literacies (Ng, 2012), ethical mindsets (Covello & Lei, 2010; Segun, 2021) and other transferable skills (eg, communication and collaboration) (Audrin & Audrin, 2022; Ng, Su, et al., 2023). Similarly, knowing how to use AI technologies is different from being AI literate. Other skills such as evaluating AI technologies critically, communicating and collaborating with AI effectively are necessary for students to go beyond just becoming tech-savvy (Ng et al., 2021b; Long & Magerko, 2020).

Understanding students' learning development plays an important role in educational research in domains of students' knowledge, skills, values and behaviours for future curriculum planning (Lin et al., 2021; Wang et al., 2022). This study further examines students' other non-cognitive forms of learning that promote effective AI literacy development. To address the gap, this study adapted the ABC framework that was originally proposed by van Harreveld et al. (2015) to create an ABCE model. The inclusion of ethics as a critical

component in the model recognizes the importance of ethical considerations in the development and use of AI technologies. With the new dimension to measure students' ethical development, the ABCE model provides a comprehensive framework for examining students' wider skill set of AI literacy, and emphasizes the need for students to think critically about the ethical concerns behind AI.

There is a need for validated AI literacy questionnaires that measure a wide collection of learning outcomes associated with AI literacy for secondary school students, with careful consideration of curriculum design and a strong theoretical framework. This study contributes to proposing an AI literacy questionnaire (AILQ) for evaluating students' attitudinal learning to understand what students feel, think and do in terms of affective, behavioural, cognitive and ethical (ABCE) learning. A 55-item questionnaire was created to assess how students foster their AI literacy. A pilot study was conducted to validate this questionnaire through an AI literacy programme in a secondary school setting.

### Definition and significance of AI literacy

The term 'literacy' originally referred to basic reading and writing skills. However, modern definitions now encompass more complex learning processes. It has broadened to include competencies in different domains, such as financial, health and scientific literacy (Carolus et al., 2023). Digital literacy is an important competency that consists of various forms such as media and information literacy. These literacies focus on the ability to apply, evaluate and communicate information effectively in a technological environment (Jin et al., 2020; Law et al., 2018). As the importance of AI continues to grow, digital literacy should also evolve to include AI literacy to prepare students to become responsible digital citizens in the modern world (Carolus et al., 2023).

AI literacy has been identified as an important DL across disciplines and in different aspects of people's lives (Ng et al., 2021a, 2021b; Kandlhofer et al., 2016; Long & Magerko, 2020). The term 'AI literacy' has various conceptualizations and definitions. Kandlhofer et al. (2016) first introduced AI literacy as a set of competencies that enable individuals to know and understand AI and use AI technologies. Long and Magerko (2020) considered AI literacy as a set of competencies that enable individuals to critically evaluate AI technologies, and apply them for more effective communication and collaboration. Ng et al. (2021b) identified that AI literacy can be categorized into four concepts: (1) know and understand AI, (2) use and apply AI, (3) evaluate and create AI and (4) AI ethics. Touretzky et al. (2019) suggested the five big ideas (perception, representation and reasoning, learning, natural interaction and societal impact of AI) as the key components for K-12 students to learn AI. Zhang et al. (2023) created a secondary school curriculum aimed at developing AI literacy, which includes three components: basic AI concepts, ethical and societal implications and AI career. However, this definition does not provide high cognition about the creation of AI. A UNESCO report (2022, p. 11) conducted a review and proposed that AI literacy can enable students to 'understand how AI collects, cleans, manipulates, and analyses data; and algorithm literacy, or the ability to understand how AI algorithms find patterns and connections in the data, which might be used for human-machine interactions'.

While AI literacy has been conceptualized in various ways, most conceptualizations of AI literacy parallel Bloom's taxonomy (Ng et al., 2021b). Therefore, this study defined AI literacy by incorporating Ng et al. (2021b)'s four cognitive domains. To further develop a deeper understanding of students' attitudes towards AI and their socio-emotional competencies, this study added affective and behavioural domains to encourage collaboration, engagement and problem solving (Figure 1). Given that the taxonomy and the ABC framework serve as the foundation for various competence frameworks in educational fields, this paper is

![Figure 1: The ABCE framework. The framework is a pyramid divided into four dimensions. The base is 'Know & understand AI' (yellow). The next level is 'Use & apply AI' (gray). The top level is 'Evaluate & create AI' (orange). The right side is 'AI ethics' (blue). The four dimensions are: Affective domains (Authors, 2022e): (1) Intrinsic motivation, (2) Self-efficacy, (3) Career interest, (4) Confidence. Behavioral domains (Authors, 2022e): (1) Behavioral intention, (2) Behavioral engagement, (3) Collaboration. Cognitive domains (Authors, 2021b): (1) Intrinsic motivation, (2) Self-efficacy, (3) Career interest, (4) Confidence.](7a3561af571faf036baa93f5f4b1bdb9_img.jpg)

Figure 1: The ABCE framework. The framework is a pyramid divided into four dimensions. The base is 'Know & understand AI' (yellow). The next level is 'Use & apply AI' (gray). The top level is 'Evaluate & create AI' (orange). The right side is 'AI ethics' (blue). The four dimensions are: Affective domains (Authors, 2022e): (1) Intrinsic motivation, (2) Self-efficacy, (3) Career interest, (4) Confidence. Behavioral domains (Authors, 2022e): (1) Behavioral intention, (2) Behavioral engagement, (3) Collaboration. Cognitive domains (Authors, 2021b): (1) Intrinsic motivation, (2) Self-efficacy, (3) Career interest, (4) Confidence.

FIGURE 1 The ABCE framework.

relevant to these frameworks in establishing the ABCE dimensions of students' AI literacy development.

### Dimensionality in AI literacy scales

As digital literacy (DL) is an umbrella term for many different technologies, it has the potential to embrace AI literacy (Tinmaz et al., 2022; Wang et al., 2022). The dimensionality of AI literacy scales can be conceptualized, referenced and derived from previous DL studies. Previous DL scales varied according to their specific geographic and demographic differences; however, they could be summarized as the measurement of affective, behavioural, cognitive and ethical perspectives, which could help serve as the basis of this study (Ng, Leung, Su, Ng, et al., 2023).

Affective learning refers to students' innate emotional and physiological change towards a particular subject matter (Rogaten et al., 2019). Previous research suggests a number of measurements such as interests, confidence, motivation, attitudes and self-efficacy. For example, Guo and Goh (2016) investigated the use of a game to promote students' digital information literacy in terms of attention, confidence, satisfaction, enjoyment and perceived usefulness. These items provide a fit into the dimension of affective learning.

Behavioural learning refers to students' actions, operational performance and external behaviours that demonstrate active learning, course completion, behavioural intention, collaboration and engagement (Jung & Lee, 2018; Lowyck & Pöysä, 2001; Yue et al., 2022). Researchers have investigated various aspects to examine students' positive learning behaviours. Choi et al. (2017) administered a questionnaire to 508 students to assess students' cognitive behaviours and network agency (eg, collaboration and communication) that are supportive of their learning in a digital citizenship course.

Cognitive learning can be classified as a range from lower- to higher-order thinking skills in Bloom's taxonomy (Coşgun Ögeyik et al., 2022). Prior DL studies evaluated the students' growth, breadth and depth of knowledge. For example, Rodriguez-de-Dios et al. (2016) employed a 47-item self-report questionnaire to investigate students' knowledge and skill acquisitions such as technological skills, personal security, devices security, communication, information analysing and seeking.

Ethical learning can be considered as moral principles and techniques intended to inform the development and responsible use of technology (Celik, 2023; Liu & Yang, 2012). It is a critical knowledge and skill that is not a subset of digital knowledge (Sims, 2002; Waelen, 2022). Some studies measured ethical dimensions such as security, social responsibility, digital

rights (Kim & Han, 2019), privacy, relationships, reputation (Searson et al., 2015), responsibilities, digital awareness, ethical use of technology, respect, cyber-bullying and digital responsibility (Kim & Choi, 2018). These ethical values enable students to use technologies in a responsible manner while mitigating potential risks.

In the field of AI literacy, related work has adopted quantitative and qualitative methods to assess students' AI learning outcomes (Ng et al., 2021b). Methods applied to evaluate students' AI knowledge, skills, attitudes and values include pre- and post-knowledge tests, self-reported questionnaires (eg, Chiu et al., 2021; Kong et al., 2022; Lin et al., 2021; Ng, Wu, et al., 2023), surveys (Druga et al., 2019), curriculum guides (Ng, Leung, et al., 2023), interview tools and project rubrics (Ng, et al., 2022; Zhang et al., 2023). In particular for questionnaires, multiple variables have been considered in prior studies to assess students' AI learning outcomes, such as confidence, readiness, relevance (Chiu et al., 2021; Xia et al., 2022), behavioural intention (Chai et al., 2021), learning perceptions towards AI (Williams et al., 2022), learning motivation (Chiu et al., 2021; Ng, Leung, Su, Yim, et al., 2023; Xia et al., 2022), attitudes and career aspirations (Zhang et al., 2023) and fun, behavioural engagement, hands-on interactivity, futuristic thinking, interdisciplinary thinking (Sakulkueakulsuk et al., 2018), social good (Ng, Su, et al., 2023; Selwyn & Gallo Cordoba, 2021) and ethical learning (Kong et al., 2022; Lin & Van Brummelen, 2021).

Only a few of the studies have validated their questionnaires and even less retested them to ensure reliability in other settings (Chai et al., 2021; Chiu & Chai, 2020; Dai et al., 2020). Most of the research has measured attitudes, motivation (Ng & Chu, 2021; Xia et al., 2022) and technical proficiency, knowledge and skill (Dai et al., 2020; Lee et al., 2019; Wan et al., 2020). Few studies explored other socio-emotional and higher cognitive dimensions of AI literacy (eg, Ali 2021; Pinski & Benlian 2023). As seen from available literature, there has yet a universal consensus as to what AI literacy should encompass among K-12 learners. Given that the taxonomy and the ABC framework serve as the foundation for various competence frameworks in educational fields, this paper adapts these frameworks in establishing the ABCE dimensions of students' AI literacy development: (1) affective, (2) behavioural, (3) cognitive and (4) ethical dimensions.

## The present study

This study aims to develop and validate a questionnaire to examine students' affects, behaviours, cognitive and ethical learning during their AI literacy development processes. By adapting constructs used in the questionnaires of existing literature and mapping them to the ABCE theoretical framework, this study first developed a 32-item questionnaire as a pilot study to measure secondary students' AI learning outcomes using the ABCE framework. More specifically, this study aimed to develop and validate an AILQ on fostering students' AI literacy in lessons, assessing their ABCE learning through programme implementation with the support of an AI literacy curricula. The contribution of this study is providing researchers with a valid measurement tool (ie, AILQ) to assess students' AI literacy development at the junior secondary level.

## METHODS

### Participants and sample

In the process of developing the AILQ, three stages of validation and iterations were undertaken. First, qualitative feedback from five teachers and ten students was collected to

evaluate the face validity of the instrument. Second, we consulted three AI education researchers who were familiar with the concepts and items to be evaluated with respect to use of terms, ambiguity, problems and comprehensibility to ensure content validity. Third, we used the items and piloted the questionnaire to ensure the effectiveness of the learning programme. The pilot study ( $N=33$ ) was conducted from September 2021 to January 2022 after 12 lessons in a computer room via a pre-post intervention design. The data helped to ensure the questions were relevant, placed in an appropriate order and understood by the respondents.

For the main implementation of the AILQ, the analytical sample consisted of 363 students ranging in age from 12 to 17 years ( $M=13.1$ ;  $SD=1.39$ ) in two secondary schools divided by high and low academic background over a 3-month period (from November 2022 to January 2023). A 5-likert scale was used for all of the scales. Demographic data information like gender, age and nationality was collected from all participants. There were 180 male (49.6%) and 183 female (50.4%) participants. All the students were Chinese students who grew up in Hong Kong.

### AILQ

At the start of the instrument development, the first version of AILQ consisted of 60 items measuring students' learning outcomes via AI learning activities implemented by teachers and students during computer lessons. The items were distributed in four subscales, measuring students' affective, behavioural, cognitive and ethical dimensions, as defined by the ABCE framework. The four subscales are illustrated as follows:

- 'Affective' learning comprises four factors that measure students' subjectively experienced feelings in terms of (1) intrinsic motivation; (2) self-efficacy; (3) career interest; and (4) confidence in learning AI.
- 'Behavioural' learning comprises two factors: (1) students' behavioural commitment and (2) collaboration to build relationships to pursue learning goals in an AI environment.
- 'Cognitive' learning comprises three factors: students' knowledge and skills achievement from (1) lower- (know and understand AI), (2) mid- (use and apply AI) to (3) high-order thinking skills (evaluate and create AI).
- 'Ethical' learning comprises a set of seven ethical aspects: (1) reliability, (2) safety, (3) privacy, (4) responsibility, (5) transparency, (6) awareness and (7) social good that bring students' positive mindsets towards AI.

### Instructional design

The AI literacy programme was designed using the revised TPACK framework (Sun et al., 2023), and Bloom's taxonomy as extending previous work by Ng, Lee, et al., (2023), as shown in Figure 2. There are two cognition levels: know and understand AI (lower cognition level), and apply, evaluate and create (higher cognition level) across 10 topics. The students were invited to a series of 12-lesson AI literacy learning workshops over 2 months and completed a project in groups of five. In the first 10 lessons, each lesson covered one topic. The last two lessons were arranged for student project presentations. The lesson materials were central to AI literacy as it imparts knowledge, skills and attitudes to address technical, ethical and social dimensions, tailored to Secondary 1 students. They were validated by researchers, educational professors, AI experts and teachers. Pedagogies and technological support (eg, Teachable Machine, Quickdraw and image stylizers) were provided

![](d864789b0d8384da1d22fd6a5d76bbdf_img.jpg)

Figure 2 illustrates the Instructional design framework of AI literacy, adapted from Ng, Leung, Su, Ng, et al. (2023). The framework is structured into three main components: Instructional design, Content choices, and Learning outcomes.

**Instructional design** is divided into two branches:

- **Technologies** (Hardware and software, Air-related agent, Gamified elements)
- **Pedagogies** (Hands-on/ playful learning, Collaborative project-based learning)

**Content choices** (AI ethics) is divided into two branches:

- **Apply, evaluate and create AI** (1. AI and game design, 2. Smart home and smart city, 3. Chatbot, 4. AI and creativity, 5. AI in environmental projection, 6. Project presentations)
- **Know and understand AI** (1. Definitions and AI Basics, 2. AI perceptions, 3. Technical skills, 4. Machine learning and deep learning, 5. Social impacts)

**Learning outcomes** (AI ethics) are:

- Affects
- Behaviors
- Cognition
- Ethics

FIGURE 2 Instructional design framework of AI literacy, adapted from Ng, Leung, Su, Ng, et al. (2023).

to scaffold students' AI literacy development. Appendix S1 shows the learning activities conducted in the lessons. Teachers adopted collaborative project-based learning to deepen students' AI knowledge, hands-on experience and collaboration between students. During the process, students were asked to seek societal problems, initiate their projects and propose AI-empowered solutions to solve these issues. Students could use various hardware, software and AI-related agents to formulate their prototypes, for example, using Teachable Machine, Scratch AI, Huskeylen and drones. Furthermore, the problems addressed in the topic areas and student projects were under the theme 'AI ethics', which develops students' AI ethical values (eg, reliability, safety, privacy, responsibility, transparency, awareness and social good), from knowing to creating AI.

### Design of the instrument

With reference to a previous study, Ng, Lee, et al. (2023) conducted a literature review on AI literacy education at a secondary level based on 38 selected studies. The methodological approaches, assessment types and constructs of instruments that have been found in the literature were reviewed, and categorized into cognitive and non-cognitive domains. These questionnaires help explore the changes in ABCE mindsets towards evaluation in different AI literacy contexts (Ng et al., 2021b; Ng, Lee, et al., 2023). Furthermore, the initial version of the instrument was developed by adapting a list of prior research such as Chiu & Chai (2020, Ng & Chu, 2021)'s and Ng & Chu (2021)'s questionnaire, the Motivational Strategies for Learning Questionnaire (MSLQ), Park et al. (2012)'s behavioural intention, Song and Keller (1999)'s confidence scale and ethical guidelines (Jobin et al., 2019; Microsoft, 2023).

The authors then generated 33 items that were then reviewed and selected according to the following criteria: (1) appropriateness of the language used and ease of comprehension for secondary students; (2) ambiguity, appropriateness and commonality of content and context; and (3) implementability on the AI literacy programme. Items that did not satisfy one or more criteria were dropped and/or combined, resulting in a total of 12 items for use in the pilot study (Table S1). Each of these items was reviewed according to its suitability for measuring the competence area for which it had been developed (Jin et al., 2020). Furthermore, to create a lean model design, variables (eg, perceptions, readiness, fun, hands-on interactivity, futuristic thinking and satisfaction) shared overlapping meanings across dimensions and hence were not included. For example, hands-on interactivity can be measured as high-order thinking skills (ie, apply, evaluate and create AI). Overall, although the remaining 12

items addressed all four key competencies defined in the DigComp, 16 sub-competences were combined and 5 of them (ie, interdisciplinary thinking, relevance, futuristic thinking, perceived usefulness and persuasion) were not covered.

After eliminating the constructs that have shared overlapping meanings, the initial version considered 60 items with 12 factors in four dimensions including (1) affective (ie, intrinsic motivation, self-efficacy, career interest and confidence); (2) behavioural (ie, behavioural intention, behavioural engagement and collaboration); (3) cognitive (ie, know and understand AI, use and apply AI and evaluate and create AI); and (4) ethical learning. For each item, we asked students to express their extent of agreement with each statement based on a 5-point scale ranging from 'strongly disagree' (1) to 'strongly agree' (5). Table S2 presents a preliminary instrument and the origin of the dimensions. A full list of the questions can be found in Appendix S2.

The AILQ was validated by expert judgement based on criteria of relevance, writing and clarity, and checked for internal consistency which resulted in a Cronbach's alpha of 0.93. Then, the questionnaire contemplates psychometric validation through first- and second-order confirmatory factor analysis. To summarize, the academic rigour of the questionnaire lies in (1) established theoretical foundation, (2) validation by experts and (3) design of contextualized AI literacy instruments specifically for secondary students which is consistent with present literature.

### Validation of the instrument

Next, the validation process involved four steps: (1) expert judgement validation, (2) qualitative interview validation, (3) pilot study and (4) validation that includes confirmatory factor analysis (MacKenzie et al., 2011).

#### Expert judgement validation (Process 1)

Six experts participated in the validation process, including three academicians and three school teachers. Their backgrounds fall within one of the following expertise: teaching AI, educational research lines in AI curriculum, technology and research methodologies. The research objectives of developing this AI literacy questionnaire were explained before sending the instrument to each expert. The experts analysed the instrument according to the characteristics of sufficiency, clarity, coherence and relevance (Fernández-Gómez et al., 2020).

One of the main recommendations by the experts was that, in the cognitive learning dimension, four statements in 'use and apply' were combined with 'evaluate and create' as students may not identify the differences among applying, evaluating and creating. This aligns with some prior studies which suggest that young learners may struggle to differentiate between the cognitive levels of using and applying AI concepts versus evaluating and creating AI artefacts and solutions (Agarwal, 2019; Ng, Su, et al., 2023; Lemov, 2017). As a result, the 'use and apply' dimension that measures students' readiness was eliminated as it was taken into account in other dimensions in the cognitive learning dimension. Several phrases (eg, 'Computer Science/AI-related problems') were paraphrased for better generalization and clarity. After improving the quality of factors, the refined instrument resulted in 55 statements.

#### Validation by interview (Process 2)

Interviews were conducted with Secondary 1–3 students following Caicedo Cavagnis and Zalazar-Jaime (2018)'s guidelines. First, the interviewer explained the process and purposes

of the interview. Learning resources were shown to the students to contextualize the questionnaire. Second, the interviewer read each statement and asked students to answer the questions. The interviewer then asked the students to explain the statements and express their thoughts. The students suggested combining the factors in 'use and apply' and 'evaluate and create' as they tend to confuse the difference among application, evaluation and creation, which is consistent with the teachers. Moreover, more clarity was needed to explain the set of ethical principles in this questionnaire.

### Pilot study (Process 3)

The instrument was administered under the intended context (ie, secondary school setting) as a pilot run for researchers to review and iterate the application of the questionnaire and AI literacy programme implementation among participants. In the pilot study, 47 Secondary 1–2 students (25 girls and 22 boys; average age = 12 years) were recruited from two secondary schools in Hong Kong. Ten lessons of learning activities related to AI were designed and delivered in class.

Take one lesson as an example. Students interacted with the Teachable Machine (a web-based tool that enables students to train ML models without prior coding experience). They gathered data samples for their AI to learn. Then, students trained their model and tested it to observe whether it can classify new examples correctly. Students then exported their model for their projects and built a game or application using blocky programming language called Scratch AI. The content of each activity was based on the AI literacy curriculum supported by the first author's university (Ng & Chu, 2021). To carry out the activities smoothly, each student had adequate learning resources such as AI-driven hardware and software, technical guidelines and textbook resources in the learning management system. Each lesson lasted 35 minutes. In the pilot study, students were given around 20 minutes to study the questions in the questionnaire.

For data analysis, paired sample  $t$ -tests were conducted to evaluate whether the learning programme and resources can stimulate students' development in ABCE aspects. Teachers and students were invited to reflect and provide feedback to improve the overall quality of interventions and the questionnaire via interviews. Table S3 illustrates the 10 major learning factors to be measured in the study. This version was further administered among 363 students to conduct the confirmatory factor analysis.

### Validation (Process 4A and 4B)

This section describes the psychometric validation process for the questionnaire validation. After removing extreme values and missing data, a preliminary analysis (Process 4A) was applied to show the mean, SD, skewness kurtosis and Cronbach's alpha ( $\alpha$ ) of collected data. Exploratory factor analysis was conducted to understand the underlying factors among all items. The four factors explained 51.41% of the total variance. The explained variance varying from 40% to 60% is considered sufficient in educational fields (Kline, 1994). Then, the theoretical model and its implementations in the AILQ were tested by confirmatory factor analysis (CFA).

After the preliminary study, structural validity was tested using the first- and second-order CFA. We adopted weighted least squares estimators to reduce outliers (Flora & Curran, 2004). Discriminant validity between latent variables was evaluated using the Heterotrait–Monotrait ratio of correlations (HTMT). Convergent validity was assessed through the average variance extracted (AVE) (Henseler et al., 2015). Given that the Chi-square test would be almost

significant when the sample size is larger than 400 (Jackson, 2003), we used a set of model fit indexes including root mean square error of approximation (RMSEA), comparative fit index (CFI), Tucker-Lewis coefficient (TLI) and standardized root mean square residual (SRMR) to evaluate the goodness of fit of the AI literacy model. The criterion of the model fit was RMSEA <0.08, CFI >0.90, TLI >0.90 and SRMR <0.08 (Marsh et al., 2004). The reliability and internal consistency of the scale were accessed by Cronbach's alpha ( $\alpha$ ) and McDonald's omega ( $\omega$ ) to measure based on the correlations between different items (Table S4). Then, the second-order model was examined versus the third-order model using the chi-square differences test and Akaike information criterion. To validate the model's fitness and confirm the hypotheses, the CFA was conducted using the R package (lmt) and Mplus 8.3 respectively.

## RESULTS

### Pilot study (Process 3)

The pilot study enables a refinement of the learning programme implementation and strategy before constructing the CFA among 47 students. In the pilot study, we evaluated the predictive validity of knowledge tests compared with cognitive ability tests and school grades. Paired sample t-tests were conducted to examine whether students gain a significant improvement in terms of ABCE learning throughout their AI literacy programme. The results of paired t-tests showed that students significantly attained improvements in AI literacy for all of the learning constructs ( $p < 0.05$ ) as shown in Table S5.

First, the factor 'career interest' does not show any significant effect ( $p=0.064$ ). This is consistent with prior STEAM/computer education studies that students may not have a strong interest in career aspirations, as it may be too young for students to imagine their future career (Aschbacher et al., 2014; Ng, Su, et al., 2023). Career counselling and mentoring play an important role in enhancing students' career development for promoting students' interests in STEAM (Stoeger et al., 2013). The teacher and expert team suggested removing 'career interest' during the validation processes as students may not be mature enough to understand career-related concepts and decision making. Furthermore, an expert reviewer suggested that there are overlapping wordings between the 'confidence' and 'self-efficacy' factors. As a result, some of the items for these two factors were combined using statistical methods and assessed for validity.

Second, the factor ('collaboration') tends to have lower mean scores, compared with the others and was marginally significant ( $p=0.057$ ). Some measures were provided to promote students' collaboration by reinforcing a project-based learning environment that emphasizes collaborating with others in an AI-driven context. These measures include demonstrating group work and engaging students in high-order thinking activities such as evaluation, critical thinking and creativity with their classmates. Each student was assigned roles and formulated into small groups of five to reinforce collaboration. Moreover, each activity should be engaging enough and delivered through a learning management system (LMS) that students are familiar with. This could also facilitate their content management and offer engaging resources (eg, videos, simulations and gamified quizzes) to develop students' AI literacy. Furthermore, the items under the factors 'behavioural intention' and 'behavioural engagement' were grouped into one factor, namely 'behavioural commitment'. As teachers pointed out, some items between factors are not easy to differentiate. Behavioural commitment refers to the level of dedication and involvement an individual has towards a specific behaviour or goal (Kell & Motowidlo, 2012). It encompasses both the intention to engage in the behaviour and the active participation and effort put forth to achieve it.

Third, the teacher and expert team mentioned that there are an exceeding number of items under the AI ethics factor. Thus, items that shared similar meanings were deleted. For example, 'guide moral conduct' (AIE01), 'minimizing data bias' (AIE03) and 'perform reliability and safely' (AIE04) were related to ethics, safety, reliability and security; 'respect privacy' (AIE06) and 'transparent about users' requests' (AIE09) can contribute to the meaning that AI should be accountable for using AI systems (AIE11), and meet ethical standards (AIE12); and 'promote human well-being' (AIE14), 'use AI to serve others' (AIE15) and 'achieve common good' (AIE16) could be similar to the tendency for students to use AI for social beneficence (AIE01). As such, eight items (AIE01, AIE03, AIE04, AIE06, AIE09 and AIE14-16) were removed.

### Preliminary analysis (Process 4A)

After the pilot study, 363 students were invited to the learning programme and to provide their responses for further CFA. At the end of the refined programme, they completed a 55-item questionnaire to evaluate their learning outcomes. The posttest data of the questionnaire was collected and analysed. The mean, standard deviation, skewness and kurtosis of the items are shown in Table S6. The values of the skewness and kurtosis of all items ranged between -1.00 and 1.00, which indicated that the items were normally distributed (DiStefano, 2002). Students obtained responses in all four dimensions with a mean greater than 3.00. The reliability and internal consistency of the scale that was higher than 0.90 were regarded as excellent when considering Cronbach's alpha ( $\alpha$ ).

### Confirmatory factor analysis (CFA) (Process 4B)

Based on the results of the graded response model, a correlated factor model was constructed using CFA to examine the structural validity of the ABCE model. Because of this two-tier structure, a second-order CFA is necessary to examine the second-order factor of the proposed model. To construct the concept of 'AI literacy', we first validate the first-order factors, and then test a second-order factor of the measurement model.

CFA was used to validate the structural validity of the ABCE model. Discriminant validity between latent variables was evaluated using the Heterotrait-Monotrait ratio of correlations (HTMT), where a value less than 0.85 was considered acceptable (Henseler et al., 2015). Convergent validity was assessed through the average variance extracted (AVE), with values exceeding 0.50 deemed acceptable (Fornell & Larcker, 1981). To gauge overall model fit, we employed multiple goodness-of-fit indices, including the root mean square error of approximation (RMSEA), comparative fit index (CFI), Tucker-Lewis coefficient (TLI) and standardized root mean square residual (SRMR). We applied stringent criteria for model fit assessment: RMSEA < 0.08, CFI > 0.90, TLI > 0.90 and SRMR < 0.08 (Marsh et al., 1999).

The individual structure of the ABCE model was analysed by four separate CFA models. In the affective learning dimension, the HTMT value of 0.88 between confidence and self-efficacy suggested an overlap. Consequently, we combined the two scales and removed four items (SE05, CL02, CL04 and CL05), resulting in a refined model with good fit (Table S7) ( $p = 0.00$ ; CFI = 0.96; TLI = 0.95; and SRMR = 0.07). Within the behavioural learning dimension, behavioural engagement exhibited substantial overlap with behavioural learning (HTMT = 0.91). This led us to combine behavioural intention and behavioural engagement to form a revised factor, namely behavioural commitment, and removed EN03 and EN04. This finding is aligned with the experts' review. Furthermore, the subsequent CFA revealed a lower factor loading for item SI04 and thus the item was removed. Similarly,

in the initial cognitive learning dimension, 'know and understand AI' displayed overlap with both 'evaluate and create AI' (HTMT=0.89). We removed items KU03, KU04, EC02 and EC05, yielding a satisfactory discrimination index (HTMT=0.81). Additionally, sub-factors of 'use and apply AI' demonstrated strong cross-loadings, necessitating the removal of the scale. In the context of AI ethics, comprising 16 items, the goodness of fit of the initial model was poor. We removed items AIE01, AIE03, AIE04, AIE06, AIE09, AIE14, AIE15 and AIE16 resulting in a satisfactory model fit.

Across all four revised models, the AVE for all latent factors exceeded 0.50, indicating strong convergent validity (Table S8), except for intrinsic motivation and AI ethics, which approached the 0.50 cut-off at 0.44 and 0.46 respectively. After thoroughly examining the individual factor structures within the ABCE model, we explored the possibility of a higher-order factor, AI literacy, encompassing affective, behavioural, cognitive and ethical learning dimensions (Figure 3). Both the second-order model ( $\chi^2$  (452) = 1001.54,  $p < 0.01$ ; RMSEA=0.06; CFI=0.92; TLI=0.91; and SRMR=0.06) and the third-order model ( $\chi^2$  (455) = 1051.03,  $p < 0.01$ ; RMSEA=0.06; CFI=0.91; TLI=0.91; and SRMR=0.06) demonstrated acceptable goodness of fit. Model comparison using the chi-square differences test and Akaike information criterion favoured the second-order model ( $\Delta\chi^2 = 49.49$ ,  $p < 0.01$ ;  $\Delta$ AIC=43.45), indicating significantly superior model fit compared to the third-order model (Vrieze, 2012).

## DISCUSSION

### The scale development

To understand the important components that should be measured in AI literacy, scale development was first launched by our literature review. Based on the review, this study then received experts' views from different fields such as computer science and educational technology, as well as teachers' and students' feedback. The questionnaire was then administered to students who participated in the AI literacy programme to refine the items through

![Figure 3 displays two factor models. The left model is a correlated factor model showing a complex structure of latent factors (e.g., e, a, b, c, d, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z) and their corresponding observed items (e.g., item 1, item 2, item 3, etc.). The right model is a third-order model, showing a hierarchical structure where the latent factors are nested within a higher-order factor (labeled 1.01).](c222a9006d6d60a8d81e6ffbfc0e74ad_img.jpg)

Figure 3 displays two factor models. The left model is a correlated factor model showing a complex structure of latent factors (e.g., e, a, b, c, d, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z) and their corresponding observed items (e.g., item 1, item 2, item 3, etc.). The right model is a third-order model, showing a hierarchical structure where the latent factors are nested within a higher-order factor (labeled 1.01).

FIGURE 3 The results of the correlated factor model (left) and third-order model (right).

a pilot study and interview. Following their suggestions, some items were rephrased and removed from the questionnaire. The initial version of the scale was conducted through literature review, yielding a four-factor structure. This study aligned with Ng & Wu et al. (2021)'s study that an explained variance (69.97%) was found. Differences in variances may be due to the number of factors in the questionnaires.

CFA suggests that the 32 items of the AILQ be reconsidered in terms of affective, behavioural, cognitive and ethical learning. It pointed out the necessity of merging and reducing some overlapping factors (ie, self-efficacy and confidence; behavioural intention and engagement). Also, career interest was excluded while behavioural intention and engagement were combined to enhance the validity of the questionnaire. This aligns with prior studies that people may perceive a close relationship between self-efficacy and confidence (Marsh et al., 2023). Behavioural intention and engagement could also be highly related to each other (Xu & Qiu, 2021). Although studies have pointed out the need to differentiate between these terms, it is worth noting that maintaining positive self-efficacy can contribute to building a student's confidence. Students who intend to act in a proactive manner often exhibit higher levels of engagement. Given the potential overlap in these constructs and the challenges associated with distinguishing them when dealing with over 30 items, it is reasonable that young students may perceive them similarly in certain contexts.

When there could be many (sub-)factors in a dimension, prior research suggested relaxing the measurement part of the model using CFA (Asparouhov & Muthén, 2014; Bandalos & Finney, 2018). In this study, the first-order CFA could suggest the proposal of using ABCE model to examine students' AI literacy. To further operationalize the sub-dimensions, the four first-order factors of AI literacy were validated, and then tested a second-order factor of the AI literacy measurement model. The second-order CFA provided alternative evidence as to whether these sub-factors were loaded on AI literacy. Through the relaxation of standards for fit of the model and examining a second-order factor of the model, we could draw a model without violating the overall significance level and theoretical framework to keep most of the important sub-factors to assess students' AI literacy.

The acceptable and good model fits from CFA and enables us to understand the final constructs verified by the ABCE framework. That is, ABCE is a potential framework to measure students' learning outcomes for their AI literacy development. Much educational research adopts the ABC approach to examine students' DL and/or learning outcomes such as educational games (Lamb et al., 2018), adaptive tutoring systems (Arroyo et al., 2014) and audience response systems (Hunsu et al., 2016). The current study also extended the version with the component of ethical knowledge (E) for assessing the responsible use of AI as critical knowledge. This is consistent with recent research that educational models should be refined to shed light on knowledge and skills to use AI ethically (Celik, 2023).

### An updated conceptual framework of AI literacy

Digital literacy education seeks to equip students with the competencies (knowledge, skills and attitudes) in the use of digital technology needed to create learning opportunities, to pursue their careers and interests, to facilitate their work and to contribute to society as responsible citizens (Jin et al., 2020). As AI affects our everyday lives, AI literacy education has been proposed in recent years to facilitate the development of students' attitudes towards AI, perceived knowledge and skills underpinning AI technologies and critical understandings towards ethical implications of AI (Chiu & Chai, 2020; Kong et al., 2022; Long & Magerko, 2020; Ng et al., 2021a, 2021b).

When embracing new technologies, researchers are always interested in understanding students' affects, behaviours, cognitive and ethical learning during their knowledge and

skills acquisition phase (eg, Chai, Wang, et al., 2020; Heyder & Posegga, 2021; Laupichler et al., 2022; Wang et al., 2022). Our proposed questionnaire items covered affective learning as the first dimension to use AI confidently and self-efficaciously. Four affective factors (ie, intrinsic motivation, self-efficacy, career interest and confidence) are hypothesized as important roles in elevating AI literacy, which are consistent with other recent AI education studies (Chiu et al., 2021; Kong et al., 2022; Lin et al., 2021; Su et al., 2022). It relates to students' feelings, which in turn shapes their attitudes towards AI. The more confident they are in using AI technologies, the more motivated and interested they are to pursue their work and studies about AI.

Second, AI technology has impacted human behaviour, and continues to change our world and how we interact with others (Araujo, 2018). In this study, behavioural learning (ie, collaboration and behavioural commitment) measures students' positive behaviour associated with usage and understanding of AI. Aligned with recent studies (Chiu & Chai, 2020; Ng, Su et al., 2023), this dimension proposed that AI literate students should be collaborators and active and productive learners who are able to facilitate group work, promote meaningful communication and perform positive actions towards using AI technologies.

Third, regarding cognitive learning, we focused on perceived AI knowledge not only for knowing how to use these technologies but also for interacting with, evaluating and creating AI tools ethically (Ng et al., 2021a, 2021b). Instead of merely becoming AI technology users, students have the potential to develop higher cognition skills and become critical thinkers, collaborators and creators who are able to apply AI applications in other contexts and create solutions to solve authentic problems (Audrin & Audrin, 2022; Ng, Su et al., 2023). Ng et al. (2021b) has categorized these cognitive skills into three cognitive domains to better understand students' AI literacy development. These domains provide a framework for examining the cognitive skills required for AI literacy, including lower cognition skills to understand the principles of AI, and higher cognition skills to use and apply AI applications, evaluating and creating AI-driven solutions. Moreover, educators can better equip students with the necessary knowledge and skills to become responsible and ethical users, and creators of AI technologies.

The framework was adapted from Bloom's taxonomy which originally consisted of six major dimensions: knowledge, comprehension, application, analysis, synthesis and evaluation. However, this taxonomy has faced criticism as it portrays cognitive processes as discrete and suggests that it is possible to perform one of these skills separately from others (Agarwal, 2019). This can lead to difficulties for students in accurately differentiating between the various levels of Bloom's taxonomy, particularly when students create artefacts, they apply knowledge and evaluate between applications at the same time (Lemov, 2017). Furthermore, knowing how to use AI tools should only be considered a part of lower cognition skills as learners may use AI applications for everyday purposes, without fully understanding the underlying principles behind them. To address these issues, Ng, Wu et al. (2023) proposed to measure students' lower and higher cognition skills in relation to AI. In Ng, Wu et al. (2023)'s study, a six-factor model structure was proposed to examine students' motivation, self-efficacy of learning AI, behavioural intention, engagement and cognition skills related to (1) knowing, understanding, (2) applying, evaluating and creating AI during literacy development. This framework goes beyond mere recall of AI knowledge and emphasizes the ability to apply, evaluate and create AI. The study also proposed a less parsimonious model, which included a four-factor structure to explain the secondary students' AI literacy. These alternative models could be adopted to meet different research goals such as measuring a wider range of AI literacy skills and using a parsimonious model that summarizes cognitive and non-cognitive domains.

Furthermore, our proposed instrument contributes to measuring students' lower and higher cognition skills in relation to AI. In terms of lower cognition skills, students are not

only end users who know how to use AI applications. They should also understand the concepts behind it. In terms of higher cognition skills, students should apply relevant AI concepts, and evaluate between AI technologies and their potential values from various perspectives for different situations. For example, they can choose suitable tools and solutions for different purposes, conduct data analysis, identify patterns and build meaningful information. Students can derive machine learning models and invent AI-driven solutions to solve authentic problems, such as robotics, games and chatbots. The inclusion of higher cognition skills in AI education has become a crucial step towards promoting critical and creative thinking skills among students.

At last, ethical mindsets are further included in this study to foster responsible digital citizens which follow some recent AI ethical guidelines (Jobin et al., 2019; Microsoft, 2023). Ethical learning is considered as a critical knowledge that students know the moral principles and techniques behind AI applications, and can inform the responsible use, development and creation of AI technology. This questionnaire contributes to developing a set of items that evaluate how students develop their proper and ethical manners (eg, reliability, safety, inclusiveness, transparency and accountability). Figure 4 summarizes the ABCE dimensions of AI literacy development.

### Significance of the study

First, although recent studies have started to explore how to develop AI literacy curricula in K-12 settings, many of the existing questionnaires have not yet been validated. Some researchers have begun to conduct CFA to examine students' AI literacy level and development. However, there is a lack of validated questionnaires for secondary students. Considering this gap, we first developed a questionnaire to examine students' learning outcomes based on the ABCE framework. We built a model to demonstrate a set of cognitions from lower to higher thinking skills. Ng et al. (2021a, 2021b) proposed four cognitive domains for AI literacy, and they (Ng et al., 2022) further conducted a qualitative study to identify a set

![](a430996a9e8993deb0c6b25da234744b_img.jpg)

Cognitive and ethical learning

Affective and behavioral learning

Higher cognition level

Lower cognition level

C. Cognitive learning: Apply, evaluate and create AI

- Create AI-driven solutions (e.g., chatbots, robotics) to solve problems.
- Evaluate AI applications and concepts for different situations
- Apply AI applications to solve problems

E. Ethical learning: Responsible users and critical thinkers

Know and understand AI

- Know the working principles behind
- How AI perceives the world
- Know what AI is and its related terms (e.g., machine learning, deep learning, NLP)
- Societal impacts
- Know how to use AI (e.g., Siri, chatbot)

A. Affective learning: Confident and self-efficacious users who have interests and motivation to learn AI

B. Behavioral learning: Active and productive users who can collaborate with others and engage themselves in AI environments

FIGURE 4 Conceptual framework of AI literacy.

of AI literacy knowledge and skills based on these domains. This study further contributes to proposing other forms of learning (eg, affects and behaviours) that promote AI literacy development in this questionnaire. Different from the studies, the current study specifically adds items in the questionnaire to inform how students could become confident, self-efficacious, motivated, active and productive users with respect to the affective and behavioural dimensions as suggested by Ng, Leung, et al. (2023). Taking a step further, this study adds ethical learning as a critical knowledge/skill in the questionnaire to examine students' ethical understanding.

Second, prior research on AI literacy has been mostly focused on development of AI knowledge, skills and attitudes (Ng, Su et al., 2023; Kong et al., 2022). However, few studies focus on socio-emotional aspects like collaboration and engagement (Heyder & Posegga, 2021), and a diverse level of thinking skills (Ng, Leung, Su, Ng, et al., 2023). For example, Heyder and Posegga (2021) structured AI literacy into sociocultural aspects and examined how an individual's attitude towards AI is influenced by their cultural environment, while Pinski and Benlian (2023) developed a scale to measure the role of AI in human–AI interaction. Students are regarded as team players and problem solvers who work with others to create solutions and evaluate others' judgements using AI (Ng, Lee, et al., 2023). Therefore, one of the key contributions is that the assessment process should consider students' affective, behavioural, cognitive and ethical learning as a whole. This study thus attempts to fill a gap by proposing the use of the ABCE framework to address a wider skill set of AI literacy.

Third, recent educational research has started to incorporate ethical values to update conventional frameworks such as the TPACK competencies and Bloom's taxonomy (Ng, Leung, et al., 2021; Celik, 2023; Yurdakul & Çoklar, 2014). Although a number of studies have designed questionnaires to investigate the multidimensionality of learning outcomes (Ben-Eliyahu et al., 2018; Wei et al., 2021), seldom do they extend the ethical knowledge into this model to examine students' digital literacy, learning performance and ethical knowledge. This study contributes to designing a questionnaire that follows this model and extends with ethical assessments of AI literacy education. It suggests a subjective measure of ethics to supplement existing ABC measures.

The research framework suggests practical implications for AI literacy curriculum design. First, it is important to address students' emotional and psychological needs that stimulate their interest and motivation towards AI literacy development (Chiu & Chai, 2020). Second, the study also sheds light on promoting positive learning behaviours that yield knowledge and skills acquisition, desire and intention to learn. Engaged students tend to spend more time studying the learning materials in the curriculum or platforms (Merzbacher, 2001). Second, collaboration and cooperation with others in AI settings are crucial behavioural factors that promote effective learning of AI literacy (Heyder & Posegga, 2021). They are life and career skills that students need to learn so that they can work in groups and tinker with their creations in AI-driven working environments (Ng, Lee, et al., 2023; Mota-Valtierra et al., 2019). Third, this study presented insights and offered a parsimonious version of lower and higher cognitive domains for AI literacy development. Finally, AI literacy has to pay attention to both students' technological knowledge and skills, and moral mindsets so as to help students become responsible citizens (Chai et al., 2021; Chiu et al., 2022; Jobin et al., 2019; UNESCO, 2022). Considering prior research, AI literacy needs to reinforce students' socially appropriate behaviours (eg, reliability, safety, privacy, responsibility, accessibility, transparency and accountability) that are expected to reduce the negative consequences of the impact of technologies (Borenstein & Howard, 2021; Zhang et al., 2021). Overall, this study contributes to providing a holistic assessment of the ABCE aspects as a whole, which helps in examining the development of students' AI literacy. This, in turn, is expected to lead to responsible AI literacy among students.

## Limitations and future research directions

A few limitations were identified. First, although articles from a systematic review (Ng et al., 2021b) and proper DL studies suggest a wide set of AI literacy skills, there may be significant unidentified knowledge in various reviews, and it would be difficult to measure all potential constructs. In this regard, it is noteworthy that this proposed questionnaire, although comprehensive, did not cover all of the competencies of AI literacy. Future studies could try to examine how students develop other learning components. Researchers could pay attention to the differences between AI literacy performance and self-reported AI literacy.

In addition, this study focused on measuring student's learning outcomes based on AI literacy programme which may limit the generalized use in other contexts. In reality, different programmes could be designed with different pedagogical, teaching tools and cultural aspects which may influence students' perceptions of the factors affecting their learning outcomes. Caution should be taken when generalizing the findings to students from different grades, contexts and schools. Prior research suggested learners' and cohort differences existed when developing DL scales (Lazonder et al., 2020). Learning programmes could have different focuses and intelligent technologies develop rapidly and are used by students – a cross-cohort/sectional longitudinal study is necessary to fully understand their AI literacy development. Also, our questionnaire only focuses on 12 types of AI literacy; future research could be conducted to evaluate other types of learning outcomes (eg, subject norms, anxiety, confidence in using AI, behavioural intention and readiness). Finally, the novelty effect when incorporating AI curricula was not considered in this study. Longitudinal and qualitative studies could help address these questions in greater depth in conjunction with diverse research methods.

## CONCLUSION

This study surveyed secondary students' affective, behavioural and cognitive learning and their AI ethics development. This study developed an instrument for strengthening and extending the conceptualization of AI literacy from a robust theoretical perspective, ABCE. Based on the model, we developed a valid and reliable measurement tool. However, knowing how to use AI and its basic understanding is not sufficient to effectively gain adequate literacy in today's world. Therefore, students should obtain other socio-emotional, meta-cognitive and higher-order thinking skills. Students' ethical mindset to utilize AI is crucial. We conclude that AI literacy requires students to possess not only technical but also affective, behavioural and ethical knowledge and skills. ABCE might be considered a robust model that sheds light on the learning outcomes for AI literacy programme assessments. The study offers an updated framework to conceptualize necessary AI literacy and provides some empirical evidence for Authors (2021b)'s four cognitive domains and Ng, Leung, et al., (2023)'s conceptual framework of AI literacy.

## FUNDING INFORMATION

None.

## CONFLICT OF INTEREST STATEMENT

The authors declare that they have no conflict of interest.

## DATA AVAILABILITY STATEMENT

The data that support the findings of this study are available on request from the corresponding author. The data are not publicly available due to privacy or ethical restrictions.

## ETHICS STATEMENT

All procedures performed in studies involving human participants were in accordance with the ethical standards of the institutional and/or national research committee and with the 1964 Helsinki Declaration and its later amendments or comparable ethical standards.

## CONSENT STATEMENT

Informed consent was obtained from all individual participants included in the study.

## ORCID

Davy Tsz Kit Ng <https://orcid.org/0000-0002-2380-7814>

Jac Ka Lok Leung <https://orcid.org/0000-0001-6490-7005>

Thomas Kin Fung Chiu <https://orcid.org/0000-0003-2887-5477>

Samuel Kai Wah Chu <https://orcid.org/0000-0003-1557-2776>

## REFERENCES

Agarwal, P. K. (2019). Retrieval practice & Bloom's taxonomy: Do students need fact knowledge before higher order learning? *Journal of Educational Psychology*, 111(2), 189–209.

Ali, S., Payne, B. H., Williams, R., Park, H. W., & Breaezeal, C. (2019, June). Constructionism, ethics, and creativity: Developing primary and middle school artificial intelligence education. In *International Workshop on Education in Artificial Intelligence k-12 (eduai'19)* (Vol. 2, pp. 1–4).

Araujo, T. (2018). Living up to the chatbot hype: The influence of anthropomorphic design cues and communicative agency framing on conversational agent and company perceptions. *Computers in Human Behavior*, 85, 183–189. <https://doi.org/10.1016/j.chb.2018.03.051>

Arroyo, I., Woolf, B. P., Burelson, W., Muldner, K., Rai, D., & Tai, M. (2014). A multimedia adaptive tutoring system for mathematics that addresses cognition, metacognition and affect. *International Journal of Artificial Intelligence in Education*, 24(4), 387–426. <https://doi.org/10.1007/s40593-014-0023-y>

Aschbacher, P. R., Ing, M., & Tsai, S. M. (2014). Is science me? Exploring middle school students' STE-M career aspirations. *Journal of Science Education and Technology*, 23, 735–743.

Asparouhov, T., & Muthén, B. (2014). Multiple-group factor analysis alignment. *Structural Equation Modeling: A Multidisciplinary Journal*, 21(4), 495–508. <https://doi.org/10.1080/10705511.2014.919210>

Audrin, C., & Audrin, B. (2022). Key factors in digital literacy in learning and education: A systematic literature review using text mining. *Education and Information Technologies*, 27(6), 7395–7419. <https://doi.org/10.1007/s10639-021-10832-5>

Bandalos, D. L., & Finney, S. J. (2018). Factor analysis: Exploratory and confirmatory. In G. R. Hancock, L. M. Stapleton, & R. O. Mueller (Eds.), *The reviewer's guide to quantitative methods in the social sciences* (pp. 98–122). Routledge.

Ben-Eliyahu, A., Moore, D., Dorph, R., & Schunn, C. D. (2018). Investigating the multidimensionality of engagement: Affective, behavioral, and cognitive engagement across science activities and contexts. *Contemporary Educational Psychology*, 53, 87–105. <https://doi.org/10.1016/j.cedpsych.2018.01.002>

Borenstein, J., & Howard, A. (2021). Emerging challenges in AI and the need for AI ethics education. *AI and Ethics*, 1, 61–65. <https://doi.org/10.1007/s43681-020-00002-7>

Caicedo Cavagnis, E., & Zalazar-Jaime, M. F. (2018). Entrevistas cognitivas: revisión, diretrices de uso y aplicación en investigaciones psicológicas. *Avaliação Psicológica*, 17(3), 362–370.

Carolus, A., Koch, M., Straka, S., Latoschik, M. E., & Wienrich, C. (2023). MAILS—Meta AI Literacy Scale: Development and testing of an AI literacy questionnaire based on well-founded competency models and psychological change-and meta-competencies. <https://doi.org/10.48550/arXiv.2302.09319>

Celik, I. (2023). Towards intelligent-TPACK: An empirical study on teachers' professional knowledge to ethically integrate artificial intelligence (AI)-based tools into education. *Computers in Human Behavior*, 138, 107468. <https://doi.org/10.1016/j.chb.2022.107468>

CEPIS. (2015). *Computing and digital literacy: Call for a holistic approach*. [http://code.intef.es/wp-content/uploads/2017/10/position\\_paper\\_computing\\_and\\_digital\\_literacy.pdf](http://code.intef.es/wp-content/uploads/2017/10/position_paper_computing_and_digital_literacy.pdf)

Chai, C. S., Lin, P. Y., Jong, M. S. Y., Dai, Y., Chiu, T. K., & Huang, B. (2020, August). Factors influencing students' behavioral intention to continue artificial intelligence learning. In 2020 International symposium on educational technology (ISET) (pp. 147–150). IEEE.

Chai, C. S., Lin, P. Y., Jong, M. S. Y., Dai, Y., Chiu, T. K., & Qin, J. (2021). Perceptions of and behavioral intentions towards learning artificial intelligence in primary school students. *Educational Technology & Society*, 24(3), 89–101. <https://www.jstor.org/stable/27032858>

Chai, C. S., Wang, X., & Xu, C. (2020). An extended theory of planned behavior for the modelling of Chinese secondary school students' intention to learn artificial intelligence. *Mathematics*, 8(11), 2089. <https://doi.org/10.3390/math8112089>

Chiu, T. K., & Chai, C. S. (2020). Sustainable curriculum planning for artificial intelligence education: A self-determination theory perspective. *Sustainability*, 12(14), 5568. <https://doi.org/10.3390/su12145568>

Chiu, T. K., Meng, H., Chai, C. S., King, I., Wong, S., & Yam, Y. (2021). Creation and evaluation of a preteritary artificial intelligence (AI) curriculum. *IEEE Transactions on Education*, 65(1), 30–39. <https://doi.org/10.1109/TE.2021.3085878>

Chiu, T. K., Sun, J. C. Y., & Ismailov, M. (2022). Investigating the relationship of technology learning support to digital literacy from the perspective of self-determination theory. *Educational Psychology*, 42(10), 1263–1282.

Choi, M., Glassman, M., & Cristol, D. (2017). What it means to be a citizen in the internet age: Development of a reliable and valid digital citizenship scale. *Computers & Education*, 107, 100–112. <https://doi.org/10.1016/j.compoeu.2017.01.002>

Coşgun Öğeyik, M. (2022). Using bloom's digital taxonomy as a framework to evaluate webcast learning experience in the context of Covid-19 pandemic. *Education and Information Technologies*, 27(8), 11219–11235. <https://doi.org/10.1007/s10639-022-11064-x>

Covello, S., & Lei, J. (2010). A review of digital literacy assessment instruments. *Syracuse University*, 1, 31.

Dai, Y., Chai, C. S., Lin, P. Y., Jong, M. S. Y., Guo, Y., & Qin, J. (2020). Promoting students' well-being by developing their readiness for the artificial intelligence age. *Sustainability*, 12(16), 6597.

DiStefano, C. (2002). The impact of categorization with confirmatory factor analysis. *Structural Equation Modeling*, 9(3), 327–346. [https://doi.org/10.1207/S15328007SEM0903\\_2](https://doi.org/10.1207/S15328007SEM0903_2)

Druga, S., Vu, S. T., Likhith, E., & Qiu, T. (2019). *Inclusive AI literacy for kids around the world*. In *Proceedings of FabLearn 2019* (pp. 104–111). New York, USA. <https://doi.org/10.1145/3311890.3311904>

Fernández-Gómez, E., Martín-Salvador, A., Luque-Vara, T., Sánchez-Ojeda, M. A., Navarro-Prado, S., & Enrique-Mirón, C. (2020). Content validation through expert judgement of an instrument on the nutritional knowledge, beliefs, and habits of pregnant women. *Nutrients*, 12(4), 1136. <https://doi.org/10.3390/nu12041136>

Flora, D. B., & Curran, P. J. (2004). An empirical evaluation of alternative methods of estimation for confirmatory factor analysis with ordinal data. *Psychological Methods*, 9(4), 466–491. <https://doi.org/10.1037/1082-989X.9.4.466>

Fornell, C., & Larcker, D. F. (1981). Evaluating structural equation models with unobservable variables and measurement error. *Journal of Marketing Research*, 18(1), 39–50.

Guo, Y. R., & Goh, D. H. L. (2016). Evaluation of affective embodied agents in an information literacy game. *Computers & Education*, 103, 59–75. <https://doi.org/10.1016/j.compoeu.2016.09.013>

Henseler, J., Ringle, C. M., & Sarstedt, M. (2015). A new criterion for assessing discriminant validity in variance-based structural equation modeling. *Journal of the Academy of Marketing Science*, 43, 115–135.

Hunsu, N. J., Adesope, O., & Bayly, D. J. (2016). A meta-analysis of the effects of audience response systems (clicker-based technologies) on cognition and affect. *Computers & Education*, 94, 102–119. <https://doi.org/10.1016/j.compoeu.2015.11.013>

Jackson, D. L. (2003). Revisiting sample size and number of parameter estimates: Some support for the N:q hypothesis. *Structural Equation Modeling*, 10(1), 128–141. [https://doi.org/10.1207/S15328007SEM1001\\_6](https://doi.org/10.1207/S15328007SEM1001_6)

Jin, K. Y., Reichert, F., Cagasan, L. P., Jr., de La Torre, J., & Law, N. (2020). Measuring digital literacy across three age cohorts: Exploring test dimensionality and performance differences. *Computers & Education*, 157, 103968. <https://doi.org/10.1016/j.compoeu.2020.103968>

Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. *Nature Machine Intelligence*, 1(9), 389–399. <https://doi.org/10.1038/s42256-019-0088-2>

Jung, Y., & Lee, J. (2018). Learning engagement and persistence in massive open online courses (MOOCs). *Computers & Education*, 122, 9–22. <https://doi.org/10.1016/j.compoeu.2018.02.013>

Kandhhofer, M., Steinbauer, G., Hirschmugl-Gaisch, S., & Huber, P. (2016, October). Artificial intelligence and computer science in education: From kindergarten to university. In *IEEE Frontiers in Education Conference (FIE)* (pp. 1–9). IEEE, Erie, USA. <https://doi.org/10.1109/FIE.2016.7757570>

Kell, H. J., & Motowidlo, S. J. (2012). Deconstructing organizational commitment: Associations among its affective and cognitive components, personality antecedents, and behavioral outcomes. *Journal of Applied Social Psychology*, 42(1), 213–251.

Kim, M., & Choi, D. (2018). Development of youth digital citizenship scale and implication for educational setting. *Journal of Educational Technology & Society*, 21(1), 155–171. <https://www.jstor.org/stable/26273877>

Kim, H. L., & Han, J. (2019). Do employees in a “good” company comply better with information security policy? A corporate social responsibility perspective. *Information Technology & People*, 32(4), 858–875. <https://doi.org/10.1108/ITP-09-2017-0298>

Kline, P. (2014). *An easy guide to factor analysis*. Routledge.

Kong, S. C., Cheung, W. M. Y., & Zhang, G. (2022). Evaluating artificial intelligence literacy courses for fostering conceptual learning, literacy and empowerment in university students: Refocusing to conceptual building. *Computers in Human Behavior Reports*, 7, 100223. <https://doi.org/10.1016/j.caeai.2021.100026>

Kong, S. C., Cheung, W. M. Y., & Zhang, G. (2023). Evaluating an artificial intelligence literacy programme for developing university students' conceptual understanding, literacy, empowerment and ethical awareness. *Educational Technology and Society*, 26(1), 16–30. <https://doi.org/10.1016/j.jifjifmgt.2021.102433>

Lamb, R. L., Annetta, L., Firestone, J., & Ettoio, E. (2018). A meta-analysis with examination of moderators of student cognition, affect, and learning outcomes while using serious educational games, serious games, and simulations. *Computers in Human Behavior*, 80, 158–167. <https://doi.org/10.1016/j.chb.2017.10.040>

Laupichler, M. C., Aster, A., & Schirch, J. (2022). Artificial intelligence literacy in higher and adult education: A scoping. *Computers & Education: Artificial Intelligence*, 3, 100101. <https://doi.org/10.1016/j.caeai.2022.100101>

Law, N., Woo, D., de la Torre, J., & Wong, G. (2018). A global framework of reference on digital literacy skills for indicator 4.4.2. UNESCO Institute of Statistics. <http://uis.unesco.org/sites/default/files/documents/ip51-globa-l-framework-reference-digital-literacy-skills-2018-en.pdf>

Lazonder, A. W., Walraven, A., Gijlers, H., & Janssen, N. (2020). Longitudinal assessment of digital literacy in children: Findings from a large Dutch single-school study. *Computers & Education*, 143, 103681. <https://doi.org/10.1016/j.compud.2019.103681>

Lee, M. K., Kusibit, D., Kahng, A., Kim, J. T., Yuan, X., Chan, A., See, D., Noothigattu, R., Lee, S., Psomas, A., & Procaccia, A. D. (2019). WeBuildAI: Participatory framework for algorithmic governance. *Proceedings of the ACM on Human-Computer Interaction*, 3(CSCW), 1–35. <https://doi.org/10.1145/3359283>

Lemov, D. (2017, April 3). Bloom's taxonomy: That pyramid is a problem [Blog post]. <http://teachlikeachampion.com/blog/blooms-taxonomy-pyramidproblem/>

Lin, P. Y., Chai, C. S., Jong, M. S. Y., Dai, Y., Guo, Y., & Qin, J. (2021). Modeling the structural relationship among primary students' motivation to learn artificial intelligence. *Computers and Education: Artificial Intelligence*, 2, 100006. <https://doi.org/10.1016/j.caeai.2020.100006>

Lin, P., & Van Brummelen, J. (2021, May). Engaging teachers to co-design integrated AI curriculum for K-12 classrooms. In *Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems* (pp. 1–12), Yokohama, Japan. <https://doi.org/10.1145/3411764.344537>

Liu, C. J., & Yang, S. C. (2012). Applying the practical inquiry model to investigate the quality of students' online discourse in an information ethics course based on Bloom's teaching goal and Bird's 3C model. *Computers & Education*, 59(2), 466–480. <https://doi.org/10.1016/j.compedu.2012.01.018>

Long, D., & Magerko, B. (2020, April). What is AI literacy? Competencies and design considerations. In *Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems* (pp. 1–16). New York, USA. <https://doi.org/10.1145/3313831.3376727>

Lowyck, J., & Pöysä, J. (2001). Design of collaborative learning environments. *Computers in Human Behavior*, 17(5–6), 507–516. [https://doi.org/10.1016/S0747-5632\(01\)00017-6](https://doi.org/10.1016/S0747-5632(01)00017-6)

MacKenzie, S. B., Podsakoff, P. M., & Podsakoff, N. P. (2011). Construct measurement and validation procedures in MIS and behavioral research: Integrating new and existing techniques. *MIS Quarterly*, 293–334. <https://www.jstor.org/stable/23044045>

Marsh, H. W., Hau, K. T., & Wen, Z. (2004). In search of golden rules: Comment on hypothesis-testing approaches to setting cutoff values for fit indexes and dangers in overgeneralizing Hu and Bentler's (1999) findings. *Structural Equation Modeling*, 11(3), 320–341. [https://doi.org/10.1207/s15328007sem1103\\_2](https://doi.org/10.1207/s15328007sem1103_2)

Marsh, H. W., Lüdtke, O., Pekrun, R., Parker, P. D., Murayama, K., Guo, J., Basarkod, G., Dicke, T., Donald, J. N., & Morin, A. J. (2023). School leaders' self-efficacy and job satisfaction over nine annual waves: A substantive-methodological synergy juxtaposing competing models of directional ordering. *Contemporary Educational Psychology*, 73, 102170.

Marsh, H. W., Hau, K. T., & Wen, Z. (2004). In search of golden rules: Comment on hypothesis-testing approaches to setting cutoff values for fit indexes and dangers in overgeneralizing Hu and Bentler's (1999) findings. *Structural Equation Modeling*, 11(3), 320–341. [https://doi.org/10.1207/s15328007sem1103\\_2](https://doi.org/10.1207/s15328007sem1103_2)

Merzbacher, M. (2001, February). *Open artificial intelligence-one course for all*. In *Proceedings of the Thirty-Second SIGCSE Technical Symposium on Computer Science Education* (pp. 110–113). Carolina, USA. <https://doi.org/10.1145/364447.364554>

Microsoft. (2023). *Responsible and trusted AI*. <https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/work/innovate/best-practices/trusted-ai>

Mota-Valtierra, G., Rodríguez-Résendiz, J., & Herrera-Ruiz, G. (2019). Constructivism-based methodology for teaching artificial intelligence topics focused on sustainable development. *Sustainability*, 11(17), 4642. <https://doi.org/10.3390/su1174642>

Ng, W. (2012). Can we teach digital natives digital literacy? *Computers & Education*, 59(3), 1065–1078. <https://doi.org/10.1016/j.compud.2012.04.016>

Ng, D. T. K., & Chu, S. K. W. (2021). Motivating Students to Learn AI Through Social Networking Sites: A Case Study in Hong Kong. *Online Learning*, 25(1), 195–208. <https://eric.ed.gov/?id=E1287128>

Ng, D. T. K., Lee, M., Tan, R. J. Y., Hu, X., Downie, J. S., & Chu, S. K. W. (2023). A review of AI teaching and learning from 2000 to 2020. *Education and Information Technologies*, 28(7), 8445–8501. <https://doi.org/10.1007/s10639-022-11491-w>

Ng, D. T. K., Leung, J. K. L., Chu, K. W. S., & Qiao, M. S. (2021a). AI literacy: Definition, teaching, evaluation and ethical issues. *Proceedings of the Association for Information Science and Technology*, 58(1), 504–509.

Ng, D. T. K., Leung, J. K. L., Chu, S. K. W., & Qiao, M. S. (2021b). Conceptualizing AI literacy: An exploratory review. *Computers and Education: Artificial Intelligence*, 2, 100041.

Ng, D. T. K., Leung, J. K. L., Su, J., Ng, R. C. W., & Chu, S. K. W. (2023). Teachers' AI digital competencies and twenty-first century skills in the post-pandemic world. *Educational Technology Research and Development*, 71(1), 137–161. <https://doi.org/10.1007/s11423-023-10203-6>

Ng, D. T. K., Leung, J. K. L., Su, M. J., Yim, I. H. Y., Qiao, M. S., & Chu, S. K. W. (2023). AI literacy in K-16 classrooms. Springer International Publishing AG.

Ng, D. T. K., Luo, W., Chan, H. M. Y., & Chu, S. K. W. (2022). Using digital story writing as a pedagogy to develop AI literacy among primary students. *Computers and Education: Artificial Intelligence*, 3, 100054. <https://doi.org/10.1016/j.caeai.2022.100054>

Ng, D. T. K., Su, J., & Chu, S. K. W. (2023). Fostering secondary school students' AI literacy through making AI-driven recycling bins. *Education and Information Technologies*, 1–32. <https://doi.org/10.1007/s10639-023-12183-9>

Ng, D. T. K., Wu, W., Leung, J. K. L., & Chu, S. K. W. (2023, July). Artificial Intelligence (AI) literacy questionnaire with confirmatory factor analysis. In *2023 IEEE International Conference on Advanced Learning Technologies (ICALT)* (pp. 233–235). IEEE. <https://doi.org/10.1109/ICALT58122.2023.00074>

Park, S. Y., Nam, M. W., & Cha, S. B. (2012). University students' behavioral intention to use mobile learning: Evaluating the technology acceptance model. *British Journal of Educational Technology*, 43(4), 592–605. <https://doi.org/10.1111/j.1467-8535.2011.01229.x>

Pinski, M., & Benilan, A. (2023). AI literacy—towards measuring human competency in artificial intelligence. <https://scholarspace.manoa.hawaii.edu/handle/10125/102649>

Proulx, V. K. (1994). *Computer science vs computer literacy. Which to teach*. Acta Informatica Comenius University. <https://www.ccs.neu.edu/home/vkp/Papers/Acta94.pdf>

Rodríguez-de-Díos, I., Igartua, J. J., & González-Vázquez, A. (2016, November). Development and validation of a digital literacy scale for teenagers. In *Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality* (pp. 1067–1072) New York, USA.

Rogaten, J., Rienties, B., Sharpe, R., Cross, S., Whitelock, D., Lygo-Baker, S., & Littlejohn, A. (2019). Reviewing affective, behavioural and cognitive learning gains in higher education. *Assessment & Evaluation in Higher Education*, 44(3), 321–337. <https://doi.org/10.1080/02602938.2018.1504277>

Sakulkueakulsuk, B., Witton, S., Ngarmkajornwiwat, P., Pataranutaporn, P., Surareungchai, W., Pataranutaporn, P., & Subsoontorn, P. (2018, December). Kids making AI: Integrating machine learning, gamification, and social context in STEM education. In *IEEE International Conference on Teaching, Assessment, and Learning for Engineering (TALE)* (pp. 1005–1010). IEEE. Wollongong, Australia. <https://doi.org/10.1109/TALE.2018.8615249>

Searson, M., Hancock, M., Soehl, N., & Shepherd, G. (2015). Digital citizenship within global contexts. *Education and Information Technologies*, 20, 729–741. <https://doi.org/10.1007/s10639-015-9426-0>

Segun, S. T. (2021). From machine ethics to computational ethics. *AI & Society*, 36(1), 263–276. <https://doi.org/10.1007/s00146-020-01010-1>

Selwyn, N., & Gallo Cordoba, B. (2021). Australian public understandings of artificial intelligence. *AI & Society*, 1–18, 1645–1662. <https://doi.org/10.1007/s00146-021-01268-z>

Sims, R. R. (2002). Business ethics teaching for effective learning. *Teaching Business Ethics*, 6(4), 393–410. <https://doi.org/10.1023/A:1021107728568>

Song, S. H., & Keller, J. M. (1999). The ARCS model for the design of motivationally adaptive computer-mediated instruction. *Journal of Educational Technology*, 14(1), 119–134. <https://files.eric.ed.gov/fulltext/ED436181.pdf>

Stoeger, H., Duan, X., Schirner, S., Greindl, T., & Ziegler, A. (2013). The effectiveness of a one-year online mentoring program for girls in STEM. *Computers & Education*, 69, 408–418. <https://doi.org/10.1016/j.compoe.2013.07.032>

Su, J., Zhong, Y., & Ng, D. T. K. (2022). A meta-review of literature on educational approaches for teaching AI at the K-12 levels in the Asia-Pacific region. *Computers and Education: Artificial Intelligence*, 3, 100065. <https://doi.org/10.1016/j.caeai.2022.100065>

Sun, J., Ma, H., Zeng, Y., Han, D., & Jin, Y. (2023). Promoting the AI teaching competency of K-12 computer science teachers: A TPACK-based professional development approach. *Education and Information Technologies*, 28(2), 1509–1533.

Tinmaz, H., Lee, Y. T., Fanea-Ivanovici, M., & Baber, H. (2022). A systematic review on digital literacy. *Smart Learning Environments*, 9(1), 1–18.

Touretzky, D., Gardner-McCune, C., Martin, F., & Seehorn, D. (2019, July). Envisioning AI for K-12: What should every child know about AI? *Proceedings of the AAAI Conference on Artificial Intelligence*, 33(1), 9795–9799. <https://doi.org/10.1609/aaai.v33i1.33019795>

UNESCO. (2022). *K-12 AI curricula: A mapping of government-endorsed AI curricula*. <https://unesdoc.unesco.org/ark:/48223/pf0000380602>

van Harreveld, F., Nohlen, H. U., & Schneider, I. K. (2015). The ABC of ambivalence: Affective, behavioral, and cognitive consequences of attitudinal conflict. *Advances in Experimental Social Psychology*, 52, 285–324. <https://doi.org/10.1016/bs.aesp.2015.01.002>

Vrieze, S. I. (2012). Model selection and psychological theory: A discussion of the differences between the Akaike information criterion (AIC) and the Bayesian information criterion (BIC). *Psychological Methods*, 17(2), 228–243.

Waelen, R. (2022). Why AI ethics is a critical theory. *Philosophy & Technology*, 35(1), 1–16. <https://doi.org/10.1007/s13347-022-00548-w>

Wan, X., Zhou, X., Ye, Z., Mortensen, C. K., & Bai, Z. (2020, June). *SmileyCluster: Supporting accessible machine learning in K-12 scientific discovery*. In *Proceedings of the Interaction Design and Children Conference* (pp. 23–35). New York, USA. <https://doi.org/10.1145/3392063.3394440>

Wang, B., Rau, P. L. P., & Yuan, T. (2022). Measuring user competence in using artificial intelligence: Validity and reliability of artificial intelligence literacy scale. *Behaviour & Information Technology*, 1–14, 1324–1337. <https://doi.org/10.1080/0144929X.2022.2072768>

Wei, X., Saab, N., & Admiral, W. (2021). Assessment of cognitive, behavioral, and affective learning outcomes in massive open online courses: A systematic literature review. *Computers & Education*, 163, 104097. <https://doi.org/10.1016/j.compedu.2020.104097>

Williams, R., Ali, S., Devasia, N., DiPaola, D., Hong, J., Kaputsos, S. P., Jordan, B., & Breaeal, C. (2022). AI+ ethics curricula for middle school youth: Lessons learned from three project-based curricula. *International Journal of Artificial Intelligence in Education*, 1–59, 1–59. <https://doi.org/10.1007/s40593-022-00298-y>

Xia, Q., Chiu, T. K., Lee, M., Sanusi, I. T., Dai, Y., & Chai, C. S. (2022). A self-determination theory (SDT) design approach for inclusive and diverse artificial intelligence (AI) education. *Computers & Education*, 189, 104582. <https://doi.org/10.1016/j.compedu.2022.104582>

Xu, J., & Qiu, X. (2021). The influence of self-regulation on learner's behavioral intention to reuse E-learning systems: A moderated mediation model. *Frontiers in Psychology*, 12, 763889.

Yue, M., Jong, M. S. Y., & Dai, Y. (2022). Pedagogical design of K-12 artificial intelligence education: A systematic review. *Sustainability*, 14(23), 15620. <https://doi.org/10.3390/su142315620>

Yurdakul, I. K., & Çoklar, A. N. (2014). Modeling preservice teachers' TPACK competencies based on ICT usage. *Journal of Computer Assisted Learning*, 30(4), 363–376.

Zhang, H., Lee, I., Ali, S., DiPaola, D., Cheng, Y., & Breaeal, C. (2023). Integrating ethics and career futures with technical learning to promote AI literacy for middle school students: An exploratory study. *International Journal of Artificial Intelligence in Education*, 33(2), 290–324. <https://doi.org/10.1007/s40593-022-00293-3>

Zhang, Y., Wu, M., Tian, G. Y., Zhang, G., & Lu, J. (2021). Ethics and privacy of artificial intelligence: Understandings from bibliometrics. *Knowledge-Based Systems*, 222, 106994. <https://www.sciencedirect.com/science/article/abs/pii/S0950705121002574>

## SUPPORTING INFORMATION

Additional supporting information can be found online in the Supporting Information section at the end of this article.

**How to cite this article:** Ng, D. T. K., Wu, W., Leung, J. K. L., Chiu, T. K. F., & Chu, S. K. W. (2024). Design and validation of the AI literacy questionnaire: The affective, behavioural, cognitive and ethical approach. *British Journal of Educational Technology*, 55, 1082–1104. <https://doi.org/10.1111/bjet.13411>