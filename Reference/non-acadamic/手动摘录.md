# 借助 GPT-5.2 推动科学和数学发展 | OpenAI

展望未来  

2025年12月11日  

[刊发](https://openai.com/zh-Hans-CN/research/index/publication/) · [产品](https://openai.com/zh-Hans-CN/news/product-releases/)[1]

# 借助 GPT-5.2 推动科学和数学发展

GPT‑5.2 是我们目前在数学和科学方面表现最出色的模型。[1]

[阅读论文  
（在新窗口中打开）](https://cdn.openai.com/pdf/a3f3f76c-98bd-47a5-888f-c52c932a8942/colt-monotonicity-problem.pdf)  

播放页面文本音频  

分享[1]

我们对强大的人工智能抱有的愿景之一，是让它能够加速科研进展，惠及全人类，并帮助研究人员探索更多想法、更快速地验证假设，将发现转化为实际效益。[1]

在过去的一年里，我们与数学、物理、生物学和计算机科学等领域的科学家密切合作，以了解 AI 能在哪些方面发挥作用，以及在哪些方面仍存在不足。 上个月，我们[发布了一篇论文[1]
⁠](https://openai.com/zh-Hans-CN/index/accelerating-science-gpt-5/)，汇集了数学、物理、生物学、计算机科学、天文学和材料科学等领域的早期案例研究，展示了 GPT‑5 如何已经开始为科研工作做出贡献。 随着 [GPT‑5.2](https://openai.com/zh-Hans-CN/index/introducing-gpt-5-2/) 的推出，我们看到这些能力变得更稳定、更可靠。[1]

***

## 在高精度任务中的卓越表现

GPT‑5.2 Pro 和 GPT‑5.2 Thinking 是我们目前在科学和数学方面实力最强的模型。[1]

强大的数学推理能力是科学与技术工作可靠性的基础。 它使模型能够遵循多步骤逻辑、保持量纲一致，并避免那些在真实分析中可能不断累积的细微错误 — 从模拟与统计，到预测与建模。 在诸如 FrontierMath 这样的基准测试中的成绩提升，体现的不是单一技能的改进，而是更强的整体推理与抽象能力，这些能力会直接融入科学工作流程，例如编程、数据分析和实验设计。[1]

这些能力也与通用智能的发展紧密相连。 一个能够稳定地进行抽象推理、在长链思考中保持一致，并能跨领域泛化的系统，展现的正是 AGI 的核心特质：不是针对某个任务的技巧，而是广泛且可迁移的推理能力，可以真正影响科学、工程以及现实世界的决策。[1]

我们深信，GPT‑5.2 Pro 和 GPT‑5.2 Thinking 是目前最能支持并加快科研进展的模型。 在研究生级防 Google 问答基准测试 GPQA Diamond 中，GPT‑5.2 Pro 取得了 93.2% 的成绩，GPT‑5.2 Thinking 紧随其后，达到 92.4%。[1]

### 图表：GPQA 钻石级 科学问题（文字版）

图表标题：GPQA 钻石级 科学问题。[1]

- 横轴：准确性（0%、20%、40%、60%、80%、100%）。[1]
- 纵轴：模型名称（GPT‑5.2 Pro、GPT‑5.2 Thinking、GPT‑5.1 Thinking）。[1]
- 数据标签：
  - model: GPT-5.2 Thinking; accuracy: 0.92 → 92.4%。[1]
  - model: GPT-5.1 Thinking; accuracy: 0.88 → 88.1%。[1]
  - model: GPT-5.2 Pro; accuracy: 0.93 → 93.2%。[1]

文字呈现为：  

> GPQA 钻石级 科学问题  
> GPT-5.2 Pro / GPT-5.2 Thinking / GPT-5.1 Thinking  
> 0% 20% 40% 60% 80% 100% 准确性  
> 92.4% · 88.1% · 93.2%[1]

在 [GPQA Diamond  
⁠（在新窗口中打开）](https://arxiv.org/abs/2311.12022) 评测中，模型回答涉及物理、化学和生物的多项选择题。 此时未启用任何工具，但推理强度同样设置为最高。[1]

在专家级数学评测 FrontierMath (Tier 1–3) 中，GPT‑5.2 Thinking 树立了新的技术标杆，解决了 40.3% 的问题。[1]

### 图表：FrontierMath (Tier 1–3) 高等数学（文字版）

图表标题：FrontierMath (Tier 1–3) 高等数学。[1]

- 横轴：准确性（0%、10%、20%、30%、40%、50%）。[1]
- 纵轴：模型名称（GPT‑5.2 Thinking、GPT‑5.1 Thinking）。[1]
- 数据标签：
  - model: GPT-5.2 Thinking; accuracy: 0.40 → 40.3%。[1]
  - model: GPT-5.1 Thinking; accuracy: 0.31 → 31.0%。[1]

文字呈现为：  

> FrontierMath (Tier 1–3) 高等数学  
> GPT-5.2 Thinking / GPT-5.1 Thinking  
> 0% 10% 20% 30% 40% 50% 准确性  
> 40.3% · 31.0%[1]

在 [FrontierMath  
⁠（在新窗口中打开）](https://epoch.ai/frontiermath) 评测中，模型需要解决专家级的数学问题。 此时启用了 Python 工具，并将推理强度设置为最高。[1]

***

## 案例研究

GPT‑5.2 is not only strong at graduate-level science problems. We now regularly see our frontier models contributing solutions to previously unsolved—and increasingly subtle—questions in mathematics and the sciences.[1]

In this case study, we describe how GPT‑5.2 Pro helped resolve an open research problem in statistical learning theory, documented in a new paper, [On Learning-Curve Monotonicity for Maximum Likelihood Estimators  
⁠（在新窗口中打开）](https://cdn.openai.com/pdf/a3f3f76c-98bd-47a5-888f-c52c932a8942/colt-monotonicity-problem.pdf).[1]

The question (“If you collect more data, do your results reliably get better?”) shows up any time you fit a model from data. You can draw a learning curve that tracks average error as you add more examples. In the best case, the curve is monotone. More data means less error, every step of the way. That is the behavior people hope for, and often assume.[1]

But over the last few years, researchers have learned that this intuition can fail. A line of work kicked off by an open problem posed at the Conference on Learning Theory (COLT) in 2019 by Viering, Mey, and Loog showed that the answer is often no. Even very simple, well-behaved toy setups can have non-monotonic learning curves, where adding data increases expected error. That surprise triggered a wave of follow-up papers. They expanded the list of settings where these reversals happen and proposed increasingly elaborate methods designed to restore monotone behavior.[1]

Still, one of the most basic cases remained unresolved. What happens in the cleanest textbook situation, where the statistical model is actually correct and the data follow the familiar bell curve pattern, with a known mean but unknown standard deviation? Researchers already knew that small changes to this setup could break monotonic behavior. But the answer remained unknown in this core case.[1]

Our new paper demonstrates that in this clean setting, intuition prevails: learning is predictably improved by more data, rather than behaving in surprising or unstable ways. What makes this paper unusual is how the proof was obtained. The authors did not work out a strategy and then ask the model to fill in steps. They did not provide intermediate arguments or a proof outline. Instead, they asked GPT‑5.2 Pro to solve the open problem directly, and then carefully verified the proof, including review and validation by external subject-matter experts.[1]

The authors then asked simple follow-up questions to see how far the idea could go. GPT‑5.2 Pro extended the result beyond the original problem to higher dimensional settings and other common statistical models. Throughout, the human role stayed focused on verification and clear writing, rather than supplying mathematical scaffolding.[1]

***

## 展望未来

这一结果为 AI 系统如何支持科学研究指明了一个有价值的方向，尤其是在数学、理论计算机科学等具有公理化理论基础的领域。 在这些场景中，前沿模型能够协助探索证明、检验假设，并发现那些原本需要大量人力才能挖掘出的潜在联系。[1]

与此同时，这些系统本身并不是独立的研究者。 专家的判断、验证过程以及对领域的深入理解依然不可或缺。 即便是能力很强的模型，也可能出错，或依赖未被明确验证的假设。 但它们同样能够生成结构清晰、细节充分的论证，值得研究者认真审视和打磨。 因此，要让 AI 带来可靠的进展，就必须依靠强调验证、透明度与协作的工作流程。[1]

从案例研究的角度来看，这一结果展示了一种正在兴起的研究模式。 像 GPT‑5.2 这样的模型可以作为工具，支持数学推理并加速早期探索阶段，而正确性、解释和语境的责任仍由研究者承担。 在谨慎使用的前提下，这类系统有望简化理论研究中的重要环节，同时不会削弱人类判断在科学探究中的核心地位。[1]

***

## 标签

- [GPT](https://openai.com/research/index/?tags=gpt)  
- [推理与政策](https://openai.com/research/index/?tags=reasoning-policy)  
- [2025 年](https://openai.com/research/index/?tags=2025)[1]

***

## 作者

[OpenAI](https://openai.com/news/?author=openai#results)[1]



# 评估人工智能在科学研究任务中的能力 | OpenAI

2025年12月16日  

[研究](https://openai.com/zh-Hans-CN/news/research/) [刊发](https://openai.com/zh-Hans-CN/research/index/publication/)

## 评估人工智能在科学研究任务中的能力

我们推出 FrontierScience，这是一项全新的基准，用于衡量 AI 在物理、化学和生物等领域进行专家级科学推理的能力。[1]

[阅读论文  
（在新窗口中打开）](https://cdn.openai.com/pdf/2fcd284c-b468-4c21-8ee0-7a783933efcc/frontierscience-paper.pdf)[1]

> 局部图像：背景为柔和的绿黄渐变，右侧有一段被裁切的以 “Fro…” 开头的大字样，叠放着多个文字块，内容包括“Factual”、“Gradable”、“Objective”、“Difficult”等评审标准。[1]

播放页面文本音频  

分享  

推理能力是科学工作的核心。科学家不仅需要记住事实，更要提出假设、不断验证与修正，并在不同领域之间整合观点。随着模型能力不断提升，我们面临的关键问题是：它们如何进行更深入的推理，从而真正推动科学研究。[1]

过去一年里，我们的模型取得了重要突破，包括在国际数学奥林匹克和国际信息学奥林匹克中达到金牌水平。同时，我们也开始看到，最先进的模型（如 GPT‑5）正在大幅提升科研速度。研究人员利用这些系统跨学科、跨语言检索文献，或处理复杂的数学证明。在许多情况下，模型将原本需要数天甚至数周的工作压缩到数小时。相关进展记录在我们于 2025 年 11 月发布的论文[利用 GPT‑5 加速科学研究的早期实验  
⁠](https://openai.com/zh-Hans-CN/index/accelerating-science-gpt-5/)中，该论文展示了 GPT‑5 能够显著加快科研速度的初步证据。[1]

***

## 推出 FrontierScience

加快科研速度是人工智能造福人类最具潜力的方向之一。因此，我们持续提升模型在高难度数学与科学任务上的表现，并开发能帮助科研人员充分利用这些模型的工具。[1]

当由博士专家撰写、旨在“防谷歌搜索”的科学基准 [GPQA  
⁠  
（在新窗口中打开）](https://arxiv.org/abs/2311.12022) 于 2023 年 11 月发布时，GPT‑4 的得分为 39%，低于专家基线的 70%。两年后，GPT‑5.2 的得分为 92%。随着模型的推理与知识能力不断提升，更具挑战性的基准对于评估和预测模型加快科研速度的潜力变得愈发重要。此前的科学基准大多集中于多项选择题，而且已趋于饱和，或并未真正聚焦于科学本身。[1]

为弥补这一缺口，我们推出了 FrontierScience：一个用于衡量专家级科学能力的新基准。FrontierScience 由物理、化学、生物等领域的专家撰写并审核，包含数百道兼具难度、原创性与科学意义的问题。它分为两个方向：Olympiad（评估类似奥赛的科学推理能力）和 Research（评估真实科研场景中的研究能力）。更深入地了解模型的科学能力，有助于我们追踪进展，并实现 AI 为科学研究提速。[1]

在我们的初步评测中，GPT‑5.2 在 FrontierScience 的 Olympiad 方向（得分 77%）和 Research 方向（得分 25%）均取得了目前最好的成绩，领先其他前沿模型。我们看到，模型在解决专家级问题上进步明显，同时仍存在提升空间，尤其是在开放式、研究型任务上。对科学家而言，这意味着当前模型已经能够支持部分需要结构化推理的研究环节，但也凸显了在提升其开放式思考能力方面仍需大量工作。这些结果与科学家目前使用 AI 的方式高度吻合：利用模型有效缩短科研流程，同时依靠人类判断来界定问题并验证结果；越来越多地借助模型探索那些原本需要更长时间才能发现的思路与联系。这包括，在某些情况下由模型提出新见解，再由专家进行评估与验证。[1]

归根结底，衡量 AI 科学能力最重要的标准，是它能帮助产生多少新的科学发现；这才是对科学与社会真正重要的成果。FrontierScience 为专家级科学推理提供了一个“上游”参考点，让我们能够在标准化的问题集上测试模型、观察其成功与不足，并确定改进方向。当然，FrontierScience 在某些重要方面相对有限（例如主要聚焦于专家撰写的受限问题），无法覆盖科学家日常工作的全部内容。但科学领域确实需要更具难度、原创性和意义的基准，而 FrontierScience 正朝这一方向迈出重要一步。[1]

***

## FrontierScience 的评测内容与构建方式

FrontierScience 评测包含 700 多道文本题目（其中 160 道属于金集），涵盖物理、化学、生物的多个子领域。该基准由 Olympiad 与 Research 两个部分组成。FrontierScience-Olympiad 包含 100 道由国际奥赛奖牌获得者设计的问题，旨在通过受限的简答形式评估科学推理能力。该题集专门设计为至少达到国际奥赛难度的理论问题。FrontierScience-Research 则由博士科研人员（博士生、教授或博士后）设计的 60 个原创研究子任务构成，并采用 10 分制评分标准。这些任务旨在模拟博士科研人员在研究过程中可能遇到的、具有自洽性且需要多步推理的高难度问题。[1]

#### 示例问题

化学奥林匹克 物理奥林匹克 生物奥林匹克 化学研究 物理研究 生物研究[1]

**示例：生物研究任务文本**

> The engineering of multicellular organisms to exhibit programmable behaviors, such as sophisticated environmental sensing, computation, and tailored responses, represents a grand challenge in synthetic biology. Achieving this requires the stable integration of complex genetic payloads into host genomes and their subsequent dynamic, multi-input regulation. Fundamental biological principles underpinning gene expression, DNA replication and repair, intercellular communication, and cellular resource management must be meticulously considered to design robust and predictable synthetic systems. These systems often need to operate orthogonally to, yet sometimes interface with, the host's endogenous regulatory networks across various developmental stages and physiological conditions.  
>  
> A research team aims to engineer a model plant (e.g., Arabidopsis thaliana) to produce a novel, high-value, three-enzyme metabolic pathway (Enzyme A, Enzyme B, Enzyme C, which must act sequentially) leading to metabolite X. The production of metabolite X needs to be tightly controlled, only occurring when both an abiotic stress signal (e.g., elevated salinity, sensed by an endogenous stress-responsive promoter P\_stress) and the presence of a specific developmental cue (e.g., flowering, sensed by an endogenous flower-specific promoter P\_flower) are detected. Furthermore, once metabolite X is produced in a specific cell, this cell must signal to its immediate, non-transgenic neighbors to upregulate a generic defense gene (G\_defense) as a localized protective measure.  
>  
> Considering the fundamental principles of eukaryotic gene expression, genome stability, intercellular signaling, and metabolic engineering, propose and critically evaluate a comprehensive design strategy for such a system.[1]

FrontierScience 中的每一个任务均由物理、化学或生物领域的专家编写并审核。对于 Olympiad 题集，所有专家至少在一次（通常是多次）国际奥赛中获得过奖牌。对于 Research 题集，所有专家均拥有相关领域的博士学位。[1]

Olympiad 题目由 42 位相关领域的前国际奥赛奖牌获得者或国家队教练共同编写，编写者累计拥有 109 枚奥赛奖牌。Research 题目则由 45 位具备资质的科学家与领域专家合作完成。所有参与的科研人员均为博士生、博士后或教授，他们的专业背景覆盖一系列重要且高度专业化的科学学科，包括量子电动力学、有机合成化学和进化生物学等。[1]

在两个题集的构建过程中，我们都会根据 OpenAI 内部模型的表现进行一定筛选（例如，剔除模型已能正确回答的题目，因此相较其他模型，这些评测可能对内部模型略有不利偏向）。我们已开源 Olympiad 金集的 100 道题目和 Research 金集的 60 道题目，其余题目则留用于监测潜在的数据污染。[1]

> 流程图展示了任务开发流程的四个阶段 — 创建、审查、解决和修订。每个步骤都有简要说明，并附有“事实性”“可评分性”“客观性”“难度”等标准的勾选标记。[1]

任务开发经历四个阶段：创建、审查、解决和修订。独立专家会相互审查彼此的任务，以确保其符合既定标准。[1]

***

## 我们如何评估模型表现

Olympiad 题集可以通过简答形式评分：答案可以是数字、表达式，或通过模糊字符串匹配判断正确性。这种方式便于验证答案，但也会在一定程度上限制题目的表达空间与开放性。[1]

对于 Research 题集，我们采用基于评分细则 (rubric) 的架构来评估开放式任务。每道题都配有包含评分细则（由多个独立、可客观判断的评分项组成），总分为 10 分。评分细则不仅考察最终答案是否正确，也评估中间推理步骤的合理性，从而实现更细致的模型性能分析与错误诊断。若某个解答分数至少为 7/10，则被视为“正确”。[1]

模型的回答由基于模型的评分器 (GPT‑5) 进行评估，依据的是简答答案或评分细则。理想情况下，我们希望由人类专家逐题评分，但这种方式难以规模化，因此我们设计了可由模型评分器执行的评分细则。我们还开发了验证流程，以确保评分细则与题目在难度与正确性方面校准良好。[1]

##### Sample chemistry research problem

> Phthalocyanines are important macrocyclic compounds widely utilized in material and life sciences due to their outstanding physicochemical properties, including intense absorption in the visible-near-IR region and high electron-accepting ability. However, unlike porphyrins, the structural diversity of phthalocyanines has been limited by traditional synthetic methods that rely on one-step condensation of isoindoline units. The development of methodologies that allow controlled modification of the phthalocyanine framework, particularly at the meso-positions, represents a significant advancement in expanding the structural and electronic diversity of these compounds.  
>  
> Analyze how the modification of meso-nitrogen atoms in nickel(II) phthalocyanine affects the electronic structure, aromaticity, spectroscopic properties, and reactivity of the resulting macrocycles. Explain the synthetic methodology that enables these modifications, the relationship between π-electron count and observed properties, and how these systems compare to traditional phthalocyanines and related porphyrinoids.[1]

##### Sample response

**1. Introduction: Why Modify the meso-Nitrogens of Phthalocyanines?**[1]

Phthalocyanines (Pc) are a class of macrocyclic dyes known for their remarkable chemical, electronic, and optical properties. Their widespread use in materials science, catalysis, and biology stems from strong absorptions in the visible/near-IR regions, exceptional redox characteristics (i.e., high electron affinity), and rigid π-conjugation.  
Traditional synthetic methods, however, typically form the macrocycle in a single step from phthalonitrile or isoindoline precursors, restricting the ability to introduce substitutions selectively at key ring positions-especially the "meso"  
(bridging) nitrogens. Recently, methodologies allowing targeted modification of the meso-nitrogen atoms in Ni(II) phthalocyanines have emerged as a powerful means to expand the structural and electronic diversity of these macrocycles.[1]

**2. Effect on Electronic Structure and Aromaticity**[1]

- a.  
  **π-Electron Count and Ring Currents**  

  Phthalocyanines are classical 18-electron aromatic systems, conferring high stability and planarity. Altering the meso-nitrogens through substitution or ring expansion can shift the effective π-electron count. For example, introducing additional unsaturated moieties or heteroatoms may expand the cyclic conjugation to 20 or 22 electrons, moving the molecule between Hückel aromaticity and other regimes that affect ring currents and magnetic properties.[1]

- b.  
  **Charge Distribution and Frontier Orbitals**  

  Meso-substitution can tune the electron density distribution around the macrocycle. Electron-donating groups can push electron density toward the ring center, lowering the energy of the lowest unoccupied molecular orbital (LUMO). Electron-withdrawing groups, conversely, stabilize the highest occupied molecular orbital (HOMO) and shift redox potentials toward more positive values, altering both the electrochemical profile and the Q- and B-band positions in the UV-vis spectrum.[1]

**3. Spectroscopic Consequences**[1]

- a.  
  **UV-Vis Absorption (Q and B Bands)**  

  The principal absorption features of phthalocyanines lie in the visible (Q-band, typically 600-700 nm) and near-UV (B-band, typically 300-400 nm).  

  Substitution that expands the ring conjugation or introduces strong electron-donating/withdrawing groups can:

  - Shift the Q-band to longer wavelengths (bathochromic shift), reaching into the near-IR, which is highly desirable for optoelectronic and photodynamic applications.  
  - Alter relative intensities of these bands and merge or split them, reflecting changes in orbital symmetries and energies.[1]

- b.  
  **NMR Spectroscopy and Aromatic Ring Currents**  

  Modifications to the π-electron count and distribution are directly observed in 1H and 13C NMR chemical shifts.  

  More highly conjugated (or expanded) aromatic rings exhibit distinct downfield shifts for protons located within induced ring currents, while any partial loss of aromaticity or incorporation of antiaromatic segments can cause atypical shielding/deshielding patterns.[1]

**4. Reactivity and Coordination Chemistry**[1]

Because phthalocyanines are often used as redox catalysts or sensors, the meso-nitrogen modifications can significantly influence reactivity:

- Electron-rich meso substituents facilitate nucleophilic or electrophilic attacks at the ring periphery, enabling site-selective functionalizations that are otherwise difficult.[1]

(... shortened for the purposes of this figure)[1]

##### Sample grading rubric

###### Analysis of Traditional Phthalocyanine Synthesis Limitations (1 point)

通过  
1.0 point: Correctly explains that traditional phthalocyanine synthesis involves one-step condensation with simultaneous formation of all meso-nitrogen bridges, providing limited control over substitution patterns at these positions.  

0.5 point: Mentions limitations of traditional methods but without specific focus on meso-position control challenges.  

0.0 point: Fails to identify key limitations of traditional synthetic approaches or provides incorrect analysis.[1]

###### Thiolate-Mediated Tetramerization Process (1 point)

1.0 point: Correctly describes the thiolate-mediated reductive tetramerization and explains how counter cation size (K+ or Cs+ vs. Na+) affects selectivity between tetramer formation and direct macrocyclization.  

0.5 point: Mentions thiolate-mediated tetramerization but without explaining factors controlling selectivity.  

失败  
0.0 point: Incorrectly describes the oligomerization process or omits critical details about selectivity control.[1]

###### Analysis of NMR Spectroscopic Features (1 point)

1.0 point: Correctly explains that upfield shifts in the 16π system indicate paratropic ring current (antiaromaticity), contrasts this with the broad signals in 17π systems due to paramagnetism, and connects these observations to the underlying electronic structures.  

通过  
0.5 point: Identifies basic NMR patterns but without clear connection to ring currents or electronic structure.  

0.0 point: Incorrectly interprets NMR data or fails to connect spectral features to electronic properties.[1]

###### Electrochemical Property Analysis (1 point)

1.0 point: Correctly explains that the 16π system shows two reversible reductions reflecting conversion to 17π radical and 18π aromatic states, while 17π systems show narrow redox gaps due to facile interconversion between 16π, 17π, and 18π states, and relates these patterns to the underlying electronic structures.  

通过  
0.5 point: Describes redox patterns without clearly connecting them to specific electronic state changes.  

0.0 point: Incorrectly interprets electrochemical data or fails to connect redox behavior to electronic properties.[1]

###### Analysis of Absorption Spectroscopy (1 point)

1.0 point: Correctly explains that the 16π system shows weak/broad absorption due to symmetry-forbidden HOMO-LUMO transitions in antiaromatic systems, while 17π systems show Q-like bands plus NIR-II absorptions characteristic of radical species, and contrasts these with typical phthalocyanine spectral features.  

通过  
0.5 point: Describes absorption features but provides limited connection to underlying electronic structures.  

0.0 point: Incorrectly interprets absorption data or fails to relate spectral features to electronic properties.[1]

###### Reactivity Analysis of Antiaromatic System (1 point)

1.0 point: Correctly explains the high reactivity of the 16π system toward nucleophiles, details specific reactions with hydroxide (ring opening) and hydrazine (ring expansion), and explains how these transformations relieve antiaromatic destabilization.  

0.5 point: Mentions reactivity but provides limited analysis of specific transformations or the driving forces behind them.  

失败  
0.0 point: Incorrectly analyzes reactivity patterns or fails to connect them to the antiaromatic character of the 16π system.[1]

(... and more)[1]

Research 题集中的每个任务都采用总分 10 分的评分细则，由专家或模型评分器使用。为了扩大评估规模，我们使用另一模型对回答进行评分。[1]

***

## 模型性能

我们在 FrontierScience-Olympiad 和 FrontierScience-Research 上评测了多款前沿模型：GPT‑5.2、Claude Opus 4.5、Gemini 3 Pro、GPT‑4o、OpenAI o4-mini 和 OpenAI o3。除 GPT‑5.2 以 “xhigh” 推理强度运行外，其余推理模型均以 “high” 推理强度评测。在我们的初步评测中，GPT‑5.2 在 FrontierScience 的 Olympiad 方向（得分 77%）和 Research 方向（得分 25%）均取得了目前最好的成绩，领先其他前沿模型。Gemini 3 Pro 在 Olympiad 部分的表现与 GPT‑5.2 接近 (76%)。[1]

我们观察到，模型在解决专家级问题方面取得了显著进展，尤其是在开放式研究类任务上，但仍有提升空间。从失败案例的记录来看，前沿模型仍会出现推理、逻辑或计算错误，对某些小众科学概念理解不足，或产生事实性错误。[1]

**不同模型在 FrontierScience-Olympiad 上的准确率**（图表用文字表示）：[1]

- GPT-4o；accuracy\_pct: 12.3；下限: 10.9；上限: 13.7  
- OpenAI o1；accuracy\_pct: 42.5；下限: 40.3；上限: 44.7  
- OpenAI o4-mini；accuracy\_pct: 61.65；下限: 59.5；上限: 63.8  
- OpenAI o3；accuracy\_pct: 62.95；下限: 60.8；上限: 65.1  
- Grok 4；accuracy\_pct: 66.2；下限: 64.1；上限: 68.3  
- Claude Opus 4.5；accuracy\_pct: 71.35；下限: 69.4；上限: 73.3  
- Gemini 3 Pro；accuracy\_pct: 76.1；下限: 74.2；上限: 78.0  
- GPT-5.2；accuracy\_pct: 77.1；下限: 75.3；上限: 78.9  

（图中以柱状条显示各模型准确率及 95% 置信区间。）[1]

**不同模型在 FrontierScience-Research 上的准确率**（图表用文字表示）：[1]

- GPT-4o；accuracy: 0.44（约 0.4%）；下限: 0.1；上限: 0.8  
- OpenAI o1；accuracy: 3.17（约 3.2%）；下限: 2.4；上限: 4.0  
- OpenAI o4-mini；accuracy: 8.33（约 8.3%）；下限: 7.1；上限: 9.6  
- Gemini 3 Pro；accuracy: 12.44（约 12.4%）；下限: 10.9；上限: 14.0  
- OpenAI o3；accuracy: 14.06（约 14.1%）；下限: 12.4；上限: 15.7  
- Grok 4；accuracy: 15.94（约 15.9%）；下限: 14.3；上限: 17.6  
- Claude Opus 4.5；accuracy: 17.5（约 17.5%）；下限: 15.7；上限: 19.3  
- GPT-5.2；accuracy: 25.17（约 25.2%）；下限: 23.2；上限: 27.2  

我们比较了多款前沿模型的准确率，其中 GPT‑5.2 在 FrontierScience-Research 和 Olympiad 两个部分均表现最佳。[1]

**不同推理强度下的 FrontierScience-Olympiad 准确率**（图表用文字表示）：[1]

GPT‑5.2：

- Low；accuracy: 67.45（约 67.5%）；下限: 65.4；上限: 69.5  
- Medium；accuracy: 71.85（约 71.9%）；下限: 69.9；上限: 73.8  
- High；accuracy: 73.75（约 73.8%）；下限: 71.8；上限: 75.7  
- XHigh；accuracy: 77.1（约 77.1%）；下限: 75.3；上限: 78.9  

OpenAI o3：

- Low；accuracy: 53.4（约 53.4%）；下限: 51.2；上限: 55.6  
- Medium；accuracy: 60.45（约 60.5%）；下限: 58.3；上限: 62.6  
- High；accuracy: 62.95（约 62.9%）；下限: 60.8；上限: 65.1  

**不同推理强度下的 FrontierScience-Research 准确率**（图表用文字表示）：[1]

GPT‑5.2：

- Low；accuracy: 18.22（约 18.2%）；下限: 16.4；上限: 20.0  
- Medium；accuracy: 20.17（约 20.2%）；下限: 18.3；上限: 22.0  
- High；accuracy: 21.22（约 21.2%）；下限: 19.3；上限: 23.1  
- XHigh；accuracy: 25.17（约 25.2%）；下限: 23.2；上限: 27.2  

OpenAI o3：

- Low；accuracy: 12.89（约 12.9%）；下限: 11.3；上限: 14.4  
- Medium；accuracy: 14.39（约 14.4%）；下限: 12.8；上限: 16.0  
- High；accuracy: 14.06（约 14.1%）；下限: 12.4；上限: 15.7  

我们对 GPT‑5.2 和 o3 在不同推理强度下的准确率进行了比较。延长思考时间可以带来更高的准确率。[1]

***

## 局限性与未来方向

尽管 FrontierScience 在科学基准的难度上迈出了重要一步，但仍存在不少局限。FrontierScience 的题目采用受限的问题形式，主要关注最终答案的正确性 (Olympiad) 或完成研究任务的推理过程 (Research)。此外，对于篇幅较长的任务，使用包含多个评分项的细则，其客观性仍不如直接核对最终答案。[1]

FrontierScience 能够更清晰地呈现模型在专家编写的高难度问题上的推理表现，但它并不能全面反映科学研究在现实中的运作方式。尤其是，它并未评估科研中至关重要的一部分：模型如何提出真正新颖的假设，或如何处理多模态信息，包括视频数据以及物理世界中的真实实验系统。[1]

展望未来，我们预计科学推理能力的提升将来自两方面：更强的通用推理系统，以及更专注的科学能力改进。FrontierScience 只是众多工具之一。随着模型不断进步，我们计划持续迭代这一基准，将其扩展到更多领域，并与更贴近现实的评测结合，关注这些系统在实际科研中真正能帮助科学家完成的工作。像 FrontierScience 这样的基准有助于我们识别当今 AI 系统的不足，从而聚焦于如何让模型成为科学探索中的可靠合作伙伴。[1]
