

![Elsevier logo featuring a tree and two figures.](935eed7aa61f7777f62cfc032e11bee9_img.jpg)

Elsevier logo featuring a tree and two figures.

ELSEVIER

Contents lists available at ScienceDirect

Journal of Business Research

journal homepage: [www.elsevier.com/locate/jbusres](http://www.elsevier.com/locate/jbusres)![Journal of Business Research cover image.](0538daaa5583c23e17db3a12f2281a55_img.jpg)

Journal of Business Research cover image.

# A taxonomy framework and process model to explore AI-enabled workplace inclusion\*

Alessandra Lazazzara<sup>a,\*</sup>, Stefano Za<sup>b</sup>, Andri Georgiadou<sup>c</sup><sup>a</sup> University of Milan, Milan, Italy<sup>b</sup> University "G. D'Annunzio" of Chieti-Pescara, Pescara, Italy<sup>c</sup> Nottingham University Business School, Nottingham, UK

## ARTICLE INFO

### Keywords:

AI-enabled workplace inclusion  
Algorithmic human resource management  
Conjoined agency  
Mitigation strategies  
Technology integration

## ABSTRACT

This study develops a taxonomy framework and a process model to explain how artificial intelligence (AI) re-shapes workplace inclusion through human resource management (HRM) practices. We analyze 25 empirical studies using a hybrid inductive-deductive method informed by Nickerson et al.'s (2013) taxonomy development framework. The resulting taxonomy classifies AI-enabled HRM practices according to their strategic goals, types of human-AI interaction, inclusion typologies, evaluation methods, and mitigation strategies. We extend this taxonomy with a process model that illustrates how different forms of AI agency – ranging from assisting to automating – shape inclusion outcomes and require differentiated mitigation strategies. Our analysis reveals three interconnected dimensions of AI-enabled workplace inclusion emerge in such contexts: inclusion in work (individual experiences), inclusion at work (organizational climate), and inclusion of work (human-AI interaction). Each dimension demands distinct context-sensitive mitigation strategies depending on the level AI agency involved. By linking AI agency to differentiated forms of inclusion and tailored mitigation strategies, this study advances theoretical understanding of AI-enabled inclusion. It also offers actionable guidance for organizations implementing AI in HRM practices while safeguarding workplace inclusion.

## 1. Introduction

The intersection of artificial intelligence (AI) and workplace inclusion is an area that is critical yet underexplored in contemporary organizational research. Despite the anticipated improvements in how organizations manage their staff, research suggests that AI tools in human resources management (HRM) often strengthen existing biases rather than eliminate them. These systems may reproduce existing human prejudices which lead to increased marginalization of already excluded groups (Kelam, 2023; Kellogg et al., 2020; von Krogh, 2018), thus pushing marginalized groups further away from workplace consideration (Ferrer et al., 2021).

The proper analysis of AI impact on workplace inclusion requires defining diversity and inclusion as distinct yet connected concepts. Workforce diversity represents organizational composition and demographic characteristics (Peretz, Levi, and Fried 2015), yet inclusion extends beyond legal requirements to include voluntary international efforts (Winters, 2013). Diversity is therefore centered more around the

"what" – namely, different identities and perspectives – while inclusion concerns "how" these differences are integrated and celebrated in the workplace. Creating an inclusive workplace involves fostering the belonging of all employees, regardless of social identity group affiliations, by establishing a culture of access and accessibility (Georgiadou et al., 2024). To this end, it is necessary to implement managerial and organizational practices offering fair access to valuable opportunities (Bell et al. 2011; Roberson 2006).

Despite substantial advances in inclusion research, the impact of AI on inclusion efforts remains largely unexamined (Kim et al., 2021; Georgiadou et al., 2024). The existing literature focuses narrowly on recruitment and minority demographics, overlooking the broader organizational and technological factors that influence both positive and negative inclusion outcomes (Hunkenschoer & Luetge 2022; Köchling & Wehner 2020; Will, Krpan, & Lordan 2022). Moreover, the current models are inadequate in capturing the influence of AI on inclusion across the different levels of individual experiences, organizational aspects and human-AI interactions. Beyond specific HRM practices such as

\* This article is part of a special issue entitled: 'AI and Algorithmic HRM' published in Journal of Business Research.

\* Corresponding author.

E-mail addresses: [alessandra.lazazzara@unimi.it](mailto:alessandra.lazazzara@unimi.it) (A. Lazazzara), [stefano.za@unich.it](mailto:stefano.za@unich.it) (S. Za), [andri.georgiadou@nottingham.ac.uk](mailto:andri.georgiadou@nottingham.ac.uk) (A. Georgiadou).

hiring (Kelam, 2023), the concept of AI-enabled workplace inclusion remains theoretically underdeveloped. By AI-enabled workplace inclusion, we refer to the systematic integration of AI technologies within organizational processes to foster workplace inclusion across multiple levels, from individual experiences to organizational structures.

To address these gaps and contribute to theory development on the impact of AI on workplace inclusion, our study develops a novel taxonomy and a process model. The taxonomy offers a systematic classification of AI-enabled HRM practices, focusing on their strategic goals, types of human-AI interaction, inclusion typologies, evaluation methods, and mitigation strategies. It is developed using a hybrid inductive-deductive method informed by Nickerson et al.'s (2013) taxonomy development framework. The purpose is to structure the fragmented empirical landscape, providing researchers and practitioners with a coherent lens through which to interpret the implications of AI for workplace inclusion. Our taxonomy is then extended through a process model that maps how different types of AI agency (from assisting to automating) affect inclusion outcomes and the mitigation strategies required at each stage. The two provide a framework for analyzing how AI adoption shapes workplace inclusion, offering a common language for researchers and practitioners and enabling the identification of patterns not apparent when examining individual cases. This framework serves as a foundation for redirecting future research, helping to address AI-related biases in ways traditional frameworks cannot and push research into this underexplored arena (Cronin and George 2023).

This study advances theory on AI-enabled workplace inclusion through three interconnected contributions. First, we reconceptualize inclusion through a multilevel framework that distinguishes between individual experiences (inclusion in work), organizational climate (inclusion at work), and human-AI interaction (inclusion of work). This reconceptualization reveals how different dimensions of inclusion require fundamentally different mitigation strategies and cannot be addressed through universal approaches. The framework moves beyond binary questions of whether AI helps or hinders inclusion to examine how different forms of conjoined agency create distinct inclusion challenges that demand tailored responses.

Second, we demonstrate how the level of AI agency fundamentally shapes inclusion outcomes in ways that current literature has failed to recognize. Our taxonomy reveals that the progression from assisting to automating AI creates escalating inclusion risks that require increasingly sophisticated mitigation strategies. This finding challenges prevalent assumptions that AI implementations can be evaluated independently of their agency characteristics and suggests that inclusion outcomes are intrinsically linked to the degree of autonomy organizations delegate to their AI systems.

Third, we provide the first systematic framework for matching mitigation strategies to specific AI implementations, offering both theoretical insight and practical guidance for navigating the complex terrain between algorithmic efficiency and inclusive outcomes. Rather than advocating for or against AI adoption, our framework enables organizations to implement AI systems thoughtfully, with clear understanding of inclusion implications and concrete strategies for promoting positive outcomes across multiple organizational levels.

## 2. AI and HRM

### 2.1. Conjoined human-AI agency in HRM

AI, encompassing a range of different technologies, is defined as “the ability of machines to perform human-like cognitive tasks, including the automation of physical processes such as manipulating and moving objects, sensing, perceiving, problem solving, decision making and innovation” (Benbya et al., 2020, p. 9). The increasing data generation has resulted in the application of AI in various industries and jobs (Pachidi et al. 2021; Shobana & Kumar 2015; Collins et al. 2021).

Our AI-enabled inclusion taxonomy is centered on two primary

HRM-relevant technologies: Machine Learning (ML) and Deep Learning (DL). ML, which learns from training data to perform specific tasks (Benbya et al., 2020), is the most widely used AI technique in HRM (Sofian et al., 2022). DL, a specific class of ML, uses artificial neural networks to learn and make decisions without human supervision. The two technologies are the foundation of the “AI Technology” dimension of our taxonomy, which classifies HRM practices according to the type of AI technology applied.

In view of this, it is imperative to discuss the technology as well as the human-AI interactions in the organizational context in order to understand AI-enabled workplace inclusion. This interaction is crucial for understanding how AI influences workplace inclusion. For our analysis of how AI influences workplace routines and decisions, we use Murray et al.'s (2021) “conjoined agency” concept, defined as “a shared capacity between humans and nonhumans to exercise intentionality” (Murray et al., 2021, p. 555). Organizational routines consist of two key components: protocol development, which involves establishing activity guidelines, and action selection, which entails making decisions and interactions that impact the routine (Piezunka & Dahlander 2019). Viewing these components as the locus of agency, Murray et al. (2021) explore the shared agency between humans and agentic technologies (e.g., AI applications), observing how these technologies “shift the locus of agency away from humans in protocol development and action selection” (p. 555).

Based on technology's role in conjoined agency and its ability to act intentionally in protocol development and action selection, we identified four different forms of technology for the “human-AI interaction” dimension of our taxonomy: assisting AI technologies, which support human decision-making without exercising independent agency; arresting AI, which automatically executes tasks under specific conditions which are then verified, allowing humans to develop protocols but imposing constraints on routine practices; augmenting AI, which develops protocols or recommendations but leaves the final decisions to humans; and automating AI, which develops protocols and makes decisions independently. The impacts of AI on inclusion can be analyzed following this classification. For example, assisting AI may standardize initial candidate screenings and reduce bias in recruitment, while, without careful oversight, automating AI could perpetuate existing biases. Regarding arresting technologies (e.g., blockchain in smart contract management), such applications are rare in HRM, as AI-imposed constraints typically operate alongside protocol development in automating technology.

The spectrum of AI involvement in HRM tasks ranges from enhancement to full automation, directly influencing inclusion outcomes. Understanding these different levels of conjoined agency helps analyze how different forms of AI affect inclusive practices and outcomes in organizations. Given AI's applicability across organizational functions and decision-making tasks (Huang & Rust 2018), it is crucial to undertake a granular analysis of AI's effects on workers and HRM practices (Benbya et al., 2021).

The intersection of AI and HRM represents a critical juncture for understanding technology's impact on workplace inclusion. To develop an integrated concept of AI-enabled workplace inclusion, we need to examine how AI technologies are specifically applied to HRM practices and the resulting implications for inclusivity.

The increased digitalization and datafication of the HRM function have boosted the development and application of AI tools in HRM decisions, processes and practices (Kim, Wang & Boon 2021). The topic has received growing scholarly interest and it is raising new questions on the opportunities and risks related to the growing use of AI in HRM (Cheng & Hackett 2021). Indeed, an increasing number of organizations are adopting AI-powered HRM solutions, where a broad range of software algorithms execute HRM activities that would normally require human cognition and intervention (Kim et al., 2021; Za et al., 2023).

Previous research has indicated potential advantages of AI in HRM, including enhanced efficiency and data-driven decision-making.

However, deeper scrutiny has uncovered substantial obstacles, particularly regarding workplace inclusion (Georgiadou et al., 2024). For example, several studies demonstrate that AI recruitment platforms might perpetuate and even intensify existing prejudices, potentially worsening discrimination faced by marginalized groups (Köchling & Wehner, 2020; Köchling et al., 2021). This paradox – the notion that AI simultaneously improves HRM practices while potentially undermining workplace inclusion – creates a fundamental tension within the integrated concept of AI-enabled workplace inclusion.

Contemporary applications of advanced AI systems in HRM reveal the complexity of inclusion challenges that organizations now face. Beyond the chatbots handling routine queries from job applicants or employees seeking policy information, some organizations are now implementing emotion-detecting video interview platforms that promise to reduce interviewer bias but could systematically disadvantage, for example, candidates whose emotional expressions differ from predominantly Western training datasets (Rhue, 2018; Köchling et al., 2021). Automated performance management systems aggregate productivity metrics, communication patterns, and task completion rates to generate ratings that could influence promotion decisions, yet these potentially undervalue employees whose contributions might be more collaborative or contextual in nature (Newman et al., 2020; Tong et al., 2021). ML algorithms now recommend employees for promotion opportunities and personalize career development pathways, however these frequently rely on historical advancement patterns that embed past biases whilst appearing objective (for a review, see Vrontis et al., 2022).

Therefore, it is of paramount importance that AI systems do not replicate or amplify discriminatory behaviors (Mehrabli et al. 2021). Indeed, when applied to specific problems, AI algorithms could worsen the situation by threatening rights, opportunities and wealth, not only the creating new inequalities but also amplifying existing ones (Hoffmann 2019). Moreover, as a large number of organizations are increasingly adopting AI applications, the decisions of such systems could simultaneously influence many people, increasing the scope of potential problems related to ethics, fairness and algorithmic bias (Zuiderwijk, Chen & Salem 2021) and undermining organizational inclusion.

### 2.2. AI and workplace inclusion

The scope of inclusion has evolved from its initial focus on enhancing the demographic representation of underrepresented groups in recruitment and retention (Fly & Thomas, 2001) to addressing contemporary challenges of digital discrimination and algorithmic bias (Ferrer et al., 2021). Hence, organizations must now balance traditional inclusion factors with emerging technological facets (Georgiadou et al., 2024).

The current state of AI deployment in HRM reflects a kind of inclusion blindness; an implementation of advanced technological solutions without adequate consideration of their differential impacts on workplace inclusion. This blindness manifests across multiple domains of contemporary practice. For example, HR chatbots designed to handle employee queries about various institutional approaches and advancement opportunities, often struggle with non-native speakers or employees less comfortable with digital interfaces, effectively creating a two-tier system of HR service delivery. Also, ML algorithms that advise employees for promotion opportunities or assess their potential, frequently rely on historical advancement patterns that reflect past biases, essentially automating discrimination whilst appearing objective and data-driven (Speer, 2021; Zhang et al., 2023).

Scholars have established that diversity alone cannot guarantee positive organizational outcomes without well-designed inclusion management approaches (Joshi et al., 2011; Williams & O'Reilly, 1998). While diversity emphasizes demographic representation, inclusion focuses on employee involvement and integrating diversity into HRM practices to optimize HR and enhance employee contributions (Georgiadou & Antonacopoulou, 2021; Roberson, 2006).

The recent academic discourse has emphasized integrating AI to foster inclusion and equity, building on organizational inclusion frameworks (Budhwar et al., 2022; Robert et al., 2020; Georgiadou et al., 2024). However, by reflecting biased data, these technologies often perpetuate historical biases (Angrave et al., 2016; Eubanks, 2018). Successful implementation therefore requires an understanding of both the social attribution processes and technological structures shaping the inclusion outcomes (Georgiadou et al., 2024).

Perhaps most concerning is how these technologies reshape the fundamental nature of workplace inclusion itself. When promotion decisions are mediated by algorithms that prioritize merely quantifiable achievements, employees whose contributions might be more interactive or contextual may find themselves repeatedly underrated (Tong et al., 2021). The concept of inclusion “of work” – that is, how humans and AI collaborate in shaping inclusive workplaces – becomes critical here, as these systems don’t merely affect individual experiences but fundamentally define the terrain on which inclusion is challenged and realized. This transformation requires attention to both the technical capabilities of AI systems and how individuals within organizations interpret and interact with these technologies. In line with Kossek and Pichler’s approach (2006), we propose that effective inclusion management in AI-enabled contexts must address selecting for inclusion, reducing workplace discrimination and enhancing financial performance, but these traditional goals now require new strategies adapted to algorithmic mediation.

Nishii et al.’s (2008) attribution framework revealed how employees’ perceptions and behaviors are shaped by their interpretations of the management’s HRM practices. These attributions become very important in AI-enabled HRM practices, particularly when staff face unfamiliar or potentially threatening changes. Newman et al. (2020) found that employees view AI-based HRM decisions as less fair than those made by humans, especially when they believe AI has been implemented to cut costs rather than to enhance fairness.

As Mor Barak (2010) emphasized, the organizational context drastically affects workplace inclusion outcomes. Employees’ perceptions of AI systems vary based on the existing inclusion climate, leadership communication and past technological experiences. The institutional context frames what the organizations value (Georgiadou & Syed, 2021) and provides legitimizing narratives (Baumeister et al., 2013).

The technological perspective of adaptive structuration theory (AST; DeSanctis & Poole, 1994) explains how cognitive, social and cultural dynamics shape human-AI interaction. How effective the adoption of technology is, depends on how people utilize the existing norms and communication processes (Leonardi & Barley, 2010). This sociological framework suggests that identical AI systems may produce different inclusion outcomes depending on the organizational context and employee attributions.

The synthesis of attribution theory and AST demonstrates that the successful implementation of AI requires attention to both technical capabilities and social interpretations. When employees perceive algorithmic decision-making in HRM, their satisfaction and commitment may decrease even in the face of objectively fair outcomes (Newman et al., 2020). This highlights the necessity of managing both the technical and social aspects of AI implementation to promote genuine workplace inclusion.

Recent management scholarship has highlighted the paradoxical relationship between automation and augmentation in AI applications. Raisch and Krakowski (2021) argue that rather than representing a simple trade-off, automation and augmentation exist in a paradoxical tension where they are both contradictory and interdependent. Whilst automation removes humans from the process, augmentation keeps them ‘in the loop’, creating opposing organizational demands. However, these approaches also enable one another across time and space. This paradox proves particularly relevant when examining how different forms of AI agency create distinct inclusion challenges. Assisting AI technologies may appear to preserve human agency whilst subtly

shaping decision-making through the information they surface or suppress. Augmenting AI systems create tensions between algorithmic recommendations and human judgment, often in contexts where the basis for algorithmic suggestions remains obscure to decision-makers. Automating AI presents perhaps the utter paradox – promising to eliminate human bias whilst potentially embedding systematic exclusion that becomes invisible within apparently objective processes.

Our approach extends recent taxonomic work in AI research, particularly frameworks developed within innovation contexts. *Mariani et al.* (2023) provided a comprehensive taxonomy of AI applications in innovation research, whilst *Gama and Magistretti* (2025) developed taxonomic approaches for AI in innovation management more broadly. However, these frameworks focus primarily on technological capabilities and innovation outcomes, without addressing the inclusion implications that prove central to AI enabled workplace. Our work redirects attention toward the inclusion dimensions these studies overlook and categorizes AI by its impact on inclusion. This shift from innovation-focused to inclusion-focused taxonomic development reflects the urgent need for frameworks that can navigate the complex terrain between algorithmic efficiency and inclusive outcomes (Georgiadou et al., 2024).

Building on these insights, we develop a comprehensive taxonomy framework and process model. Our approach integrates attribution theory's insights on employee perceptions with AST's understanding of technology adoption, while incorporating *Murray et al.'s* (2021) concept of conjoined agency. Drawing on Pratt and Ashforth's meaningfulness typology (2003), we advocate for a reconceptualization of inclusion across three interconnected dimensions: inclusion in work (individual experiences), *at work* (organizational climate), and *of work* (human-AI interaction). This integrated framework enables analysis of AI-enabled workplace inclusion, examining how different forms of AI agency interact with workplace inclusion and the mitigation strategies organizations employ to promote inclusive outcomes.

## 3. Method

Due to the lack of direct evidence in published research, we aim to redirect scholarly attention toward previously overlooked dimensions of workplace inclusion in AI adoption and opening new theoretical avenues for future research (Cronin & George 2023). To achieve this, we conducted a taxonomy-based exploratory study (Nickerson et al. 2013) using an iterative inductive (reviewing literature based on available evidence) and deductive (grounded in existing theoretical framework) analytical approach.

Our research objectives require a thorough analysis of the dimensions and interrelationships influencing AI adoption and workplace inclusion outcomes. Utilizing a taxonomy approach allows to identify the key variables – referred to as dimensions – and their relationships, contributing to a nuanced understanding of how AI-enabled HRM practices influence workplace inclusion.

Taxonomies structure and classify empirical evidence, enabling scholars to analyze and theorize complex domains (Bailey, 1994). Specifically, taxonomies describe and categorize subjects by highlighting commonalities among discrete observations (Fawcett & Downs 1986), which facilitate deeper analysis of selected studies based on these shared characteristics (Cipriano & Za 2022).

In this research, we systematically coded these commonalities to develop a taxonomy that captures central themes concerning the debate on the impact of AI on inclusion (Gregor 2006). Based on the taxonomy method proposed by Nickerson et al. (2013), we implemented three sequential stages, detailed in the following sections. The resulting taxonomy provides a robust foundation for our process model, which clarifies the relationships among identified dimensions in AI-enabled workplace inclusion.

### 3.1. Step 1: Locating relevant research

The first step in our research protocol consisted of identifying studies following the PRISMA guidelines that could be considered relevant to our taxonomy. Table 1 presents an overview of the data collection and selection criteria, providing a summary of the screening, eligibility and inclusion process.

Two of the authors independently reviewed all the papers and together reached a final agreement on the dataset for the study. As a result, the literature search yielded a set of 25 contributions (last update February 2024).

### 3.2. The taxonomy development process

As a second step, once the dataset was defined, we started constructing the taxonomy through the iterative development process suggested by Nickerson et al. (2013).

The iterative taxonomy development process (Nickerson et al., 2013) represents the core of this study (see Fig. 1). This process starts with the definition of the *meta-characteristics* (key objects relevant to a taxonomy) and the ending condition (criteria for concluding the iterative analysis). Then, two iterative approaches are used for analyzing the papers in the dataset (empirical-to-conceptual and conceptual-to-empirical) in order to reshape and validate the dimensions and their values through a compatibility check with the papers until the ending condition is met. The empirical-to-conceptual approach (inductive) begins by analyzing empirical data or existing evidence to identify common characteristics among objects or phenomena. Researchers start from empirical observations or specific examples and progressively group these into categories, subsequently abstracting these commonalities into broader conceptual dimensions. The dimensions and their characteristics emerge directly from empirical evidence. In contrast, the conceptual-to-empirical approach (deductive), begins conceptually,

Table 1

| Step                                                                                     | Process                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Result                                                                                                                                                                                                                                                                                                                                                                  |
|------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Term search to identify the initial dataset by performing a query on the Scopus platform | Definition of the adopted technologies (based on the title, abstract and authors' keywords)<br>Definition of the context in which the technology is used (based on the title, abstract and authors' keywords)<br>Definition of the aspects to be investigated (based on the authors' keywords)<br>Following a search for publications (published and unpublished) in scientific networks, reference lists and by authors active in the field, 7 additional papers were added to the dataset. | "artificial intelligence"<br>OR "robot" OR "chatbot"<br>OR "cobot" OR<br>"machine learning" OR<br>"deep learning" OR<br>"neural network" OR<br>"algorithm"<br>"Human resource" OR<br>"HR" OR "workplace"<br>OR "worker" OR<br>"employee"<br>"inclusion" OR<br>"inclusiv*" OR "equity"<br>OR "diversity" OR<br>"discrimination" OR<br>"fairness" OR "bias"<br>304 papers |
| Expansion of the dataset                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 311 papers                                                                                                                                                                                                                                                                                                                                                              |
| Automatic refining                                                                       | Exclusion of papers not in English (9), not scientific (1) or duplicated (1).                                                                                                                                                                                                                                                                                                                                                                                                                | 300 papers                                                                                                                                                                                                                                                                                                                                                              |
| Manual refining                                                                          | Removal of 275 papers for the following reasons:<br>– AI used as a research method: 20 papers<br>– focus of the AI adoption not in HRM practices or organizational contexts: 123 papers<br>– focus not on AI: 16 papers<br>– focus not on inclusion: 58 papers<br>– no primary data (e.g., systematic literature reviews, conceptual papers, commentaries): 58 papers                                                                                                                        | 25 papers                                                                                                                                                                                                                                                                                                                                                               |

![](7a3561af571faf036baa93f5f4b1bdb9_img.jpg)

Figure 1 is a flowchart illustrating the taxonomy development process across five iterations (1<sup>st</sup> to 5<sup>th</sup>).

**1<sup>st</sup> Iteration:**

- **Empirical-to-Conceptual:** Description & Sources: Identification of the objects by analyzing the content of the full texts of the 25 articles composing the final dataset.
- **Conceptual-to-Empirical:** Dimensions: Focus on the research question, Focus on HRM domain, HRM practices, AI technology.

**2<sup>nd</sup> Iteration:**

- **Empirical-to-Conceptual:** Description & Sources: Selection and analysis of previous theoretical frameworks or reasonable view of technology in HRM (e.g., Desanctis & Poole, 1994; Kosek & Pichler, 2010; Kim et al., 2021).
- **Conceptual-to-Empirical:** Dimensions: Types of diversity, Diversity typology, HRM practices, Diversity typology, HRM strategic goals, Diversity typology, HRM AI interaction, Diversity typology, HRM strategic goals, Diversity typology, HRM AI interaction, Diversity typology, HRM strategic goals.

**3<sup>rd</sup> Iteration:**

- **Empirical-to-Conceptual:** Description & Sources: Identification of the objects and characteristics strictly focusing on DEAI considering the usage of AI in HRM practices. Refinement of the dimensions and values.
- **Conceptual-to-Empirical:** Dimensions: Human-AI collaboration, Inclusion evaluation, Human-AI collaboration, Inclusion evaluation, HRM strategic goals, HRM strategic goals, HRM AI interaction, HRM AI interaction, HRM strategic goals, HRM strategic goals.

**4<sup>th</sup> Iteration:**

- **Empirical-to-Conceptual:** Description & Sources: Selection and analysis of previous theoretical frameworks or AI adoption and strategic diversity management (e.g., Murray & Simron 2021; Kosek & Pichler, 2006).
- **Conceptual-to-Empirical:** Dimensions: HRM strategic goals, HRM strategic goals, HRM AI interaction, HRM AI interaction, HRM strategic goals, HRM strategic goals.

**5<sup>th</sup> Iteration:**

- **Empirical-to-Conceptual:** Description & Sources: Refinement of the set of values assigned to each dimension. Checking and testing the objective and subjective ending conditions as suggested by Nickerson et al. (2013).
- **Conceptual-to-Empirical:** Dimensions: HRM strategic goals, HRM strategic goals, HRM AI interaction, HRM AI interaction, HRM strategic goals, HRM strategic goals.

The final set of dimensions is derived from the 5<sup>th</sup> iteration, which includes:

- Dimensions and values are unique, no value is empty.
- All objective and subjective ending conditions are met (see Table 2 and Table 1).

Legend: New Dimension or Value, Previously Defined Dimension or Value, Revised Dimension or Value.

Fig. 1. Summary of the taxonomy development process.

with researchers using existing theories or frameworks to propose dimensions and their characteristics deductively. These theoretically derived dimensions are then tested empirically against available evidence to validate or refine their relevance and accuracy.

The aim is to achieve mutual exclusiveness and collective exhaustiveness (objective ending condition). This ensured that each contribution is assigned exactly one value for every dimension. Therefore, no contribution could acquire two different values for the same dimension. Finally, the characteristics of the dimensions and their values should also meet Nickerson et al.'s (2013) subjective ending condition: dimensions and values must be concise, robust, comprehensive, extendible and explanatory. Thus, every dimension helps to capture the taxonomy scope of each paper while keeping track of the specific issue under debate. In other words, to meet the ending condition, exhaustive dimensions and their respective values have to be defined (Kutzen et al., 2018; Nickerson et al., 2013).

In our study, all the authors engaged in multiple iterations resolving disagreements through discussion until achieving consensus on categorization (Gregor, 2006; Miles & Huberman, 1994). In the taxonomy development process, we used a combination of approaches, which allowed us to systematically code the gathered contributions (McKelvey, 1982; Za et al., 2018). We conducted five iterations.

The first iteration employed an empirical-to-conceptual approach to gain an initial understanding of the subjects directly aligned with our dataset. We systematically analyzed the full texts of the 25 selected articles in order to clearly define our research domain and formulate a clear research question. Our research domain focused on papers investigating the adoption of AI-enabled HRM practices aimed at enhancing workplace inclusion. This analysis allowed us to formulate the following research question: How can AI adoption foster workplace inclusion, particularly regarding the potential exacerbation or alleviation of inequalities among marginalized groups?

In the second iteration, adopting a conceptual-to-empirical approach, we selected and examined previous theoretical frameworks on the integration of technology within HRM contexts (Kim et al., 2021; Leonardi & Barley, 2010; DeSanctis & Poole, 1994). This theoretical review helped us establish three initial dimensions: 1) types of inclusion, 2) HRM practices, 3) AI technology.

The third iteration, again empirical-to-conceptual, further refined the dimensions based on a detailed analysis and coding of the papers. As a result, we fine-tuned the previous dimensions, adding a fourth one: (I) Inclusion typology, (ii) Inclusion strategic goals, (iii) Human-AI

collaboration, and the newly introduced (iv) Inclusion evaluation. However, further refinement was necessary to precisely define each dimension's specific values.

In the fourth iteration, employing a conceptual-to-empirical approach, we conducted another theoretical examination focusing on previous frameworks related to AI adoption and strategic diversity management (Murray, Rhymer & Simron 2021; Kosek & Pichler 2006). With theoretical consolidation as our goal, we refined existing dimensions and introduced a fifth dimension, (v) "Mitigation strategy", explicitly designed to capture actions aimed at addressing potential inclusion-related issues arising from AI adoption.

Finally, the fifth iteration was again empirical-to-conceptual, involved a comprehensive and collaborative refinement of all dimensions and their respective values, ensuring clarity, eliminating overlaps, and rigorously checking against both the objective and subjective ending conditions outlined by Nickerson et al. (2013). Upon meeting these conditions – ensuring dimensions and values were unique, exhaustive, and clearly defined – the taxonomy development process concluded.

More specifically, in the empirical-to-conceptual (inductive) iterations of the taxonomy development process, as suggested by Boy et al. (2025), we followed the guidelines provided in the literature (Miles & Huberman, 1994) and three independent coders carefully analyzed and coded the dataset according to these predefined themes and subthemes, ensuring accuracy and reliability throughout the coding process. Fig. 2 illustrates how the coding led to the taxonomy.

Subsequently, we applied the finalized taxonomy framework to systematically classify the selected dataset papers.

### 3.3. Step 3: process model development

The resulting taxonomy framework served as the basis for our process model on AI-enabled workplace inclusion. We used a causal network approach to evaluate which conditions, factors and patterns lead AI to achieve workplace inclusion (Miles & Huberman, 1994). We created a within-case processual matrix in which each row represented a study in our dataset and each column represented the dimensions resulting from the taxonomy development process. This matrix thus provided a summary of the types of dimensions that were consistent with or constrained other dimensions and revealed underlying themes and patterns. The final step was to connect specific variables and relations in an overarching model. To move to this cross-study level of analysis, we created meta-causal network sequences of the variables

![](55d2bfe1c3d04e86df8d7a104d802172_img.jpg)

| Increasing efficiency in HRM decision-making; understanding how inclusiveness relates to AI adoption; improving inclusiveness; assessing moral outrage in algorithm discrimination                                                                                                                                                                               | Inclusion                         | Strategic inclusion goal |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------|--------------------------|
| Increasing the accuracy of the evaluations of employee performance; predicting workers' performance; attrition modeling; freeing operators from routine tasks through AI                                                                                                                                                                                         | Performance                       |                          |
| Mitigating biases in AI-recruitment systems; enhancing the recruitment and selection process; reducing unconscious biases in hiring decisions; increasing applicant fairness perceptions in job interviews; bias detection in job descriptions; to prevent the pitfall of unfairness and discrimination; to produce fair and not discriminatory hiring decisions | Recruitment                       |                          |
| Robot-mediated interviews; robotized workplace; ranking algorithms                                                                                                                                                                                                                                                                                               | Assisting AI                      |                          |
| Job matching machine learning algorithms; Deep learning; Natural Language Processing; Software for facial recognition; hiring algorithms                                                                                                                                                                                                                         | Augmenting AI                     | Human-AI interaction     |
| AI generated feedback; AI-led performance management; autonomous gender bias evaluation and debiasing algorithm; automated hiring decisions                                                                                                                                                                                                                      | Automating AI                     |                          |
| Gender; race; age; ethnicity; nationality                                                                                                                                                                                                                                                                                                                        | Socio-demographic characteristics | Inclusion typology       |
| Unemployed; part-time employees; lower seniority                                                                                                                                                                                                                                                                                                                 | Work status                       |                          |
| After AI implementation discrepancies emerged between the predefined notions of inclusion and the interpretation that was shaped by interacting with AI; integration of the AI developers and HR professionals' perspective on the meaning of inclusion                                                                                                          | Inclusion meaning                 |                          |
| Perceived fairness of AI-driven HR decisions; AI involvement and fairness perceptions; AI feedback impact by tenure; moral outrage in algorithmic discrimination; fairness in robot vs. face-to-face interviews; older workers' AI motivation; job satisfaction differences by gender and AI interaction                                                         | Individual subjective evaluation  | Inclusion evaluation     |
| Bias mitigation in AI decision-making; reduction of bias in job descriptions; effectiveness of fair ranking algorithms in increasing diversity; racial disparities in AI emotional recognition                                                                                                                                                                   | Objective evaluation              |                          |
| Introducing fairness measures; representation imbalance approach; perceived impact of statistical parity on trust in AI decisions; adoption of ranking algorithms                                                                                                                                                                                                | AI Data                           | Mitigation strategies    |
| HR managers assisting AI developers in data labeling; gender bias evaluation and debiasing guidance in text; necessity of debiasing or auditing before deployment; embedding algorithmic safeguards to detect and flag biases; pre-processing approach using differential privacy to mitigate bias; deep learning model for detecting unconscious bias           | AI Design                         |                          |
| Human oversight in AI-driven decision-making; tiered AI deployment for performance feedback; balancing AI and human judgment in HRM; enhancing fairness through selective AI application                                                                                                                                                                         | AI Usage                          |                          |
| Mitigation strategy absent or not mentioned                                                                                                                                                                                                                                                                                                                      | None                              |                          |

Fig. 2. Sample coding.

identified in each study. Using a compare-and-contrast exercise at the cross-case study level (Miles & Huberman, 1994), we matched each case-specific causal network to determine how specific relations performed across the complete set of studies. To ensure the validity of these relations, all the researchers conducted this analysis jointly. Divergent judgments regarding potential relations were assessed and resolved to capture the relevant issues emerging from the different studies.

## 4. Findings

### 4.1. The taxonomy-based framework

A synthesis of the dimensions and values of the taxonomy framework is presented in Table 2, Table 3 presents the taxonomy dimensions and their codes for each paper in our dataset.

As mentioned in the previous section, the taxonomy consists of five dimensions, namely the strategic goal of the implemented AI-led HRM practice, human-AI interaction, inclusion typology, inclusion evaluation and mitigation strategies. Below, we describe and provide examples for each dimension and the related values.

#### 4.1.1. The strategic goal of the implemented AI-led HRM practice

The strategic goals dimension identifies the underlying reasons for AI adoption in HRM practices, specifically focusing on the management's motivations for using AI to manage workplace inclusion. Following Kossek & Pichler's (2006) framework, these goals encompass

recruitment, inclusion and performance outcomes.

The literature has so far been mainly concerned with the implementation of AI in inclusive recruitment practices (Kelan 2023). Existing research shows that the use of AI in recruitment primarily aim to support decision-making and to prevent unfair treatment and prejudice (Deleczar et al. 2022; Rhue 2018; Stihr, Hilgard & Lakkaraju 2021; Zhou et al. 2021; Köchling et al. 2021; El Ouadhriri & Abdelhadi, 2021; Kubiak et al. 2023). Other studies have investigated bias reduction in hiring decisions (Soleimani, Intezari & Pauleen 2022; Hofeditz et al. 2022), the detection of bias (Bashar et al. 2021; Ramezanzadehmoghaddam et al. 2021; Zhang et al. 2023) and the enhancement of applicants' fairness perceptions during job interviews (Norskov et al. 2022; Bedemariam & Wessel 2023).

In the inclusion subdomain, the use of AI was intended to reduce workplace discrimination. For example, several studies (Bigman et al. 2020; Newman, Fast & Harmon 2020) explored the fairness perceptions of those affected by decisions made by AI algorithms, which in turn could challenge the notion of inclusion *per se* as AI shaped what came to be understood as inclusive in the physical (van den Broek et al., 2019) and virtual (Cutler et al. 2021) workplace.

With regard to performance, the use of AI was intended to create value from inclusion and generate financial effectiveness. This strategic goal highlighted the potential tension between the reasons for AI adoption (such as the promise of improved performance) and the actual outcomes (such as improvements in both performance and inclusion).

Several papers focused on the effect of AI on certain categories in the

**Table 2**  
AI-enabled workplace inclusion taxonomy dimensions.

| Dimension                | Value                             | Value description                                                                                                                                                            |
|--------------------------|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Strategic inclusion goal | Inclusion                         | AI employed to promote and support inclusion among employees                                                                                                                 |
|                          | Performance                       | AI employed to improve performances                                                                                                                                          |
|                          | Recruitment                       | AI employed in order to attract and hire a more diverse workforce                                                                                                            |
| Human-AI interaction     | Assisting AI                      | Non-agentic technologies wielded by humans in both protocol development and action selection                                                                                 |
|                          | Augmenting AI                     | Agentic technologies which exercise intentionality over protocol development but not over action selection                                                                   |
|                          | Automating AI                     | Agentic technologies which exercise intentionality over both protocol development and action selection                                                                       |
| Inclusion typology       | Socio-demographic characteristics | Groups of people positively or negatively impacted by AI according to their individual characteristics                                                                       |
|                          | Work status                       | Groups of people positively or negatively impacted by AI according to their work status                                                                                      |
|                          | Inclusion meaning                 | Challenges to the meaning of inclusion shared within the organizational context due to AI adoption                                                                           |
| Inclusion evaluation     | Individual subjective evaluation  | Subjective perceptions related to the adoption of AI-technologies for HRM purposes                                                                                           |
|                          | Objective evaluation              | Intended and actual outcomes that AI adoption implies for diversity, equity and inclusion                                                                                    |
| Mitigation strategy      | AI data                           | Mitigation strategies to remove systematic bias and account for skewed representation of individual or work characteristics in the trained dataset                           |
|                          | AI design                         | Mitigation strategies to reduce risk of discrimination by introducing discrimination checks and debiasing strategies or including more diversity in the AI development stage |
|                          | AI usage                          | Mitigation strategies involving managers and HR practitioners to increase sense of AI usage in HRM                                                                           |
|                          | None                              | No mitigation strategies adopted                                                                                                                                             |

**Table 3**  
AI-enabled workplace inclusion taxonomy dimensions and codes for each study.

| Paper                                    | Strategic goal | Human-AI interaction type | Inclusion typology                | Inclusion evaluation | Mitigation strategy |
|------------------------------------------|----------------|---------------------------|-----------------------------------|----------------------|---------------------|
| 01. Bashar et al. (2021)                 | Recruitment    | Augmenting AI             | Socio-demographic characteristics | Objective            | Design              |
| 02. Bedemarian & Wessel (2023)           | Recruitment    | Automating AI             | Socio-demographic characteristics | Subjective           | Usage               |
| 03. Bigman et al. (2020)                 | Inclusion      | Automating AI             | Socio-demographic characteristics | Subjective           | Usage               |
| 04. Blagoev et al. (2022)                | Performance    | Assisting AI              | Socio-demographic characteristics | Subjective           | None                |
| 05. Cutler et al. (2021)                 | Inclusion      | Assisting AI              | Work status                       | Objective            | None                |
| 06. Delecrac et al. (2022)               | Recruitment    | Augmenting AI             | Socio-demographic characteristics | Objective            | Design              |
| 07. Harris (2022)                        | Recruitment    | Augmenting AI             | Socio-demographic characteristics | Objective            | Data                |
| 08. Hofeditz et al. (2022)               | Recruitment    | Augmenting AI             | Socio-demographic characteristics | Objective            | Usage               |
| 09. Hu et al. (2022)                     | Recruitment    | Automating AI             | Socio-demographic characteristics | Objective            | Design              |
| 10. Köchling et al. (2021)               | Recruitment    | Augmenting AI             | Socio-demographic characteristics | Objective            | Data                |
| 11. Kubiak et al. (2023)                 | Recruitment    | Augmenting AI             | Socio-demographic characteristics | Objective            | Data                |
| 12. Newman et al. (2020)                 | Inclusion      | Automating AI             | Socio-demographic characteristics | Subjective           | Usage               |
| 13. Norskov et al. (2022)                | Recruitment    | Assisting AI              | Work status                       | Subjective           | None                |
| 14. El Ouadrhiri & Abdelhadi, 2021       | Recruitment    | Automating AI             | Socio-demographic characteristics | Objective            | Design              |
| 15. Ramezanadadhehmaghadam et al. (2021) | Recruitment    | Automating AI             | Socio-demographic characteristics | Objective            | Design              |
| 16. Rhue (2018)                          | Recruitment    | Augmenting AI             | Socio-demographic characteristics | Objective            | Design              |
| 17. Soleimani et al. (2022)              | Recruitment    | Augmenting AI             | Inclusion meaning                 | Objective            | Design              |
| 18. Speer (2021)                         | Performance    | Augmenting AI             | Socio-demographic characteristics | Objective            | Design              |
| 19. Sühir et al. (2021)                  | Recruitment    | Assisting AI              | Socio-demographic characteristics | Objective            | Data                |
| 20. Tong et al. (2021)                   | Performance    | Automating AI             | Work status                       | Subjective           | Usage               |
| 21. Toyoda et al. (2021)                 | Performance    | Augmenting AI             | Socio-demographic characteristics | Objective            | Design              |
| 22. Turja et al. (2022)                  | Performance    | Assisting AI              | Socio-demographic characteristics | Subjective           | None                |
| 23. van den Broek et al. (2019)          | Recruitment    | Augmenting AI             | Socio-demographic characteristics | Subjective           | Data                |
| 24. Zhang et al. (2023)                  | Recruitment    | Augmenting AI             | Socio-demographic characteristics | Objective            | Design              |
| 25. Zhou et al. (2021)                   | Inclusion      | Augmenting AI             | Inclusion meaning                 | Subjective           | Usage               |

evaluation of employee performance (Tong et al. 2021; Toyoda, Lucas & Gratch 2021) or on reducing the potential for group discrimination when predicting turnover (Speer 2021). In other cases, AI was adopted in order to improve organizational effectiveness, thus impacting certain groups' motivation (Blagoev, Shustova & Protas 2022) or job satisfaction (Turja et al. 2022).

This threefold categorization of the strategic goals provides a comprehensive framework for understanding the motivations for AI adoption in HRM. The framework enables a nuanced analysis of how varying strategic objectives influence workplace inclusion outcomes, contributing to an integrated understanding of AI-enabled workplace inclusion. The taxonomy specifically allows the examination of potential disconnects between intended and realized outcomes across these strategic dimensions. Consider, for instance, how organizations implementing emotion-detecting video interview platforms typically frame their adoption around recruitment goals – seeking to eliminate interviewer bias and create fairer selection processes. However, when these systems systematically misinterpret emotional expressions from candidates with different cultural backgrounds, the intended recruitment fairness transforms into exclusion by design, revealing the gap between strategic theoretical intentions and inclusion pragmatic outcomes.

#### 4.1.2. Conjoined agency: human-AI interaction

The human-AI interaction dimension encompasses both technology types and the conjoined agency of humans and agentic technologies. This framework enables the categorization of AI applications in HRM based on their level of conjoined agency and inclusion impacts. We identified three key values – *assisting AI*, *augmenting AI* and *automating AI* – drawing from Murray et al. (2021). Note that *arresting AI* did not emerge in our analysis. These distinctions prove crucial in practice. An HR chatbot that simply provides information about practices and policies (assisting AI) creates minimal inclusion risks, primarily affecting employees with potential language barriers. However, an algorithmic performance management system that independently generates employee ratings (automating AI) could disadvantage employees whose work styles and patterns (i.e. different approach to time – see *Georgia-dou & Damianidou, 2025*) don't align with algorithmic expectations, creating profound inclusion implications that require complicated mitigation strategies.

These categories of human-AI interaction provide a framework for

understanding how different levels of conjoined agency in HRM practices may influence workplace inclusion. By including this concept in our taxonomy, it is possible to analyze the complex interplay between AI technologies and human decision-makers in shaping inclusive workplace environments.

**Assisting AI** is non-agentic technology in which humans control both protocol development and action selection. These tools enhance efficiency while maintaining human control over decision-making processes. Examples in the literature included AI-mediated job interviews (Norskøv et al. 2020), routine task automation (Turja et al. 2022) and recruitment ranking algorithms (Sühr, Hilgard & Lakkaraju 2021).

**Augmenting AI** technologies exercise intentionality in protocol development but not in action selection. These systems provide recommendations while preserving human decision-making authority. Examples included job-matching ML algorithms for fair recruitment decisions (Delecrac et al. 2022), employee turnover prediction (Speer 2021), facial recognition for performance predictions (Köchling et al. 2021; Toyoda, Lucas & Gratch 2021) and bias detection in job descriptions (Bashar et al. 2021; Ramezanzadehmoghadam et al. 2021).

Finally, **automating AI** exercises intentionality in both protocol development and action selection. These systems independently gather data, formulate rules and make decisions, effectively substituting human decision-making. Notable applications included state-of-the-art DL neural network-based performance evaluation systems (Tong et al. 2021) and bias evaluation algorithms for job descriptions (Hu et al. 2022; El Ouadhriri & Abdelhadi, 2021).

Critical examination of these technologies has revealed complex implications for workplace inclusion. For instance, Bedemariam & Wessel (2023) conducted a study testing an AI-based selection tool that autonomously declined candidates. Their findings indicated that Black applicants rejected by the AI algorithm reported more negative procedural justice reactions compared to those rejected by human evaluators. Furthermore, a pairwise comparison revealed that Black participants accepted by the AI algorithms reported stronger procedural justice reactions than those accepted by human HRM. However, the condition (AI vs. HRM) had no significant effect on the general fairness reactions of White applicants, regardless of the outcome (acceptance or rejection). These findings highlighted the complex and potentially differential effects of automating AI on workplace inclusion, particularly for marginalized groups.

The human-AI interaction dimension reveals critical relationships between adoption factors and outcomes. While organizations may implement assisting AI to support decision-making and reduce bias, its success depends on the human utilization of the AI insights. Similarly, automating AI aimed at eliminating human bias requires careful design and monitoring to prevent its adoption perpetuating or amplifying existing biases. This framework enables the analysis of the complex interplay between AI technologies and human decision-makers in creating inclusive workplace environments.

#### 4.1.3. Inclusion typology

The inclusion typology dimension relates to how inclusion is understood and put into practice in the literature examining how AI adoption affects inclusion in HRM decision-making. This part of our taxonomy is vital as it helps us categorize and analyze the various aspects of diversity and inclusion that AI-enabled HRM practices try to address. Including this dimension gives us a more detailed understanding of how AI affects different elements of workplace inclusion.

Most studies in our dataset mentioned inclusion in terms of special groups of people who may be either positively or negatively influenced by the AI adoption based on *socio-demographic characteristics* like race, gender, and age. This focus on demographic characteristics of diversity and inclusion tradition, although important, may not capture the whole picture of the workplace inclusion when AI is used in HRM practices.

Gender and race were the most frequent characteristics explored in relation to AI adoption (Zhang et al. 2023; Hofeditz et al. 2022;

Bedemariam & Wessel 2023; Kubiak et al. 2023; Zhou et al. 2021; Delecrac et al. 2022; Toyoda, Lucas & Gratch 2021), followed by age (Bigman et al. 2020; Harris 2022; Blagoev, Shustova & Protas 2022). These findings highlighted the potential of existing or even worsening biases in the application of AI and the ability to tackle more implicit forms of exclusion.

A number of studies considered *work status* as a source of inclusion and more specifically how aspects like tenure, unemployment and part time work impacted people's perceptions of AI in HRM decisions (Tong et al. 2021; Cutler et al. 2021; Norskøv et al. 2022). This extended the view of inclusion to factors that are not demographic in nature, but rather derive from the organization and its employment relationships.

Interestingly, a few studies questioned what inclusion actually means, highlighting how using AI for HRM purposes challenged the shared understanding of inclusion within specific organizational settings (Soleimani et al., 2022; van den Broek et al., 2019).

The inclusion typology dimension of our taxonomy allows the examination of both the intended targets of AI-enabled inclusion efforts (adoption factors) and the actual impact on different groups (outcome). For instance, an organization might adopt AI-powered recruitment tools with the intention of increasing gender diversity (adoption factor targeting socio-demographic characteristics). However, the outcome of this initiative would be evaluated by the actual change in gender representation and the experiences of women in the organization's post-adoption phase.

#### 4.1.4. Inclusion evaluation

The inclusion evaluation dimension considers whether the effect of AI on inclusion is measured according to individual subjective perception or objective gauges. This dimension of our taxonomy is crucial as it captures the multifaceted nature of inclusion outcomes in AI-enabled HRM practices, acknowledging that the impact of AI on workplace inclusion can be assessed from different perspectives.

*Individual subjective evaluation* has been adopted, for example, in laboratory studies to measure if people affected by algorithmic decisions (e.g., employees, job candidates) felt these decisions were less fair than those made by humans, even when the results were exactly the same (Bedemariam & Wessel 2023; Newman, Fast & Harmon 2020). This approach showed how important perceived fairness is in AI-enabled HRM practices, which can greatly affect how included employees feel and how committed they are to their organization.

Research examining AI's impact on inclusion has employed various methodological approaches. Tong et al. (2021) conducted field experiments examining how employees responded to AI-generated versus human-manager performance feedback. In recruitment contexts, Norskøv et al. (2022) investigated the fairness perceptions of robot-mediated job interviews from both the applicants' and HRM professionals' perspectives, while Zhou et al. (2021) studied perceptions of AI fairness when gender-focused fairness measures were introduced.

The evidence regarding AI's effect on inclusion presents a complex picture. Some studies have indicated positive outcomes: unemployed job candidates viewed AI-led selection processes as fairer (Norskøv et al. 2022) and recruitment process demonstrated improved gender fairness when specific measures were adopted (Zhou et al. 2021). Hofeditz et al. (2022) found that AI-based systems reduced discrimination against older and female candidates, though notably, the same system selected fewer candidates from foreign backgrounds.

However, research has also uncovered potential problems. Newman et al. (2020) found that people viewed algorithmic HRM decisions as less fair than identical decisions with more human involvement, regardless of gender, ethnicity or age. Interestingly, Bigman et al. (2020) found that discriminatory outcomes caused less moral outrage when blamed on algorithms rather than human decision-makers, suggesting that AI might accidentally normalize certain types of discrimination.

*Objective evaluation* is related to the analysis of intended and actual outcomes on inclusion implied by AI adoption, mainly through DL

models that detect unconscious bias in content (Bashar et al. 2021; Ramezanzadehmohghadam et al. 2021) or algorithmic analysis for facial recognition on a real dataset of videos or pictures (Rhue 2018; Köchling et al. 2021). As positive consequences on inclusion, objective evaluation has shown that fair ranking algorithms increased female candidates (Kubiak et al. 2023; Sühr, Hilgard & Lakkaraju 2021) and that recruitment biases were mitigated when AI was adopted, thereby keeping the recruitment process fairer (Bashar et al. 2021; Ramezanzadehmohghadam et al. 2021; Hu et al. 2022). On the flip side, several studies have shown that AI repeated existing inequalities and bias in the recruitment process and negatively affected the inclusion of female, older and dark-skinned workers (Zhang et al. 2023; Toyoda, Lucas & Gratch 2021; Speer 2021).

Including both subjective and objective evaluation measures in our taxonomy fits with the theoretical concept of organizational justice (Colquitt et al., 2013), which covers both perceived (subjective) fairness and actual (objective) fair treatment. This dual approach allows for a more thorough evaluation of AI's impact on workplace inclusion, capturing both the lived experiences of employees and measurable changes in organizational diversity and inclusion metrics.

#### 4.1.5. Mitigation strategies

Given the potential negative impact that AI can have on inclusion, the last dimension refers to strategies to mitigate the effects analyzed by the papers in our dataset. This dimension is crucial for our integrated concept of AI-enabled workplace inclusion as it acknowledges the proactive measures organizations can take to ensure that AI implementation helps rather than hinders inclusion. The subset of values for this dimension consists of *AI data*, *AI design* and *AI usage*.

Mitigation strategies focusing on *AI data* aimed to remove systematic bias and account for skewed representation of individual or work characteristics in the trained dataset that the AI relies on. For example, underrepresentation of a certain gender or ethnicity in the training dataset may lead to an unpredictable overestimation and/or underestimation of the likelihood of inviting representatives of these groups to a job interview. Ways to help reduce such a risk were: using gender-blind data (Kubiak et al. 2023); introducing to the dataset fairness measures such as statistical parity to represent fairness or implementing calibration and correction methods to tackle class imbalance problems (Köchling et al. 2021; Zhou et al. 2021).

Another set of mitigation strategies referred to *AI design*. These were initiatives undertaken in the design stage of the AI system in order to reduce discrimination risk. This was done by introducing discrimination checks and debiasing strategies or by including more diversity in the AI developing teams. Some examples of AI design strategy were: adopting a pre-processing approach that used differentiated privacy properties to mitigate sensitive attribute bias by introducing a randomized response mechanism to reduce inequity and avoid discrimination resulting from the training dataset (El Ouadrhiri & Abdelhadi, 2021); embedding safeguards in the AI-based automation process algorithm that flagged possible biases and measured them in order for the decisions made by the ML algorithm matching job applicants with job offers to be as fair as possible (Deleczar et al. 2022); auditing and adjusting algorithms (e.g., removing variables, changing variable weights) to remove bias (Speer 2021; Toyoda, Lucas & Gratch 2021). Interestingly, collaboration between the HRM department and AI developers was advocated in the design phase of AI for HRM decision-making in order to reduce discrimination risks (Soileimani, Intezari & Pauleen 2022).

The *AI usage* mitigation strategy was more related to creating a sense and vision around the adoption of AI for HRM decision-making and increasing awareness of discrimination risks and debiasing strategies. According to the papers in our dataset, some companies considered training HR practitioners to deploy AI in a differentiated rather than uniform approach where the level of AI use varied based on employees' characteristics or roles. For example, companies could consider using AI to provide performance feedback to veteran employees thanks to their

familiarity with the organizational context, but using managers to provide performance feedback to novices who lacked information about the context and the overall reasons for AI adoption in the HRM systems (Tong et al. 2021). Moreover, HRM managers could reduce the employees' perceptions that AI-led HRM decision-making was reductionistic and therefore less fair by emphasizing that the algorithm was just one factor human decision-makers had the option of considering when selecting which actions to implement (Newman, Fast & Harmon 2020) or providing job candidates with information about the functionality of an AI-based system (Hofeditz et al. 2022).

The mitigation strategies dimension of our taxonomy aligns with the theoretical concept of responsible AI, which emphasizes the need for ethical considerations in the development and deployment of AI systems. By categorizing mitigation strategies into data, design and usage-related approaches, our taxonomy provides a comprehensive framework for implementing responsible AI-based practices in HRM.

Moreover, this dimension helps reconceptualize inclusion in the context of AI-enabled HRM by highlighting the proactive measures organizations can take to ensure AI promotes inclusivity. It shifts the focus from merely avoiding discrimination to actively fostering an inclusive environment through thoughtful AI implementation.

### 4.2. The AI-enabled workplace inclusion process model

In order to answer our research question on how AI adoption can help or hinder a company's efforts to promote inclusion, we modelled a sequence of events based on the underlying patterns present in the studies included in our dataset. Indeed, in addition to Table 3, which summarizes the variables identified in each paper on the effect of AI adoption on inclusion, we put together a pattern of sequencing variables considered meaningful across all studies. From this, we built a meta-causal network (Miles & Huberman, 1994) describing what happens when companies adopt AI in order to achieve inclusion goals. Fig. 3 summarizes the sequence of events involved in the AI adoption process model leading to workplace inclusion. The grey boxes represent the stages in AI adoption, moving from non-agentic to agentic technologies that influence what kind of mitigation strategies are needed in order to overcome discrimination risks and lead to inclusion. Moreover, the solid arrows in Fig. 3 point to mainly technology-related sequences of events, while the broken lines indicate the themes related to the notion of inclusion that emerge when AI is adopted.

The genesis of an AI process for all stages of HRM decision-making is contingent upon the organizational impetus to incorporate AI to boost inclusion in HRM decision-making. Our findings suggest that these factors can be broadly categorized into three principal types, as per Kossek & Pichler's (2006) model: recruitment, inclusion and performance. In line with this taxonomy, strategic inclusion goals may be geared towards current employees with contextual knowledge of the organization, for example, in initiatives to foster inclusivity and performance. Conversely, initiatives to amplify inclusion in recruitment may target prospective employees who possess a limited understanding of the company's context. In the absence of other pertinent information, these may make use of peripheral cues from the AI technology adopted to infer the company's inclusiveness.

While the outcomes of AI adoption to promote inclusion may be either positive or negative, the link between AI adoption and its consequences appears to depend on the capacity of the AI technology adopted to exercise intentionality. More specifically, when non-agentic assisting technologies are adopted (e.g., robots, ranking algorithms), there seems to be a lower risk of AI exacerbating discrimination since the impact of these technologies on inclusion is highly dependent on the humans who wield them. Indeed, most of the studies on the effect of adopting assisting AI on inclusion have reported positive outcomes, such as an increase in female candidates due to fair ranking algorithms (Sühr, Hilgard, and Lakkaraju 2021), increased fairness perceptions among the unemployed when teleoperated robots are adopted for job interviews

![](7801d00a216dc4dc8a7d210dcb5fe3c5_img.jpg)

Figure 3 illustrates the AI-enabled workplace inclusion process model. The model shows the relationship between DE&I goals, AI adoption, AI mitigation strategy, and AI-enabled workplace inclusion.

The process starts with **DE&I goals**, which influence **AI adoption**. **AI adoption** is categorized into three types of AI technology:

- **Assisting AI** (Human agency)
- **Augmenting AI** (AI agency)
- **Arresting AI\*** (AI agency)

**AI mitigation strategy** influences **AI adoption** (specifically Augmenting AI and Arresting AI\*). **AI adoption** (Augmenting AI and Arresting AI\*) leads to **AI-enabled workplace inclusion**.

**AI data** flows from **AI adoption** (Assisting AI) to **AI mitigation strategy**. **AI data** also flows from **AI adoption** (Augmenting AI and Arresting AI\*) to **AI mitigation strategy**. **AI data** flows from **AI adoption** (Augmenting AI and Arresting AI\*) to **AI-enabled workplace inclusion**.

**AI design** and **AI usage** flow from **AI adoption** (Augmenting AI and Arresting AI\*) to **AI-enabled workplace inclusion**.

**AI-enabled workplace inclusion** feeds back to **DE&I goals** and **AI mitigation strategy**.

**Rethinking inclusion** is shown as a separate box, influencing **AI adoption** (Assisting AI) and **AI mitigation strategy**.

\* No use of Arresting AI technology emerges, but we cannot exclude its future adoption in HRM

Fig. 3. The AI-enabled workplace inclusion process model.

(Norskov et al. 2022) and improved inclusiveness of part-time employees (Cutler et al. 2021). Notably, only one case – among older workers in the bank sector – showed the adoption of robots leading to lower motivation (Blagoev, Shustova & Protas 2022). Therefore, in order to achieve workplace inclusion, it seems that no mitigation strategies are needed when assisting AI is involved.

However, the situation is slightly more nuanced when organizations move towards adopting more agentic AI technologies. In this respect, the situation becomes more complicated upon the adoption of augmenting AI that develops protocols and predictive recommendations, thus complementing humans. In this situation, several papers have suggested that ML systems promote gender fairness in the recruitment process (Kubiak et al. 2023; Zhou et al. 2021) or that DL models decrease bias in job descriptions, therefore keeping the recruitment process fairer (Harris 2022; Bashar et al. 2021; Ramezanzadehmoghadam et al. 2021). On the opposite side, the likelihood of augmenting AI having an adverse impact on women, older and dark-skinned workers, thus replicating existing inequalities and bias in the recruitment process, has been found to be high (Toyoda, Lucas & Gratch 2021; Speer 2021). Interestingly, the findings have shown that AI data mitigation strategies need to be adopted in order to move from assisting AI to augmenting AI adoption without undermining inclusion efforts. For example, in the case of systems that use historical recruiting data to train a ML model and predict whether a candidate would be hired, systematic information (Kubiak et al. 2023) or bias needs to be removed from the trained dataset, in order to promote fairness in the recruitment process (Zhou et al. 2021). Therefore, augmenting AI may lead to more inclusion if AI data mitigation strategies such as fairness measures or calibration and correction methods are adopted in advance (Köchling et al. 2021; Zhou et al. 2021). Interestingly, the meaning of inclusion within the specific organizational context is called into question when companies start delegating the development of protocols and recommendations to augmenting AI, for example (Soleimani et al. 2022; van den Broek et al., 2019). Indeed, if no agentic technology is adopted, the meaning of inclusion is considered unproblematic and shared among members. But the delegation of inclusion responsibilities to agentic technologies leads stakeholders to rethink what comes to be understood as inclusive under the AI umbrella and to reconsider and negotiate the inclusion goals as well as the mitigation strategies that need to be put in practice to keep the AI system inclusive. Thus, the notion of inclusion needs to be rethought to adapt to the new circumstances of AI adoption in HRM decision-making.

Also, the transition towards the most agentic stage, namely the adoption of automated AI that substitutes humans in HRM decision-making, requires specific mitigation strategies to reduce the risk of discrimination. More specifically, the objective outcome of automating AI adoption seems to be positive since it seems to mitigate recruitment biases, but this can only be achieved if AI design mitigation strategies such as pre-processing or debiasing algorithms are adopted (Hu et al. 2022; El Ouadrhiri & Abdelhadi, 2021). Moreover, studies on the effect of automating AI on inclusion have mainly been focused on exploring how algorithm-driven HRM decisions are perceived by current employees. For example, AI feedback disclosure (namely informing employees that feedback is AI produced) seems to produce a less severe negative effect among employees with longer tenure (Tong et al. 2021). Interestingly, in some cases, HRM decisions taken by automating AI were perceived to be less fair than identical decisions featuring more human involvement (Newman, Fast & Harmon 2020). In other cases, people were less morally outraged when age, race and gender discrimination was perpetrated by an algorithm rather than by humans (Bigman et al. 2020). Therefore, only when the organization internally adopts automating AI for inclusion and performance goals an AI usage mitigation strategy is needed to create a sense and vision among employees around inclusion-oriented AI adoption for HRM decision-making.

## 5. Discussion

### 5.1. Theoretical implications

When examining AI-enabled workplace inclusion through a multi-level lens (Georgiadou et al., 2024), we can identify three distinct but interrelated level of analysis (Fig. 4): inclusion in work (individual level), inclusion *at* work (organizational level) and inclusion *of* work (human-AI interaction level). Each level requires specific consideration of how different forms of AI agency interact with workplace dynamics.

At the individual level, inclusion *in* work focuses on personal experiences and engagement with AI-enabled HRM systems. Our process model reveals that as organizations progress from assisting to augmenting AI, individual experiences of inclusion become increasingly dependent on AI data mitigation strategies. For example, when implementing AI-enabled performance management systems, organizations must ensure that individual work styles and contributions are fairly recognized. This calls for specific strategies to address data problems,

![Venn diagram illustrating the Reconceptualization of workplace inclusion in the context of AI adoption. The diagram shows three overlapping circles: INDIVIDUAL LEVEL, HUMAN-AI INTERACTION LEVEL, and ORGANIZATIONAL LEVEL. The intersection of INDIVIDUAL LEVEL and HUMAN-AI INTERACTION LEVEL is labeled 'Inclusion in work' with the description: 'Focus on personal experiences and engagement with AI-enabled HRM systems'. The intersection of HUMAN-AI INTERACTION LEVEL and ORGANIZATIONAL LEVEL is labeled 'Inclusion of work' with the description: 'Focus on how humans and AI interact in shaping inclusive practices'. The intersection of ORGANIZATIONAL LEVEL and INDIVIDUAL LEVEL is labeled 'Inclusion at work' with the description: 'Focus on how AI systems shape organizational culture and collective experiences of belonging'.](10c82dcc5f2c237961329dd29d65859c_img.jpg)

Venn diagram illustrating the Reconceptualization of workplace inclusion in the context of AI adoption. The diagram shows three overlapping circles: INDIVIDUAL LEVEL, HUMAN-AI INTERACTION LEVEL, and ORGANIZATIONAL LEVEL. The intersection of INDIVIDUAL LEVEL and HUMAN-AI INTERACTION LEVEL is labeled 'Inclusion in work' with the description: 'Focus on personal experiences and engagement with AI-enabled HRM systems'. The intersection of HUMAN-AI INTERACTION LEVEL and ORGANIZATIONAL LEVEL is labeled 'Inclusion of work' with the description: 'Focus on how humans and AI interact in shaping inclusive practices'. The intersection of ORGANIZATIONAL LEVEL and INDIVIDUAL LEVEL is labeled 'Inclusion at work' with the description: 'Focus on how AI systems shape organizational culture and collective experiences of belonging'.

Fig. 4. Reconceptualization of workplace inclusion in the context of AI adoption.

like cleaning systematic bias from the datasets used for training and putting in place fairness measures for AI-based systems.

Whether these strategies work depends on keeping distributive fairness (Colquitt et al., 2013); that is making sure that when AI makes decisions about resources and opportunities, these are shared fairly among all staff members. This individual dimension is especially critical as employees come to interpret the AI systems, with what they attribute to these systems significantly influencing their sense of inclusion (Newman et al., 2020; Nishii et al., 2008). Therefore, to address inclusion in work, a focus is needed on individual experiences and the personal implications of AI technologies.

Inclusion at work examines how AI systems shape organizational culture and collective experiences of belonging. Our process model illustrates that as organizations adopt more agentic AI technologies, there is an increasing need for organizational-level mitigation strategies. This dimension requires careful consideration of how AI systems influence the organizational climate, group dynamics and shared understanding of inclusion.

For the successful implementation of automating AI, there are specific mitigation strategies that should be used at the organizational level, starting from the idea of a shared meaning of AI adoption and trust in AI-based decisions. These organizational level strategies are consistent with procedural fairness concept (Colquitt et al., 2013), which deals with the issues of openness and regularity of the AI enabled processes within the organization. The effectiveness of AI in this level depends on the general organizational context in which it is applied because the same systems can lead to different inclusion outcomes in different organizations (Mor Barak, 2010). In order to achieve workplace inclusion, the focus has to be on the collective experiences and shared perceptions that are influenced by AI systems.

Inclusion of work concerns the meta-level integration of AI into workplace systems, emphasizing how humans and AI interact in shaping inclusive practices. The process model shows that this dimension becomes increasingly important as organizations move towards more agentic AI technologies. The model indicates that to achieve inclusion of work, specific design-related mitigation strategies are needed, particularly when implementing augmenting or automating AI. These strategies focus on interactional fairness (Colquitt et al., 2013); namely they address how staff members interact with and experience AI systems in their everyday work.

The human-AI interaction dimension shows how important

collaborative approaches are when implementing AI, involving different stakeholders in designing and deploying the system. This aligns with AST (DeSanctis & Poole, 1994), which posits that the effectiveness of technological systems largely depends on how people utilize existing norms and communication processes to structure their interactions with technology.

The interaction between inclusion in work, at work and of work is crucial to understanding the complexities of AI implementation in organizations. Our process model provides a nuanced perspective by connecting these levels of inclusion, which are often addressed separately in the existing research, highlighting their interdependence and collective impact on workplace dynamics.

First of all, this model builds on the existing theoretical frameworks in several critical ways. In relation to attribution theory, it increases our knowledge about how employees interpret the meaning of AI-enabled HRM practices, particularly with respect to how these attributions affect their perceptions of inclusion across the individual, organizational and interactive levels (Nishii et al., 2008). For instance, based on their beliefs, employees may consider AI systems to be more unbiased and more transparent than human driven systems, which would impact their sense of inclusion in work, or may consider them as impersonal and detached and, therefore, may have a different perception of inclusion at work. These interpretations can shape how AI is received and trusted across different organizational layers, demonstrating that attribution is not a uniform process but one deeply influenced by the specific type of AI technology and its role in the HRM strategy. Moreover, our model builds on AST (DeSanctis & Poole, 1994), revealing the dynamic relationship between technology and social systems by illustrating how various forms of AI agency influence workplace inclusion. AST posits that technology use in organizations is shaped by both existing norms and the way individuals and groups adapt to new systems. Our findings indicate that AI adoption, from assisting to automating technologies, can redefine organizational norms and practices around inclusion. As employees adapt and respond to AI interventions, they either reinforce or transform organizational practices and structures, with each level of inclusion requiring distinct strategies to align AI use with the organization's inclusion goals.

Secondly, our model suggests that the three dimensions of inclusion are deeply interconnected in ways that become more apparent as organizations adopt more sophisticated AI technologies. For example, individual experiences of inclusion (in work) significantly influence and

are influenced by the organizational climate (at work), while both dimensions shape and are shaped by the ways in which humans and AI systems interact (of work). This interconnectedness suggests that organizations must maintain a balanced focus across all three dimensions to achieve meaningful inclusion outcomes.

When examining assisting AI technologies, we found that inclusion outcomes are highly dependent on human agency, with relatively few discrimination risks requiring mitigation strategies. This suggests that to build trust and understanding, organizations might begin their AI implementation journey with these technologies while developing their inclusion capabilities. The transition to augmenting AI introduces new complexities across all three dimensions of inclusion. At the individual level (inclusion in work), our model shows that AI systems can promote gender fairness in recruitment processes but may also replicate existing inequalities if not carefully managed. The dual nature of AI systems demonstrates why data-related mitigation strategies are essential for addressing bias in training datasets (Köchling et al., 2021; Zhou et al., 2021).

The organizational dimension of inclusion at work becomes vital when organizations implement AI because employees start to redefine organizational inclusion according to their workplace context (Soleimani et al., 2022). Organizations can develop advanced workplace inclusion strategies through this questioning process which addresses the distinctive characteristics of digital technologies. The transition to automated AI systems creates the most significant barrier to work inclusion which demands advanced design-based solutions to establish fair and transparent decision-making systems.

The model indicates that AI system success during this stage requires stakeholder participation from all organizational levels during both design and operational phases. Research indicates that automated systems can minimize recruitment biases when properly designed (Hu et al., 2022; El Ouadrihi & Abdelhadi, 2021). Each dimension requires unique mitigation strategies that resolve its particular challenges. Nevertheless, consideration must also be given to how these strategies interact. For example, data-related strategies primarily support inclusion in work by ensuring fair treatment at the individual level, while design-related strategies foster inclusion of work by promoting positive human-AI interactions. Usage-related strategies promote inclusion at work by helping organizations cultivate supportive climates for AI adoption.

Thirdly, our model emphasizes the need for a continuous, iterative approach to AI implementation, wherein organizations pay attention to the evolving organizational climate and employee experiences so as to maintain inclusion outcomes across all dimensions. The progression from assisting to automating AI should not be viewed as a linear path but rather as an iterative process in which organizations may need to move between different forms of AI agency depending on the specific context and inclusion goals. Indeed, our model reveals that the impact of automating AI on inclusion varies significantly depending on the strategic goal being pursued. This dynamic perspective helps explain why similar AI implementations can produce different inclusion outcomes across organizations. When used for recruitment purposes, automated systems might reduce discrimination through objective decision-making processes. However, when applied to internal organizational processes like performance evaluation, these systems need careful attention to usage-related mitigation strategies to maintain employees' trust and perception of fairness.

The model also highlights the critical role of feedback loops in maintaining and enhancing inclusion across all dimensions. As organizations implement AI systems, they must continuously monitor and adjust their approaches based on inclusion outcomes. This ongoing process of assessment and adaptation is particularly important given the rapidly evolving nature of AI technology and its applications in HRM. Our multilevel framework extends paradox theory (Schad et al., 2016) by revealing how the automation-augmentation tension identified by Raisch and Krakowski (2021) manifests differently across levels of

inclusion. At the individual level (inclusion in work), the paradox emerges as organizations attempt to reduce subjective bias through automation whilst needing human expertise to ensure fair data strategies. At the organizational level (inclusion at work), it surfaces in tensions between standardized, automated processes and the human judgment needed for inclusive climate development. At the interaction level (inclusion of work), the paradox appears in the delicate balance between AI's consistent application of rules and humans' contextual understanding. These nested paradoxes suggest that effective AI-enabled inclusion requires organizations to develop capabilities for managing tensions across multiple levels simultaneously.

### 5.2. Limitations and future research

Although our study has taken the theory on AI adoption in HRM and workplace inclusion forward by drawing up a taxonomy framework and a process model across a diverse set of studies, the following limitations should be considered when evaluating the findings. Even though the sample size of the dataset was aligned with other studies developing process models (e.g., Lazazzara et al., 2020), it was not easy to locate papers relevant to the specific research question addressed in our study, also considering the current early phase of knowledge of the possible pitfalls of AI in HRM and the lack of relevant empirical studies (Köchling & Wehner 2020). Moreover, owing to the novelty and large amount of ongoing research on this topic, it is impossible to guarantee full coverage of the papers empirically dealing with it. In addition, most of the papers in the first iteration of our taxonomy development process were excluded due to the lack of primary data (e.g., conceptual papers, literature reviews). However, this study does not adopt a systematic review approach; rather, it can be defined as a theory development approach based on redirection (Cronin & George 2023). Therefore, future research would benefit from more relevant empirical studies, particularly those that are sensitive to contextual aspects, in order to empirically test the theory we have developed.

An additional particular concern in the search for pertinent research for our investigation was the ambiguity surrounding the concept of inclusion. It is important to acknowledge that inclusion is related to, yet distinct from, non-discrimination and bias. The former centers on the engagement and integration of specific groups in HRM policies and practices that facilitate the complete utilization of human resources and enhance employees' optimum contribution potential (Georgiadou & Antonacopoulou, 2021; Roberson, 2006). To avoid potential concept drift, we limited our final dataset to studies that concurrently examined specific HRM practices and well-defined target groups (e.g., those distinguished by specific demographic or job-related features). Interestingly, our results indicate that the implementation of AI shapes a new interpretation of inclusion. Consequently, further clarification and operationalization of inclusion, especially inclusion of work, is necessary to create it within contemporary digital workplaces.

Finally, we acknowledge that, following the classification of human-AI collaboration, there is a lack of evidence of the adoption of one type of conjoined agency, namely arresting technologies, in HRM. Considering the four forms of conjoined agency defined by Murray et al. (2021), the use of AI as arresting technology seems difficult to pinpoint. Indeed, to be classified as arresting technology, its behavior (its decisions or constraints) should be encoded in the algorithm so that it is executed every time specific conditions are satisfied, while nevertheless allowing humans to develop the protocol (guidelines on what to do). Instead, what emerged from this research is that AI technology is often only used to assist humans or develop the protocol, in the latter case also with the possibility of letting AI technology make decisions on behalf of humans. There is no evidence of the adoption of arresting technologies in HRM.

Future research may explore if and how arresting technologies (e.g., blockchain) may be applied to HRM and the related mitigation strategies needed for their adoption in HRM. This represents a significant gap in our understanding of AI applications in inclusion contexts, as blockchain

and similar technologies could offer transparent, immutable record-keeping that potentially fosters trust in algorithmic decision-making whilst still maintaining human oversight. Such research could examine whether arresting technologies might provide a middle ground between full automation and human-intensive augmentation approaches.

Moreover, the three forms of AI technology applied in our process model (i.e., assisting, augmenting and automating) could provide the starting point for defining a maturity model for the adoption of AI and the related mitigation strategies. Such a maturity model would map the progression of AI implementation across different HRM functions, identifying the capabilities, resources, and cultural changes needed at each stage of adoption. It could examine whether organizations necessarily proceed linearly through these forms of AI agency or whether different functions within the same organization might simultaneously operate at different levels of AI maturity. This model could also take into consideration the external factors (e.g., organizational characteristics) potentially helping or hindering the positive effects on inclusive algorithmic HRM. Of particular interest would be longitudinal studies tracking how organizational inclusion cultures evolve in response to different AI implementation approaches, and whether certain organizational characteristics predispose companies toward more or less successful integration of AI into their inclusion strategies.

### 5.3. Practical implications

Our taxonomy and process model offer actionable guidance for organizations navigating the increasingly complex landscape of AI-enabled workplace inclusion. By classifying AI systems according to their level of agency and corresponding inclusion risks, our framework supports decision-makers in matching appropriate mitigation strategies to specific HRM applications. This is particularly relevant in domains where AI adoption is most pronounced, such as recruitment, performance management, and career development, each of which presents distinct inclusion challenges that must be proactively addressed.

In recruitment, where our dataset reveals the highest concentration of AI applications, the transition from assisting to automating systems illustrates a steep increase in inclusion risk. AI chatbots used to answer applicant questions represent low-agency technologies with limited inclusion implications, aside from accessibility concerns for digitally disadvantaged candidates. However, organizations that deploy automated resume screening systems powered by natural language processing encounter more profound risks. These systems often reproduce historical hiring biases embedded in the training data. Our framework highlights the need for AI data mitigation strategies, such as fairness metrics and dataset debiasing, to avoid systematically excluding underrepresented candidates.

In performance management, AI systems that automatically evaluate employee output—often using email metadata, task completion metrics, or keystroke activity—exemplify high-agency (automating) technologies with significant inclusion consequences. These systems may undervalue less quantifiable yet organizationally vital behaviors, such as mentoring, emotional labor, or collaborative work. Here, AI design mitigation strategies, including algorithmic transparency, human review loops, and inclusive value mapping, become essential for preventing hidden exclusions.

Career development tools present a third, equally critical frontier. AI-powered learning platforms and internal mobility engines increasingly drive decisions about who is recommended for advancement. These augmenting AI systems personalize career pathways, but if trained on historical promotion data, they risk systematically overlooking women, minorities, or employees in non-traditional roles. Our framework's multilevel lens – inclusion in work, at work, and of work – provides a powerful diagnostic tool to understand how such technologies shape not only individual outcomes but broader organizational advancement norms. Mitigation at this level may include user

transparency, participatory system design, and bias-sensitive recommendation engines.

However, the applicability of our framework is not universal. It depends on several key boundary conditions. First, organizational inclusion maturity is critical. Organizations lacking inclusive leadership, measurement systems, or cultural readiness for addressing bias may find our framework too demanding. Without preexisting efforts to engage with inclusion meaningfully, proposed mitigation strategies risk devolving into symbolic compliance.

Second, technological readiness limits applicability. Organizations with underdeveloped digital HR infrastructure will struggle to implement or audit AI systems effectively, while those that have rushed into AI adoption without inclusion safeguards face different risks—namely, perpetuating bias through opaque, high-agency systems. For these organizations, our framework offers a roadmap for corrective action but requires substantial shifts in both process and mindset.

Third, the degree of AI agency already present in an organization determines the complexity of implementation. Organizations using mostly assisting AI (e.g., chatbots or dashboards) face manageable risks and may require minimal intervention. However, as they move toward augmenting and automating AI, inclusion risks increase in both subtlety and severity. Our model helps identify not only when to act, but how to act differently depending on the AI's role in protocol development and decision-making.

Finally, data quality and feedback mechanisms represent non-negotiable prerequisites for responsible implementation. Effective mitigation depends on access to representative, debiased training data, as well as systems for monitoring unintended outcomes. Organizations lacking these capabilities will struggle to apply the model meaningfully, especially in high-stakes areas like hiring or promotion.

In summary, our framework provides both practical guidance and diagnostic clarity for organizations adopting AI in HRM, but its effectiveness depends on recognizing contextual limitations. By emphasizing not just whether AI supports inclusion but how, when, and under what conditions, our model enables organizations to adopt AI technologies in a more responsible, inclusive, and strategically aligned manner.

## 6. Conclusion

The multi-dimensional framework developed in this study advances understanding of AI-enabled workplace inclusion by examining the complex interplay between technological systems and HRM practices across individual, organizational, and interaction levels. Our findings demonstrate that inclusion manifests differently at each level – in work, at work, and of work – requiring distinct but interconnected mitigation strategies. This reconceptualization extends beyond existing research by revealing how AI adoption fundamentally reshapes inclusion processes through the lens of conjoined agency.

Looking ahead, our model provides a foundation for future research examining how different types of AI technology might be optimally deployed across these three dimensions. Particularly promising areas include investigating how organizations align AI implementation with specific inclusion goals and identifying effective combinations of mitigation strategies for different AI applications and organizational contexts.

Our findings are extensible to the broader discussion about AI and workplace inclusion. The model thus becomes a challenge to purely technological approaches to the implementation of AI, by requiring attention to both the technical design and the social dynamics of the model in order to achieve genuine inclusion. The key practical implications for organizations trying to navigate the complexities of AI adoption in their practices are thus understood from this holistic perspective, and the critical role of human agency in shaping inclusive workplaces is also highlighted.

## CRediT authorship contribution statement

**Alessandra Lazazzara:** Writing – review & editing, Writing – original draft, Methodology, Investigation, Formal analysis, Conceptualization. **Stefano Za:** Writing – review & editing, Writing – original draft, Visualization, Methodology, Formal analysis, Data curation, Conceptualization. **Andri Georgiadou:** Writing – review & editing, Writing – original draft, Formal analysis, Conceptualization.

## Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## Data availability

Data will be made available on request.

## References

Angrave, D., Charlwood, A., Kirkpatrick, I., Lawrence, M., & Stuart, M. (2016). HR and analytics: why HR is set to fail the big data challenge. *Human Resource Management Journal*, 26(1), 1–11. <https://doi.org/10.1111/1748-8583.12090>

Bailey, K.D. 1994. *Typologies and Taxonomies: An Introduction to Classification Techniques*. Sage, Thousand Oaks.

Bashar, M. A., Nayak, R., Kothare, A., Sharma, V., & Kandadai, K. (2021). Deep Learning for Bias Detection: From Inception to Deployment. In *In Communications in Computer and Information Science: Vol. 1504 CCIS*. Springer Singapore. <https://doi.org/10.1007/978-981-6-8531-6-7>

Baumeister, R. F., Vohs, K. D., Aaker, J. L., & Garbinsky, E. N. (2013). Some key differences between a happy life and a meaningful life. *The Journal of Positive Psychology*, 8(6), 505–516.

Bedemarim, R., & Wessel, J. L. (2023). The roles of outcome and race on applicant reactions to AI systems. *Computers in Human Behavior*, 148(June). Article 107869. <https://doi.org/10.1016/j.chb.2023.107869>

Bell, M. P., Ozbilgin, M. F., Beauregard, T. A., & Sürgevil, O. (2011). Voice, silence, and diversity in 21st century organizations: strategies for inclusion of gay, lesbian, bisexual, and transgender employees. *Human Resource Management*, 50(1), 131–146.

Benbya, H., Davenport, T. H., & Pachidi, S. (2020). Special issue editorial: Artificial intelligence in organizations: Current state and future opportunities. *MIS Quarterly Executive*, 19, ix–xxi.

Benbya, H., Pachidi, S., & Jarvenpaa, S. L. (2021). Special issue editorial: Artificial intelligence in organizations: implications for information systems research. *Journal of the Association for Information Systems*. <https://doi.org/10.17705/jais.00662>

Bigman, Y. E., Gray, K., Armstead, M. N., & Wilson, D. (2020). Algorithmic discrimination causes less moral outrage than human discrimination. *Journal of Experimental Psychology: General*. <https://doi.org/10.31234/osf.io/m3rpp>

Blagoev, V., Shustova, E., & Protas, N. (2022). Work motivation of bank employees in case of implementing ai and robots in the bank activities: Comparative analysis of Russia and Kazakhstan. *Ekonometriki Izdeleniya*, 31(7), 63–80.

Budhwar, P., Malik, A., De Silva, M. T. T., & Thevisuthan, P. (2022). Artificial intelligence-challenges and opportunities for international HRM: a review and research agenda. *International Journal of Human Resource Management*, 33(6), 1065–1097. <https://doi.org/10.1080/09585192.2022.2035161>

Cheng, M. M., & Hackett, R. D. (2021). A critical review of algorithms in HRM: Definition, theory, and practice. *Human Resource Management Review*, 31(1), Article 100698. <https://doi.org/10.1016/j.hrmr.2019.100698>

Cipriano, Michele, and Stefano Za. 2022. "Which Digital Transformation Strategy for Non-Profit Organisations Organisations." In: *38th European Conference on Information Systems*, 1–18. Timisoara.

Collins, C., Dennehy, D., Conboy, K., & Mikalef, P. (2021). Artificial intelligence in information systems research: A systematic literature review and research agenda. *International Journal of Information Management*, 60(June), Article 102383. <https://doi.org/10.1016/j.ijinfomgt.2021.102383>

Colquitt, J. A., Scott, B. A., Rodell, J. B., Long, D. M., Zapata, C. P., Conlon, D. E., et al. (2013). Justice at the millennium, a decade later: a meta-analytic test of social exchange and affect-based perspectives. *J. Appl. Psychol.*, 98, 199. <https://doi.org/10.1037/a0031757>

Cronin, M. A., & George, E. (2023). The why and how of the integrative review. *Organizational Research Methods*, 26(1), 168–192. <https://doi.org/10.1177/1094428120935507>

Cutler, Ross, Yasaman Hosseinkashi, Jamie Pool, Senja Filipi, Robert Aichner, Yuan Tu, and Johannes Gehrke. 2021. "Meeting Effectiveness and Inclusiveness in Remote Collaboration." *Proceedings of the ACM on Human-Computer Interaction 5* (CSCW1). DOI: 10.1145/3449247.

Deleczau, S., Eltarr, L., Becuwe, M., Bouxin, H., Boutin, N., & Oullier, O. (2022). Making recruitment more inclusive: Unfairness monitoring with a job matching machine-learning algorithm. *Proceedings - International Workshop on Equitable Data and Technology, FairWare*, 2022, 34–41. <https://doi.org/10.1145/3524491.3527309>

DeSanctis, G., & Poole, M. S. (1994). Capturing the Complexity in Advanced Technology Use: Adaptive Structuration Theory. *Organization Science*, 5(2), 121–147. <https://doi.org/10.1287/orsc.5.2.121>

Ouadrihi, A., & Abdelhadi, A. (2021). Differential privacy for fair deep learning models. In *15th Annual IEEE International Systems Conference*. <https://doi.org/10.1109/SysCon48628.2021.9591252>

Ely, R. J., & Thomas, D. A. (2001). Cultural diversity at work: The effects of diversity perspectives on work group processes and outcomes. *Administrative Science Quarterly*, 46(2), 229–273.

Eubanks, V. (2018). *Automating inequality: How high-tech tools profile, police, and punish the poor*. St. Martin's Press.

Fawcett, J., & Downs, F. S. (1986). *The Relationship of Theory and Research*. Norwalk, CT: Appleton Century Crofts.

Ferrer, X., Nuenen, T. Van, Such, J. M., Cote, M., & Criado, N. (2021). Bias and Discrimination in AI: A Cross-Disciplinary Perspective. *IEEE Technology and Society Magazine*, 40(2), 72–80. <https://doi.org/10.1109/MTS.2021.3056293>

Gama, F., & Magistretti, S. (2025). Artificial intelligence in innovation management: A review of innovation capabilities and a taxonomy of AI applications. *Journal of Product Innovation Management*, 42(1), 76–111.

Georgiadou, A., & Antonacopoulou, E. (2021). Leading through social distancing: The future of work, corporations and leadership from home. *Gender, Work & Organization*, 28, 749–767.

Georgiadou, A., & Damianidou, E. (2025). Inclusion o'clock—Time embodiment in the experiences of disabled employees. *Gender, Work and Organization*, 32(3), 1079–1094. <https://doi.org/10.1111/gwao.13204>

Georgiadou, A., & Syed, J. (2021). The interaction between gender and informal social networks: An East Asian perspective: Gender diversity management in East Asia. *Human Resource Management Journal*, 31(4), 995–1009. <https://doi.org/10.1111/1748-8583.12347>

Georgiadou, A., Özbilgin, M., & Özkançan-Pan, B. (2024). Working from everywhere: The future of work and inclusive organizational behavior (IOB). *Journal of Organizational Behavior*, 45(1307), 1314. <https://doi.org/10.1002/job.2840>

Gregor, S. (2006). The nature of theory in information systems. *MIS Quarterly: Management Information Systems*, 30(3), 611–642. <https://doi.org/10.2307/25145742>

Harris, Christopher G. 2022. "Age Bias: A Tremendous Challenge for Algorithms in the Job Candidate Screening Process." In: *International Symposium on Technology and Society, Proceedings*, 2022-Novem1–5. IEEE. DOI: 10.1109/ISTAS55053.2022.10227135.

Hofeditz, L., Clausen, S., Rieß, A., Mirbabaei, M., & Stiegelitz, S. (2022). Applying XAI to an AI-based system for candidate management to mitigate bias and discrimination in hiring. *Electronic Markets*, 32(4), 2207–2233. <https://doi.org/10.1007/s12525-022-00600-9>

Hoffmann, A. L. (2019). Where fairness fails: Data, algorithms, and the limits of antidiscrimination discourse. *Information Communication and Society*, 22(7), 900–915. <https://doi.org/10.1080/1369118X.2019.1573912>

Huang, M. H., & Rust, R. T. (2018). Artificial intelligence in service. *Journal of Service Research*, 21(2), 155–172. <https://doi.org/10.1177/1094670517752459>

Hu, S., Al-Ani, J. A., Hughes, K. D., Denier, N., Konnikov, A., Ding, L., Xie, J., et al. (2022). Balancing gender bias in job advertisements with text-level bias mitigation. *Frontiers in Big Data*, 5(February), 1–10. <https://doi.org/10.3389/fdata.2022.805713>

Hunkenschroer, Anna Lena, and Christoph Luette. 2022. *Ethics of AI-Enabled Recruiting and Selection: A Review and Research Agenda*. *Journal of Business Ethics*, 178. Springer Netherlands. DOI: 10.1007/s10551-022-05049-6.

Joshi, A., Liao, H., & Roh, H. (2011). Bridging domains in workplace demography research: A review and reconceptualization. *Journal of Management*, 37(2), 521–552.

Kelan, E. K. (2023). Algorithmic inclusion: Shaping artificial intelligence in hiring. *Human Resource Management J*, 1–14. <https://doi.org/10.5465/ambbpp.2021.11338abstrac>

Kellogg, K. C., Valentine, M. A., & Christin, A. (2020). Algorithms at work: The new contested terrain of control. *Academy of Management Annals*, 14(1), 366–410. <https://doi.org/10.5465/annals.2018.0174>

Kim, S., Wang, Y., & Boon, C. (2021). Sixty years of research on technology and human resource management: looking back and looking forward. *Human Resource Management*, 60(1), 229–247. <https://doi.org/10.1002/hrm.22049>

Köchling, A., & Wehner, M. C. (2020). Discriminated by an algorithm: A systematic review of discrimination and fairness by algorithmic decision-making in the context of HR recruitment and HR development. *Business Research*, 13(3), 795–848. <https://doi.org/10.1007/s40685-020-00134-w>

Köchling, A., Riazy, S., Wehner, M. C., & Simbeck, K. (2021). Highly accurate, but still discriminatory: A fairness evaluation of algorithmic video analysis in the recruitment context. *Business and Information Systems Engineering*, 63(1), 39–54. <https://doi.org/10.1007/s12599-020-00673-w>

Kossek, Ellen Ernst, and Shaun Pichler. 2006. "EEO and the Management of Diversity." *The Oxford Handbook of Human Resource Management*, no. September. DOI: 10.1093/oxfordhb/9780199547029.003.0013.

Kubiak, E., Efremova, M. L., Baron, S., & Frasca, K. J. (2023). Gender equity in hiring: examining the effectiveness of a personality-based algorithm. *Frontiers in Psychology*, 14(August). <https://doi.org/10.3389/fpsyg.2023.1219865>

Kutzner, K., Schoormann, T., & Knackstedt, R. (2018). Digital Transformation in Information Systems Research: A Taxonomy-Based Approach to Structure the Field. *26th European Conference on Information Systems: Beyond Digitization - Facets of Socio-Technical Change*. ECIS.

Lazazzara, A., Tims, M., & De Gennaro, D. (2020). The process of reinventing a job: A meta-synthesis of qualitative job crafting research. *Journal of Vocational Behavior*, **116**, <https://doi.org/10.1016/j.jvb.2019.01.001>.

Leonardi, P. M., & Barley, S. R. (2010). What's under construction here?: Social action, materiality, and power in constructivist studies of technology and organizing. *Academy of Management Annals*, **4**(1), 1–51. <https://doi.org/10.1080/19416521003654160>.

Mariani, M. M., Machado, I., Magrelli, V., & Dwivedi, Y. K. (2023). Artificial intelligence in innovation research: A systematic review, conceptual framework, and future research directions. *Technovation*, **122**.

McKelvey, B. (1982). *Organizational Systematics: Taxonomy, Evolution, Classification*. Los Angeles: University of California Press.

Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Gatsbyan, A. (2021). A Survey on bias and fairness in machine learning. *ACM Computing Surveys*, **54**(6), <https://doi.org/10.1145/3457607>.

Miles, Matthew B., and Michael A. Huberman. 1994. *Qualitative Data Analysis*. Second. London: SAGE Publications, Ltd.

Miles, M. B., & Huberman, A. M. (1994). *Qualitative Data Analysis: An Expanded Sourcebook*. Bervley Hills, CA: Sage.

Miles, M. B., and M. Huberman. 1994b. *Qualitative Data Analysis*. Second. London: SAGE Publications, Ltd.

Mor Barak, E. M. (2010). *Managing Diversity—Toward a Globally Inclusive Workplace 2e*. SAGE Publications.

Murray, A., Rhymer, J., & Sirmon, D. G. (2021). Humans and technology: Forms of conjoined agency in organizations. *Academy of Management Review*, **46**(3), 552–571. <https://doi.org/10.5465/amr.2019.0186>.

Newman, D. T., Fast, N. J., & Harmon, D. J. (2020). When eliminating bias isn't fair: Algorithmic reductionism and procedural justice in human resource decisions. *Organizational Behavior and Human Decision Processes*, **160**(March), 149–167. <https://doi.org/10.1016/j.obhdp.2020.03.008>.

Nickerson, R. C., Varshney, U., Muntermann, J., Varshney, J., & Muntermann, U. (2013). A Method for taxonomy development and its application in information systems. *European Journal of Information Systems*, **22**(3), 336–359. <https://doi.org/10.1057/ejis.2012.26>.

Nishii, L. H., Lepak, D. P., & Schneider, B. (2008). Employee attributions of the why" of HR practices: their effects on employee attitudes and behaviors, and customer satisfaction. *Personnel Psychology*, **61**(3), 503–545.

Narskow, S., Damholdt, M. F., Ulhøj, J. P., Jensen, M. B., Ess, C., & Selbit, J. (2020). Applicant fairness perceptions of a robot-mediated job interview: A video vignette-based experimental survey. *Frontiers in Robotics and AI*, **7**, <https://doi.org/10.3389/frob.2020.586263>.

Norskow, S., Damholdt, M. F., Ulhøj, J. P., Jensen, M. B., Mathiassen, M. K., Ess, C. M., & Selbit, J. (2022). Employers' and applicants' fairness perceptions in job interviews: Using a teleoperated robot as fair proxy. *Technological Forecasting and Social Change*, **179**, <https://doi.org/10.1016/j.techfore.2022.121641>.

Pachidi, S., Berends, H., Faraj, S., & Huysman, M. (2021). Make way for the algorithms: Symbolic actions and change in a regime of knowing. *Organization Science*, **32**(1), 18–41. <https://doi.org/10.1287/orsc.2020.1377>.

Perez, H., Levi, A., & Fried, Y. (2015). Organizational diversity programs across cultures: Effects on absenteeism, turnover, performance and innovation. *The International Journal of Human Resource Management*, **26**(6), 875–903.

Piezunka, H., & Dahlander, L. (2019). Idea rejected, tie formed: Organizations' feedback on crowdsourced ideas. *Academy of Management Journal*, **62**(2), 503–530. <https://doi.org/10.5465/amj.2016.0703>.

Raisch, S., & Krakowski, S. (2021). Artificial Intelligence and Management: The Automation-Augmentation Paradox. *Academy of Management Review*, **46**(1), 192–210. <https://doi.org/10.5465/amr.2018.0072>.

Ramezanadadhemoghaddam, M., H. Chi, E. L. Jones, and Z. Chi. 2021. "Inherent Discriminability of BERT." In *Computational Science and Its Applications – ICCSA 2021*, edited by O. Gervasi and Et Al., 12951:256–71. Springer.

Rhue, L. (2018). Racial influence on automated perceptions of emotions. *SSRN Electronic Journal*, **9**, <https://doi.org/10.2139/ssrn.3281765>.

Robertson, Q. M. (2006). Disentangling the meanings of diversity and inclusion in organizations. *Group and Organization Management*, **31**(2), 212–236. <https://doi.org/10.1177/1059601104273064>.

Roy, S. K., Dey, B. L., Brown, D. M., Abid, A., Apostolidis, C., Christofi, M., & Tarba, S. (2025). Business Model Innovation through AI Adaptation: The Role of Strategic Human Resources Management. *British Journal of Management*, **00**, 1–14. <https://doi.org/10.1111/1467-8551.12894>.

Schad, J., Lewis, M. W., Raisch, S., & Smith, W. K. (2016). Paradox research in management science: looking back to move forward. *Academy of Management Annals*, **10**(1), 5–64. <https://doi.org/10.5465/19416520.2016.1162422>.

Shobana, V., & Kumar, N. (2015). Big data - A review. *International Journal of Applied Engineering Research*, **10**(55), 1294. <https://doi.org/10.26634/ijit.6.1.13507>.

Sofian, H., Nur Arzilawati Md, Y., & Ahmad, R. (2022). Systematic mapping: Artificial intelligence techniques in software engineering. *IEEE Access*, **10**, 51021–51040. <https://doi.org/10.1109/ACCESS.2022.3174115>.

Soleimani, M., Intezari, A., & Pauleen, D. J. (2022). Mitigating cognitive biases in developing AI-assisted recruitment systems: A knowledge-sharing approach. *International Journal of Knowledge Management*, **18**(1), 1–18. <https://doi.org/10.4018/IJKM.290022>.

Speer, A. B. (2021). Empirical attrition modelling and discrimination: balancing validity and group differences. *Human Resource Management Journal*, <https://doi.org/10.1111/1748-8583.12355>.

Sühr, Tom, Sophie Hilgard, and Himabindu Lakkaraju. 2021. "Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay of Human and Algorithmic Biases in Online Hiring." *AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society*, 989–99. DOI: 10.1145/3461702.3462602.

Tong, S., Jia, N., Luo, X., & Fang, Z. (2021). The Janus face of artificial intelligence feedback: Deployment versus disclosure effects on employee performance. *Strategic Management Journal*, **42**(9), 1600–1631. <https://doi.org/10.1002/smj.3322>.

Toyoza, Yuushi, Gale Lucas, and Jonathan Gratch. 2021. "Predicting Worker Accuracy from Nonverbal Behaviour: Benefits and Potential for Algorithmic Bias." *ICMI 2021 Companion - Companion Publication of the 2021 International Conference on Multimodal Interaction*, 25–30. DOI: 10.1145/3461615.3485427.

Turja, T., Särkkilä, T., Koistinen, P., Krutova, O., & Melin, H. (2022). Job well robotized – Maintaining task diversity and well-being in managing technological changes. *European Management Journal*, <https://doi.org/10.1016/j.emj.2022.08.002>.

Broek, E. van den, A. Segeeva, and M. Huysman. 2019. "An Ethnography of Fairness in Practice." *Fortieth International Conference on Information Systems, Munich*. [https://aisel.aisnet.org/icsis2019/future\\_of\\_work/future\\_work/6](https://aisel.aisnet.org/icsis2019/future_of_work/future_work/6).

von Krogh, G. (2018). Artificial Intelligence in organizations: New opportunities for phenomenon-based theorizing. *Academy of Management Discoveries*, **4**(4), 404–409.

Vrontis, D., Christofi, M., Pereira, V., Tarba, S., Makrides, A., & Trichina, E. (2022). Artificial intelligence, robotics, advanced technologies and human resource management: A systematic review. *International Journal of Human Resource Management*, **33**(6), 1237–1266. <https://doi.org/10.1080/09585192.2020.1871398>.

Will, P., Krpan, D., & Lordan, G. (2022). People versus machines: Introducing the HIRE framework. *Artificial Intelligence Review*, <https://doi.org/10.1007/s10462-022-10193-6>.

Winters, M. F. (2013). *From Diversity to Inclusion: An Inclusion Equation*. In B. F. Ferdinand, & B. R. Deane (Eds.), *Diversity at Work: The Practice of Inclusion* (pp. 205–228). John Wiley & Sons.

Za, S., Spagnolletti, P., Winter, R., & Mettler, T. (2018). Exploring foundations for using simulations in IS research. *Communications of the Association for Information Systems*, **42**(1), 268–300. <https://doi.org/10.17705/1CAIS.04210>.

Za, S., Lazazzara, A., Shaba, E., & Scornavaca, E. (2023). Is Artificial Intelligence Disrupting Human Resource Management? A Bibliometric Analysis. In T. Bondarouk, & J. Meljerrink (Eds.), *Research Handbook on Human Resource Management and Disruptive Technologies*. Cheltenham: Edward Elgar.

Zhang, N., Wang, M., Heng, X., Koenig, N., Hickman, L., Kuruzovich, J., Ng, V., et al. (2023). Reducing subgroup differences in personnel selection through the application of machine learning. *Personnel Psychology*, **76**(4), 1125–1159. <https://doi.org/10.1111/peps.12593>.

Zhou, Jianlong, Sunny Verma, Mudit Mittal, and Fang Chen. 2021. "Understanding Relations Between Perception of Fairness and Trust in Algorithmic Decision Making." *Proceedings of 2021 8th IEEE International Conference on Behavioral and Social Computing, BESC 2021*. DOI: 10.1109/BESC53957.2021.9635182.

Zuidervijk, A., Chen, Y. C., & Salem, F. (2021). Implications of the use of artificial intelligence in public governance: A systematic literature review and a research agenda. *Government Information Quarterly*, **38**(3), Article 101577. <https://doi.org/10.1016/j.giq.2021.101577>.

Dr. Alessandra Lazazzara (alessandra.lazazzara@unimi.it) is an Associate Professor of Organization Theory and Human Resource Management at the University of Milan. Her research interests include AI and HRM, workplace inclusion, and job crafting. Her research has been published in several international journals, including *Journal of Vocational Behavior*, *The International Journal of Human Resource Management*, *International Journal of Project Management*, and *Personnel Review*, among others. She is an Associate Editor of *Gender, Work and Organization* and serves as Vice President of IAIS (the Italian Chapter of the Association for Information Systems).

Dr. Stefano Za (stefano.za@unich.it) is an Associate Professor of Organization Studies and Information Systems at the University of Chieti-Pescara (Italy). He is the president of the Italian chapter of the AIS. He has been a member of program committees for national and international conferences in the IS domain (e.g. ICIS, ECIS, and AMCIS). His research interests include the analysis and design of digital artefacts and organizational systems. He is currently focused on digital business transformation affecting people and organizations. He has published in international conference proceedings (e.g. ICIS, ECIS, HICSS), journals (e.g. *Journal of Information Technology*, *Government Information Quarterly*, *Information and Management*, *Communications of the Association for Information Systems*, *Information Systems and e-Business Management*, *Technology Forecasting and Social Change*), and book series.

Dr. Andri Georgiadou (andri.georgiadou@nottingham.ac.uk) is the Director of Equality Diversity and Inclusion and an Associate Professor in the Nottingham University Business School. Her work revolves around two main themes: fostering inclusion and diversity in the workplace and exploring how AI and algorithmic solutions can contribute to more inclusive and sustainable work environment. Dr. Georgiadou is an Associate Editor of *Gender, Work & Organization* and the Chair of the Diversity and Equality EuroMed Academy Research Interest Group. Her research has been published in leading peer-reviewed journals, including the *Journal of Organizational Behavior*, *Human Resource Management*, *Human Resource Management Journal*, *Gender, Work & Organization*, *Journal of International Management*, and *European Management Review* among others. She has also contributed to numerous book chapters and has edited books. She actively collaborates with a wide range of organizations, providing support to achieve sustainable results in enhancing inclusion within the hybrid workplace.