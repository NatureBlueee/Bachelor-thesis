

【教育创新探索】

# 学校人工智能应用亟需有效的社会监督

(英)本·威廉森<sup>1</sup> (美)亚历克斯·莫尔纳<sup>2</sup> (美)费斯·博宁格<sup>2</sup>

(1.英国爱丁堡大学 数字教育研究中心, 英国 爱丁堡 EH8 8AQ; 2.美国科罗拉多大学  
博尔德分校国家教育政策中心, 美国 科罗拉多博尔德 80309-0249)

肖俊洪 译

【摘 要】尽管缺乏完善的研究基础和监管保护措施,技术公司和相关政府部门依然在向学校推广人工智能(AI)应用。目前在学校采用未经监管的AI应用程序已给社会与个人的自主带来了严重威胁。多年以来学界不断地发出警告,技术的应用所造成的问题也非鲜见,AI时代之前的数字技术在学校的广泛使用产生诸多显而易见的风险,这些技术使决策变得不透明,导致学生数据被挪为他用。在缺乏有效的社会监管的情况下引进不透明和未经证明的AI系统和应用程序很可能使原有的问题变得更加严重。如果立法者和其他相关人士不拿出周密的想法和措施进行干预,以消除这些广泛存在的风险,则很可能出现各种危害。文章探讨了这种情况下可能出现的危害,呼吁学校领导在政策制定者尚未完全了解AI并出台保证AI在学校的应用受到社会有效监督和控制的政策之前暂缓推广AI应用程序。

【关键词】人工智能;企业化学校改革;教育技术;管理;学校商业主义

【中国分类号】G434

【文献标识码】A

【文章编号】2096-1510 (2024) 05-0001-13

译者导读: 这是一篇促使我们以理性的态度对待AI在教育中的应用的文章。澳大利亚莫纳什大学(Monash University)的教授尼尔·塞尔温(Neil Selwyn)经常被视为反对教育技术的“斗士”,但在我看来,这是一种误解,误解了他的良好初衷。他著述甚丰,贯穿其中的是他的教育技术理念——要时刻警惕围绕教育技术的炒作,以理性和负责任的态度对待教育技术在教育中的应用。2011年他在为《英国教育技术期刊》(British Journal of Educational Technology)撰写的一篇卷首语中<sup>①</sup>,提出了“技术悲观主义”这个概念。他所提倡的悲观主义不是被动地接受命运的安排,而是不断地积极探索其他可能性。长期以来,教育技术领域充满技术乐观主义,人们坚信技术对教育来说有百利而无一弊,因此不愿意“往坏处”想。稍有理性的人都应该明白这种思维定势的危害。技术越先进,技术乐观主义所带来的风险可能越大。我认为当今尤为需要塞尔温教授的技术悲观主义论。

本期推出的这篇文章是由美国科罗拉多大学博尔德分校国家教育政策中心(National Education Policy Center, University of Colorado Boulder)总结的关于教育人工智能在中小学的应用的报告,也可看作是一篇文献综述,主要目的是给美国各级政府的教育政策制定者提出关于如何对待教育人工智能的建议。这篇文章所提出的建议对其他国家也有借鉴意义,且不局限于中小学教育。

文章首先简要回顾了AI的三个重要发展阶段, AI在教育中的应用和教育人工智能差强人意的研究现状并介绍了这个领域的新发展。文章在归纳和分析相关文献的基础上讨论了AI给教与学以及行政管理工作带来的风险,指出“当务之急是要重点保证AI的开发符合伦理,保障学生权益并体现社会责任”。因为从相关研究的结果看, AI在教育中的应用并非是不可避免的,教育人工智能既不是不可或缺的,也不总能产生好的结果。文章最后分别给美国的联邦、州和学区三级教育政策制定者提出如何通过政策手段保证AI在教育领域得到负责任的应用的建议,并且呼吁“在政策

[DOI编码]10.19605/j.cnki.kfxyyj.2024.05.001

制定者全面掌握AI知识并出台政策和法规确保AI在学校的运用受到社会的有效监督和控制之前”，学校应该暂停使用AI技术。事实上，原文的标题包含“是时候暂停使用”（Time for a Pause）的字眼，但是为了不引起读者不必要的误解，文章的中文标题没有包含这个意思。

近年来，技术悲观主义的观点得到越来越多学者的认同，虽然诸如此类的声音还无法从根本上与教育人工智能的“主流话语”抗衡。如2024年6月在瑞士举行的国际远程开放教育理事会领导力峰会（ICDE Leadership Summit 2024）上，该会主席马克·尼尔斯（Mark Nichols）阐述了AI给高等教育带来的三大挑战。

① 教育意义上的挑战（educational challenge）：如学生最终是否会丧失批判性思维能力？走AI的捷径会不会导致学生思维能力的退化？诸如此类的风险值得我们警惕。

② 组织意义上的挑战（organisational challenge）：如AI将如何变革高等教育领域和大学的运行机制？如果教学工作全部由AI完成，大学该如何制定其运行和教学的策略？凡此种均对教育机构的管理构成了严重挑战。

③ 本体论意义上的挑战（ontological challenge）：在尼尔斯斯看来，这是最大也是最令人不安的一个挑战。随着教育人工智能的功能越来越强大，数字鸿沟进一步加剧。据估计2023年全球有33%的人口（25亿人）用不上因特网，另一方面，世界上大多数民族的文化并没有在AI大型语言模型中得到体现。因此，“数字殖民化”（digital colonisation）将越来越严重。这种情况促使他提出了第三个挑战：AI将影响到我们如何看自己，如何看我们的世界以及如何表达我们的好奇心，“数字殖民化”将极大地改变我们的思想和我们子孙后代的思想。OpenAI之类的工具给我们呈现的是一个扭曲的世界。接受这种教育令人忐忑不安，因为我们的存在、认知、探索、人性、文化以及传承和发展民族遗产的权利等都因此将变得“岌岌可危”。

我在领英上点评尼尔斯的观点并指出我认为还有一个重大挑战——元挑战（meta-challenge），即一切挑战之挑战。既然人工智能在教育中的应用存在很多不确定因素，不管是好处还是风险（危害），都有待进一步经过严谨科学研究的证明，那么为何还要争先恐后地追赶这趟大篷车呢？难道不应该更加谨慎些吗？毕竟，这场大试验的对象是人，人的事无小事。英国伦敦大学学院的研究者利奥·哈夫曼（Leo Havemann）也跟帖指出：“教育和人类正在未经伦理审查和知情同意的情况下被当成试验品。”

衷心感谢三位作者慷慨授权这项成果在本刊发表！

## 一、引言

公立教育是民主的公民生活必不可少的公共物品和私人物品。因此，公众必须能够通过透明的社会治理结构对学校进行有意义的监督。人工智能（AI），文章主要指涉及使用机器学习算法对行为或结果进行测量和预测的数据系统以及大型语言模型这两种AI，由于缺乏社会监督正在日益发展成为学校的“标配”。现有证据表明学校“全盘照收”未加监管的AI应用程序会对社会和个人的自主构成严重的威胁。

2023年春，数字技术巨头们曾警告说AI会给“社会和人类带来巨大风险”（Center for AI Safety, 2023a; 2023b; Future of Life Institute, 2023），呼吁监管AI发展并暂时停止实施相关应用程序（Brodkin, 2023; Edwards, 2023）。但与此同时，Google、Microsoft、Meta和Amazon却在争先恐后地把AI融合到它们的平台上（Lee, 2023; Wall Street Journal, 2023）并且力求规避监管（Kak, Myers West, & Whittaker, 2023;

Perrigo, 2023）。技术产业界则开展强大营销攻势（Andreessen, 2023），大众传媒很快便充塞着技术产业界的炒作和对AI的潜能与隐患的种种猜测（Kapoor & Narayanan, 2022）。铺天盖地吸引眼球的预言、企业的自私行为、夸大其词的营销口号以及不辨是非的报道掩盖了快速采用AI所造成的直接危险（Gebru, Bender, McMillan-Major, & Mitchell, 2023; Morozov, 2023）。在这种背景下，所谓AI能够促进教与学转型的论调主导了有关AI对教育影响的讨论（Devlin, 2023），这点不足为奇。

数字化技术已经被广泛应用于学校管理和教学中（Nichols & Dixon-Roman, 2024）。正因如此，教育的决策日益不透明，学生的隐私权受到侵害，学生数据被用于非教育用途（Boninger & Molnar, 2020）。如果缺乏有效的社会监督，把AI系统和应用程序引入到教育中很可能进一步加剧以上这些问题，甚至造成更多问题（Giannini, 2023; Laird, Dwyer, & Grant-Chapman, 2023）。

因为专门为学校开发的现有平台和应用程序需

要更新以融合AI技术,我们面临的直接危险不再是遥远的大变革,而是AI模型和应用程序必将被融合到学校各项工作的过程和程序中,其后果是私有企业对公立教育的结构和内容拥有越来越大的控制权,从而让他们能进一步强化监视活动并加剧现有偏见和不平等(Gebu et al., 2023; Williamson, Komljenovic, & Gulson, 2024)。几十年来,学术界致力于研究适用学校的AI模型(Porayska-Pomsta, 2023),然而今天把AI以及随之而来的各种风险强行推进课堂的却是商业企业(State of Big Tech, n.d.)。

向教育领域推销AI,体现在半个世纪以来工商、政治和意识形态领域试图努力将教育私有化和商业化的行动上(Molnar, 2005; Williamson & Hogan, 2020)。虽然我们都清楚AI会给教育带来很多风险,但是商业公司及其研究人员和政府却在尚未出台具有法律效力的监管框架以确保AI程序透明并能够在接受有效的社会监督和控制的情况下积极推进AI的使用(Holmes & Tuomi, 2022; McQuillan, 2023)。如果考虑到那些想私有化和商业化教育的企图,这种情况是意料之中的。但是学校为此却要承受巨大的压力,把AI当成实现学校“升级”的不二选择(U.S. Department of Education, Office of Educational Technology, 2023)。

计算机科学家和软件开发人员主要关注技术性的工程问题(D'Agostino, 2023),而企业领导和投资者则优先考虑利润而非公共利益(Yelenevych, 2022)。虽然他们没有教育知识和经验且很可能在经济上受益于AI在学校的使用,但是教育工作者却被要求必须相信这些人最有资格设计并引领教育转型(Garcia & Nichols, 2021)。

## 二、文献综述

虽然AI的很多底层数学过程和机械程序可以追溯到19世纪那些早期的计算和制造模型,但AI这个术语是20世纪50年代计算机科学家首次提出来的(Whittaker, 2023)。根据美国斯坦福大学的定义, AI通常指由计算机执行一般只有人才能完成的任务(Boden, 2016)。但是今天AI已经成为一个难以清晰定义的术语,其含义缺乏广泛共识(Suchman, 2023; Tucker, 2022)。正因如此,营销人员能够给几乎所有数字化过程或产品贴上“AI”的标签

(Siegel, 2023)。

### (一) AI的发展——从基于规则的系统到生成式AI

过去几十年,很多人和企业在发展AI技术时有明确的目的和策略(Elish & Boyd, 2018)。他们的工作随时都可能影响AI在学校的应用,尤其是随着AI的快速发展和AI营销力度的加大,情况更是如此(Williamson, & Eynon, 2020)。

AI发展经历了三个主要阶段。第一个阶段始于20世纪,当时研发的AI系统以源自专家知识的预设规则或算法为基础(Council of Europe, n.d.),且过去二十年间出现了机器学习。机器学习是使用复杂数学过程(即学习算法)分析海量数据(即大数据)以辨识隐藏于数据中的趋势或共性(Alpaydin, 2016)。对大数据进行分析便能够准确辨识趋势。机器学习程序正是基于这种假设预测复杂事件和现象中的行为的,包括人的行为(Broussard, 2019)。但是,这种分析的准确性喜忧参半(AIAAIC, n.d.)。例如:有一项研究邀请六组数据科学家利用大量数据和先进机器学习工具预测儿童的人生结果,然而没有一组预测的准确性达到可以接受的合理水平,导致人们对AI在诸如教育这样的社会政策领域的应用提出严重质疑(Hao, 2020)。

自从2022年底以来,机器学习进一步发展到了生成式AI的阶段(Clark, Milmo, & Blight, 2023; Visual Storytelling Team & Murgia, 2023),引起了公众、媒体和政界的极大兴趣(Ball, 2023a; Lazar, 2024)。生成式AI应用更加复杂的学习算法,根据从万维网或其他渠道收集的数据生成原创性文本、图片和音频(Baack, 2024)。这个方向的研究已经研制出一系列程序模型,这些模型现在常被称为基础模型,因为可以对它们进行改造以适合各种用途(Jones, 2023),如大型语言模型(Clark et al., 2023)。

大型语言模型的支持者宣称它们能完成日常笔头工作,因此能够提高工作效率(Mollick, 2023b)。很多人对此倍感兴趣。此外,据称教师和学生同样能受益于这些模型(Hill, 2024; Phillips, 2023)。但是,这些许诺能否实现尚无法确定(Acemoglu, 2024; Garcia, Logan, & Nichols, 2024)。如虽然它们经常给出正确的回答,但是并

不能理解语言所表达的意义 (Warner, 2024)。所以, 不能想当然地认为模型生成的答案都是准确无误且有价值的 (Hao, 2020)。事实上目前阶段的AI经常是“愚蠢的” (Bridle, 2023), AI会编造事实, 混淆分析结果, 信息来源不稳定以及生成涉及社会、公众和文化等方面重要事情的危险错误言论 (Ananny, 2024)。例如: 2023年为了演示 Bard 这款大型语言模型作为教育工具的功能, Google用它回答有关詹姆斯·韦勃 (James Webb) 太空望远镜的问题, 结果出现了有违事实的情况, 导致 Google 的市值大幅下降 (Pearl, 2023)。

AI的开发过程也引发了人们的担忧。出于技术和商业上的原因, AI模型不透明, 即没有公开详细说明其机制 (AI Now Institute, Ada Lovelace Institute and Open Government Partnership, 2021)。例如: 对10款基础模型的分析结果表明, 公众难以获取相关方面的信息, 包括数据来源、模型研制所需的计算量以及分析算法的详情等 (Bommasani et al., 2023)。很多机器学习模型是“黑盒子”模型, 即其机制太复杂而难以解释或者说从根本上是解释不了的 (Bagchi, 2023)。有些模型的机制则因其开发商拥有所有权而不必向公众披露 (Rudin, 2019)。

“黑盒子”AI基础模型是大科技企业向所有领域推广其拥有所有权的模型、扩大全球市场份额和获取最大利润等计划的核心 (Knight, 2021)。在缺乏有效的社会监督和管理的情况下, 目前使用生成式AI系统只能借助科技巨头的数据库、高性能计算能力和财政资源。因此, 少数几家企业拥有所有权的AI模型很可能成为绝大多数AI应用程序的基础 (Kak & Myers West, 2023)。同时, 监管者和立法者将疲于应急制订事后的监管保护措施 (Satariano & Kang, 2023)。

### （二）AI在教育中的应用

自从20世纪60年代以来, 学界和业界一直在探索AI的教育应用。如今教育人工智能 (AIED) 是一个重要的研发领域 (Doroudi, 2022; Hwang, Xie, Wah, & Gašević, 2020; Springer Nature, 2023)。如今我们在向学校推销AI应用程序, 而在20世纪60年代和70年代则是推销“智能辅导系统”和“计算机辅助教学”系统 (Alkhatlan & Kalita, 2018)。

从21世纪初开始, 研究者收集、储存和分析了

海量教育数据, 旨在为制定机构策略和教学策略提供依据 (Lockyer, Heathcote, & Dawson, 2013)。这些工作如今被视为AIED且被教育技术行业迅速商业化 (Buckingham Shum & Luckin, 2019; Gašević, Siemens, & Sadiq, 2023)。大多数AIED应用程序依靠大数据和机器学习进行各种预测并执行自动化任务, 如预测某个学生可能不及格或推荐“个性化”干预措施以达成预期的学习目标 (Gillan, Eynon, Chiabaut, & Finkel, 2023)。

AIED的研究提出了检验其有效性的不同方法并对这些方法进行了验证, 结果表明AIED对于可测量的学习进步有一定效果, 如能够提高测试和小测验的成绩 (Zheng, Niu, Zhong, & Gyasi, 2023)。人们对AIED这方面的潜在优势倍感兴趣, 公立和私立机构由此而慷慨资助相关研究, 以探索使用AI提高学习成绩的方法 (Nietzel, 2023)

AIED可以被视为主要是关乎技术的事情, 因此由科学家和科技公司提出解决方案最为合适。但是, 这种假设越来越受到质疑。有研究者认为如此狭隘的技术视角可能导致相关的AI政策和教学方法的失败 (Holmes, 2023a)。他们指出AI存在于社会、经济和政治环境中, 而这些方面的因素会影响AI的发展和应用 (Gulson, Sellar, & Webb, 2022; Williamson, Eynon, Knox, & Davies, 2023)。不同的教育利益相关者 (包括AIED研究者、教育技术企业家、商界领导和政策制定者, 对待AI的态度将对AI在学校的运用产生重要影响 (Eynon & Young, 2021)。

由风险投资和私人股权投资提供资金的企业家和公司正在争先恐后地向教育领域推广AI, 这必将导致可供选择的AI应用程序局限于在财政上有利害关系的教育利益相关者“钦定”的范围 (Komljenovic, Williamson, Eynon, & Davies, 2023)。不管是教育技术小型初创公司还是科技巨头都把AI视为一个机会 (Berger, 2023; HolonIQ, 2023), 纷纷通过操控对AI的炒作向学校推销个性化学习系统、教案自动生成器和“教学辅导机器人”等教育产品 (Blackboard, 2022; Duolingo, 2023; Gasvoda, 2023; Instructure, 2023; Microsoft, 2023)。虽然能够证明这些产品产生良好教学效果的证据仍然十分匮乏 (Wu & Yu, 2023), 但是企业家和研究者依然宣称

它们行之有效 (Gašević et al., 2023; Singer, 2024)。

政策制定者经常夸大AI的作用,号召学校实施“数字化转型”(Broadband Commission for Sustainable Development, 2020; U.S. Department of Education, Office of Educational Technology, 2010),而且没有考虑社会、经济、法律或伦理方面的因素(Schiff, 2022)。这与当今被视为需要管理上优先考虑的问题(即绩效监控、问责、效率和效果)相吻合,因为所有这些都要求广泛采集学生数据(Wyatt-Smith, Lingard, & Heck, 2021)。虽然自从20世纪90年代以来学校便开始实施基于考试(成绩)的评价制度(Anagnostopoulos, Rutledge, & Bali, 2013),但是随着AI被用于持续监控和评估学生的学习,这些制度会得到发展和强化(Rupp & Lorrie, 2023)。如此一来,商业性AI系统将越来越多地以私营参与者的身份服务公立教育,因为学校、学区和政府把重要的任务、功能和责任移交给第三方的技术供销商负责(Crawford & Schultz, 2019)。

AI的教育用途,不管是已有的实践还是潜在的作用,都不只是教与学实践具有创新性的技术的附加成分或解决学校现有的教学或行政管理问题的工程学方案,而是多种力量推动的结果,包括科学家长期以来试图测量、预测和支持学习过程和结果的种种努力,商界想通过向学校兜售产品获利的愿望和在降低成本的情况下达成提高学校效率、加强对学校问责的治理目标。从目前情况看,各方寄予AI的厚望已经开始汇集成一幅由AI驱动的学校教育的愿景,即使用商业性产品考核学生的学习结果,实现教学自动化以及决定学生学习路径。

### (三) 研究差强人意

虽然AIED领域的研究数量可观,机器学习的研究蓬勃发展,但是鲜有证据支持AI能够让学校“转型”这种主张(Selwyn, 2019a)。虽然AIED研究成果丰硕,但这些研究往往聚焦学生个体的学习投入和表现的测量方法(根据标准化成就测验的成绩进行评估)或诸如设计更加复杂的算法和提高机器学习效果之类的“工程学”问题(Selwyn, 2019b)。

总的看来,AIED的研究往往没有得出明确的研究结论,缺乏独立性和规模化,也没有涉及教育目标之类更加根本的问题(Holmes & Tuomi, 2022)。AIED研究常旨在推广教育转型,即提高可以测量的

个人学习结果这种观点,即使AI“能行”的证据非常有限(Selwyn, 2024)。事实上,这些研究把各种曾经有过深入研究、非常细致的学习理论简化为一个数学模型(不管这个模型多复杂),而忽视公立学校应该追求什么目标和与目标相一致的课程设置这些颇具争议的问题(Perrotta & Selwyn, 2020)。AI能够解决重大教育问题的说辞在很大程度上是推测而非基于证据,如解决师资不足、学生成绩不理想和教育不公平等等问题(Holmes, Persson, Chounta, Wasson, & Dimitrova, 2022)。

更加严重的是机器学习研究存在重大的研究方法缺陷,因此数以百计的研究其效度是受到质疑的(Gibney, 2022)。由于存在这些缺陷,当前研究者对机器学习应用程序在各领域的有效性价值“过于乐观”(Kapoor & Narayanan, 2023a; Navarro et al., 2023)。这种情况尤为令人担忧,因为人们有理由怀疑各种商业营销的说辞,也不会相信AI系统在各领域(包括教育)的广泛应用是建立在科学证据的基础上的(Ball, 2023b)。

最后,因为机器学习模型的运行将产生高昂的计算费用,大多数研究者不得不依靠主流AI公司提供的系统开展研究(Gulson & Webb, 2023),而AI研究的赞助者通常也正是这些公司(Menn & Nix, 2023)。因此,研究不可避免地依赖企业的资源、资金和商业行为,从而使得这些公司不但能够在很大程度上影响AI的发展,而且也会左右学术研究(Whittaker, 2021)。另一方面,这种情况还会削弱研究的可重复性,而可重复性是研究的一个重要方面,目的是验证相关研究的效度。如果一家公司改变或停止支持某一个模型,研究者就无法继续重复开展相关研究(Kapoor & Narayanan, 2023b)。如此一来,整个研究基础便不稳定、不可验证,因此不能作为评价后续模型的基础。

## 三、AI新发展

2022年11月,OpenAI发布了ChatGPT,由此开发和推销生成式AI平台和应用程序的竞赛变得更加激烈。已经拥有AI基础模型的公司,如OpenAI、Google、Meta和Amazon都在快速扩张,在它们已进入的领域(包括教育)扩大应用规模(Davies, Eynon, Komljenovic, & Williamson, 2022)。虽然媒

体关注的焦点是学生使用ChatGPT，但是这些公司正在迅速扩大AI的教育用途，包括：

① 向学校销售数字系统的使用权。如Amazon向学校和学区销售云计算设施的使用权，以便它们能够使用其AI系统分析机构和学生的数据(Williamson, Gulson, Perrotta, & Wittenberger, 2022)。

② 给学校已经在使用的产品增加AI功能。如Google已经开始把AI引入其为学校开发的Workspace套件中，包括在全球广泛使用的Classroom平台(Herold, 2020; Stumpf, 2024)。

③ 把AI应用程序融入新产品或升级的产品中。如OpenAI与教育技术公司合作把其语言模型融合到其推销的服务中，如AI教学助理(Franzen, 2023)，并正在探索ChatGPT自己的教育应用程序(Davis, 2024; Tong, 2023)。

④ 把AI嵌入到搜索引擎和课堂上常用的其他日常应用程序中。如Microsoft基于OpenAI的技术，把新的AI应用程序引入其Office软件中，并向教育界推广(Brewin, 2023)。

目前向教育界推广的主要是具备自动生成语言和图像能力的AI应用程序在教学中的应用(Skates, 2023)，例如：学生使用ChatGPT之类的AI工具完成作业(Klein, 2023a)和促进个性化学习的辅导机器人模拟一对一辅导(Young, 2024)，教师使用AI“助理”设计教案、给学生作业评分或检查学生学习进展情况等(Singer, 2023a)。支持者坚称这些AI应用程序能够给学生提供个人的学习帮助并能为教师节省时间(Chen, 2024)。企业负责教育板块的部门则制作各种教师指南和培训材料(Mollick, 2023a)，如OpenAI推出《使用AI开展教学》的指导，帮助教师使用其应用程序(OpenAI, 2023b)。

国际组织和政府部门对这些所谓的好处信以为真，因此支持AI产品在学校的运用。如经济合作与发展组织担心AI在完成很多认知性任务方面很快会超越人类，因此呼吁马上改革正式教育系统以便学生能够学到与AI互补的技能而非完成那些很快可能实现自动化的任务的技能(OECD, 2023)。美国教育部教育技术办公室则鼓励教师参与AI教育应用程序的开发和评估(US Department of Education's Office of Educational Technology, 2023)。凡此种种

终将迫使教师接受未经检验和不透明的商业性AI应用程序以免“掉队”(Klein, 2023b)。

近来出台的监管建议也没能让AI快速进入学校的步伐放缓(Miao & Holmes, 2023)。美国总统拜登于2023年10月发布的行政命令中提出了一系列相关要求，包括加强美国联邦政府对基础模型测试的监督、保护数据隐私和“促进创新和竞争”。这份行政命令要求教育部创建一个“AI工具包”，包含“由人负责对AI做出的决定进行适当检视，设计更加可信、安全并且能遵守与教育环境下的隐私相关的法律法规的AI系统，制订专门针对教育的保护措施”(Biden, 2023)。

但是，该行政命令的配套清单却对美国教育部提出与上述任务不同的要求，要求美国教育部“创建支持教师使用AI使能(AI-enabled)的教育工具(如个性化辅导)以发挥AI变革教育的潜能”(The White House, 2023)。虽然美国教育部对照“人机回环”(keep humans in the loop)的目标创建其AI工具包和指导，但是这种方法基于两个假设：第一，“学习”仅指数字化程序能够测量的东西；第二，AI程序能够并应该从可以测量这个方向“优化”学习(U.S. Department of Education, Office of Educational Technology, 2023)。

## 四、讨论与分析

如前所述，虽然仓促地把未经检验的技术应用到课堂教学中存在各种风险、缺乏实施标准或监管制约(Dusseault & Lee, 2023)，但是学校持续面临采用AI实现学校“现代化”的压力(Merod, 2023b)。虽然AI应用程序被当成可以解决教与学问题和提高学校行政管理流程效率的途径进行推销，但是其模型存在各种局限、问题和风险(Weidinger, 2022)。如果仓促地把AI应用到学校的教学和管理工作中，很可能不仅不能解决学校的很多问题，甚至会让这些问题变得更加突出，AI潜在的弊大于潜在的利。

我们可以从两款被大力推广的应用程序窥见学校盲目采用AI的后果。一是基于大型语言模型、旨在实现教学的个性化和自动化的辅导聊天机器人(Schwartz, 2023)，一是基于海量学生数据进行预测、教学资源定制和教学活动推荐以及自动干预教

学过程的自适应学习平台 (Cormie, 2022)。这些产品带来的后果包括教师需要投入更多时间而非省时;违背了教学规律让“学习”的定义狭义化;弱化了教师专业知识和技能以及师生关系的重要性;课程包含错误内容,进一步加剧存在于教育教学中的偏见和歧视。鉴于存在这些风险,在学校采用或扩大AI应用程序的应用之前,政策制定者和教育部门领导应该思考如何确保负责任地使用AI、AI的潜在好处是否大于需要付出的代价以及AI在教育中的应用是否真的不可避免。

### (一) AI给教与学带来的风险

#### 1. 教与学的简化

可汗学院 (Khan Academy) 开发了与AI相关的几款产品并于2023年3月开始加大了推广力度,重点向家长、教师和学区推销Khanmigo“教学辅导机器人”(Khan, 2023; Khan Academy, 2023a; OpenAI, 2023a),声称Khanmigo能够与学生进行个性化和“会话式”的教学互动,利于促进“个性化学习”(Phillips, 2023)。

Khanmigo采用OpenAI的GPT-4语言模型,能够生成定制的教学内容、教案和测试,还能完成诸如监控学生进步之类的工作。其营销资料上保证向学生提供一对一“虚拟辅导”并成为教师的“私人教学助理”,声称能够理解学生的教育和职业目标并针对个人提供单独帮助,也能向教师提供教案、学生进步情况报告(以及相应的建议)等资源从而节省教师时间并提供“私人礼宾服务”,让教师能“做更多事情”(Khan Academy, 2023b)。

可汗学院的营销资料还说Khanmigo能够模仿真人教师的思维和反应并做出决定,因此既能满足需要教师关注的学生的需求也能满足因为其他工作而无法关注到具体某个学生需要什么的教师的需求。我们还能够从这些营销资料中了解到这款应用程序中植入的涉及教与学本质的假设,而这些假设不可避免地会被强加给使用这款产品的真人教师、学生和相关人士。

个性化学习应用程序的算法体现的是狭隘的学习观和受到极大限制的教学模式 (Boninger, Molnar, & Saldaña, 2019)。这些算法的“学习”通常指的是学生完成基于计算机的活动并能够给出“正确”或“可接受”的答案(这些答案不能超出预先设定

的、数量有限的答案范围) (Mollick, 2024)。AI应用程序则会向教师报告学生完成可以测量的任务的“进步”情况并提出如何提高学生“成绩”的策略建议 (Perrotta & Williamson, 2018)。任何采用这些应用程序的学校都默认学习在很大程度上可以被简单化为有限的行为反应,排除以学生为中心的更加复杂的习行为 (Kerssens, 2022)。

像Khanmigo这样的AI聊天机器人所采用的学习理论重点考虑的是基于学生数据、能以数字形式呈现的那些方面(即有“客观”衡量标准的表现),它不信任教师拥有能够对自己的学生做出专业判断的能力,它贬低教师的实践经验、学科专业知识和技能、对学生个体的背景或社会环境的了解以及师生课堂交流的重要性 (Kerssens & van Dijck, 2022)。

这些AI产品的一个所谓好处是能够减轻教师的行政工作量,让他们能把更多时间用在教学上。例如:Khanmigo的广告说“您又能够不折不扣地享受属于您自己的夜晚”(Khan Academy, n.d.)。这样的广告对工作量超负荷的教师而言能达成的营销效果很可观,他们很可能希望能够把疲于应付的行政工作交给电脑应用程序来完成,他们也很可能因为把学生交给理论上能够支持学生学习的技术,让学生不必在拥挤的教室里轮流等待教师解答问题而感到如释重负。

然而,把这些工作交给教学辅导机器人意味着师生之间多了一个“中间人”,而且机器人与真人教师不同,不是学科教学专家,也不了解学生以及他们所处的环境。再者,虽然生成式AI能够实现如同真人一般的交流,但是它只能对程序产品预设的参数范围内的提示词做出回应。结果,教师还必须花时间了解这些局限并掌握正确提示方式,还要教学生如何提问和验证答案的真伪 (Khan Academy, 2023c)。如此一来,即便AI实现了某些行政工作的自动化,但它也很可能把其他非常耗时的压力和重担强加到教师身上 (Singer, 2023b)。

这些由私营商业机构推销的AI产品并没有减轻教师负担,相反,教师最终被迫沦为技术的仆人,必须保证技术在课堂上得到顺利应用,因此导致教师的工作性质变得更加复杂。为了迎合AI特性而变革教育的核心系统,即教学、课程和考核,这要求

教育工作者付出巨大精力改变自己的职业实践。与此同时,教师对于这些变革是否有真正的价值、是否应该实施等方面没有发言权。学术界与教师和学生合作开展AIED的研究和开发,但是这些努力会被淹没在当下供学校使用的商业性AI应用程序充满鼓动性和揣测的炒作中(Possible.fm, 2024)。因此, AI给学校在各种决策中应该拥有的机构自主权,尤其是教师对自己的教学实践做出专业决策的自主权带来了严重挑战。

#### 2. 课程包含错误内容

生成式AI经过训练能够生成即使可能包含假信息却仍然貌似可信的文本(Angwin, 2023; Bender, Gebru, McMillan-Major, & Schmitthell, 2021)。这对教师而言是一个重大挑战。例如:如果教师使用生成式AI备课并生成资源,那么课堂上就可能充塞误导学生的错误信息或假信息(Ferlazzo, 2023)。

这个问题不大可能得到解决,因为随着AI自动生成的内容在因特网上蔓延,生成式AI应用程序赖以运行的数据的质量很可能进一步变差(Smith, 2023)。整个信息环境泛滥着AI生成的内容,因此无法确定任何来自网上的内容的权威性或真实性(Kirschenbaum, 2023),网上内容不适合用于教育目的或具有误导性(BBC, 2023)。这才是真正的危险。

根据有关生成语言和图像的AI研究文献(Tiku, Schaul, & Chen, 2023),底层数据的不稳定和谬误导致AI混乱和不可靠(Lykiardopoulou, 2023)。毫无疑问,这就是为什么Khanmigo建议用户不要依赖其所提供的内容,而是要使用其他资源,如“教科书、论文或其他可信来源的东西”(Khan Academy, 2023c)进行验证,同时也能说明为什么Khanmigo提出要限制学生使用时间,因为“长时间的使用更有可能影响AI的表现”(Khan Academy, 2023d)。我们不禁要问:既然这款应用程序或任何AI应用程序的使用似乎显而易见会增加工作量,也可能影响教学效果,为什么还要花时间使用它们呢(O'Boyle, 2023)?

主导教育技术和高科技市场的企业成为获取在线学习内容的主要渠道,由此引发了一些迄今尚未得到解决的问题。例如:它们的AI应用程序提高课堂上教授的内容的质量了吗?诸如OpenAI这类公司是使用网上抓取的材料训练AI,教师和学生可以

在多大程度上信任其所生成的内容?“表现欠佳”的教学辅导机器人对学生有何影响?检查AI生成内容的质量应该是谁的责任?Khanmigo悄悄把检查错误和提供反馈以便程序员能够予以纠正这些“没有摆上台面”的工作转移给早已不堪负荷的教师完成(Singer, 2023b)。因此,很可能会出现这种局面:这些错误被忽视了,学校通过AI获取信息的准确性越来越低(Cooke, 2023)。

AI给学校传授的知识的准确性不足,由此波及课程材料的可信度不高,这带来了实实在在的威胁。鉴于学生以及很多教师已经从网上获取了内容,如通过诸如Teachers Pay Teachers (Shelton, Koehler, Greenhalgh, & Carpenter, 2022)等网站,可能越来越难以把权威、来源准确的内容与由AI生成的、貌似可信但却是错误的内容区别开来。随着OpenAI发布可以由用户自行定制的、从ChatGPT衍生而来的各种GPT,用户个人能够创建自己的教育应用程序并在网上免费分享,但却几乎无需经过任何质量检查和控制,因此很可能导致假信息的问题更加严重(OpenAI, 2023c)。

#### 3. 可能进一步加剧偏见和歧视

因为AI模型的训练数据要么是因特网上的数据,要么是历史数据,这些数据所包含的偏见可能会转移到教育应用程序中(David, 2023)。例如:2022年11月OpenAI发布ChatGPT时,教师担心学生用它作弊完成书面作业,因此教育技术公司就研发出AI自动检测系统。2023年初,广泛用于检测学生剽窃行为的Turnitin增添了AI检测功能,声称这样便能够检测到AI生成的文本独有的标记(Mathewson, 2023)。然而,独立研究发现这些AI检测系统很容易出错,导致错误指责学生作弊的情况激增(Fowler, 2023),而且这些指责会不成比例地体现出对非英语母语者的偏见,因为他们往往用简单的句子写作,而AI则把这些句子标识为可疑情况(Mathewson, 2023)。

在美国许多州使用的自动AI作文评分程序很容易出现对某些族群和种族的偏见,并且错误地给包含复杂语言和结构的作业打高分,全然不顾其内容或质量(Feathers, 2019)。这是因为从任何意义上讲AI程序“不懂”一篇作文的好坏而只能寻找与高分或低分相对应的模式。同样的,GoGuardian通过

算法和AI监控学生的社交媒体和网站浏览活动以发现是否涉及“可疑”的内容或行为。它经常把各种不涉及性或暴力的材料标识为有害或危险类别,包括一般教育网站的材料(Kelley & Prince, 2023)。

AI剽窃检测系统、作文自动评分系统和监控学生的平台均是建立在一个错误假设的基础上,即自动分析数据能够客观地呈现学生所做的事和行为。对于一些尚存疑问的情况,AI用机器学习算法生成的自动化甄别可疑行为数量的分数代替真人教师的斟酌决定,并据此对作业、成绩和学生行为做出评价。学生很难对这些评价提出质疑,因此,如果把AI融合到学校教育结构体系中,很有可能导致当今教育的很多不平等现象进一步加剧而不是消除。不透明的机器学习系统将基于不可知的数学运算做出影响学生学习过程、结果和前途的决定。

### （二）AI给行政管理工作带来的风险

#### 1. AI增加成本

学校常见的学习管理系统(如Google Classroom、Blackboard和Canvas)已经开始把AI融合到原来的平台上(Blackboard, 2022; Instructure, 2023)。Google Classroom以其一整套名义上“免费”的软件和低成本的Chromebook硬件在市场上占据了主导地位(Kerssens, Pangrazio, & Nichols, 2023; Singer, 2017)。它发布用于Classroom、基于AI的自适应学习插件(学校要支付额外费用)并计划用生成式语言AI升级Classroom(Williamson, 2021)。Practice Sets是Google为教育开发的基于AI的自适应学习系统,而Duet AI则是专门为教师开发的“协作伙伴”(Google, n.d.)。Google Classroom的使用会对教学产生多方面影响,除此之外,它进一步把AI和自动化融合到学校运作的许多工作中,有可能对行政管理工作产生重大影响(Gulson, Perrotta, Williamson, & Witzenburger, 2021)。

最大的影响是:当我们把决策权拱手相让给由技术公司控制的不透明的机器学习系统时,涉及机构很多重大问题的决策的理论依据便无从知晓。例如:Google Classroom与其他数以百计的教育技术产品融为一体,能够同步获取一所学校的学生信息系统数据(Perrotta, Gulson, Williamson, & Witzenburger, 2021)。它提供Google云服务(单点登录、身份管理和设备管理)以及剽窃检测、自

动评分、教学模板、学生分组和方便“数据驱动决策”的行政管理分析技术。这些管理系统促使学校的控制权从公立机构转移到私营公司,因为它们扮演集中通道的角色,即一所学校的所有数字化活动都必须经过它们,这样一来,教师或行政管理人员便难以明白基于这些数据的决定是如何做出的(Perrotta et al., 2021; Veale, 2022)。

因为运行AI的成本高,学校使用AI程序必然会被要求学校为越来越多的教学和行政管理的相关AI应用程序支付使用费。所谓AI能够降低人工开支从而为学校省钱的许诺很可能变得不切实际,因为学校可能不付使用AI设施的不菲费用。换言之,行政管理应用程序的使用可能仅是把原来的开支转移给垄断技术的公司而已,不会为学校节省开支。

Khanmigo和Google Classroom已经证明了这一点。可汗学院向学区提供Khanmigo,目前的收费是每位学生每年60美元,他们声称之所以收取这么高的费用,是因为使用了OpenAI的GPT-4(Singer, 2023b)。同样的,学区也必须支付Google Classroom用AI升级的费用。要使用其最新的自适应学习应用程序Practice Sets,学校必须从免费的基本模式改为收费的高级套餐(Kieczka, 2023)。换言之,技术公司正在想方设法让学校财政预算承担与AI相关的高昂计算费用(和增加公司收入)(Komljenovic, 2021)。

#### 2. AI对学生隐私造成更大威胁

AI应用程序必须通过收集和汇聚数据才能运行。在这个过程中,它们使数字化监视和隐私侵犯成为学校的常态(Yu & Coulddy, 2021)。在实践中,教育技术公司日常通过Google Classroom之类的应用程序大肆收集数据,远远超出正常运作所需的范围(Hooper, Livingstone, & Pothong, 2022)。

虽然AIED支持者强调数据驱动的行政管理系统非常高效,但是这些AI应用程序本身可能存在对公平造成威胁、与隐私相关的问题(Fair, 2022)。这是因为AI模型是建立在海量数据集的基础上,这些数据可被用于对个人进行画像、比较和评估,因此很可能根据反映他们个人生活的“统计档案”(statistical dossiers)对他们做出歧视性的决定(Solove, 2001)。因此,数字技术,尤其是侵犯隐私的AI模型的一个重大危害是:它们可能因使用

包含历史性偏见和歧视的数据集而复制和加剧当下教育的不平等现象 (Perrotta, 2023a)。例如:如果一个大数据集显示某些边缘化群体在历史上成绩欠佳,那么其软件在今后可能会对这些群体的学生持有偏见,把他们列为“高危”对象并剥夺或限制他们获取信息和资源的机会 (Whitman, 2020)。

此外,学校数据系统很容易出现漏洞、被黑客入侵、被植入勒索软件和遭受拒绝服务攻击 (Levin, 2022)。例如:Illuminate这家教育技术公司曾经发生过数据泄漏,导致至少100万公立学校学生的教育数据受到影响,迫使美国纽约市教育局要求学校停止使用其产品 (Singer, 2022)。学校数据系统储存了非常详细和私密的学生信息,包括个人的数据、人口统计数据、分数、考勤、行为以及其他需保密的信息。教育技术产品的AI能力越来越强大,这可能进一步加剧上述风险,因为为数众多的技术公司以超大规模收集着学生的数据,却在数据隐私保护方面含糊其辞 (Merod, 2023a)。

#### 3. AI降低决策透明度并弱化问责要求

把AI应用于学校行政管理事务会导致决策的透明度降低和弱化问责要求 (Ada Lovelace Institute, AI Now Institute, & Open Government Partnership, 2021)。目前学校所使用的很多数字产品既不透明也不用承担责任,因为根据现行的法律法规,技术公司这些产品的内在运行机制受到专利保护,不必公之于众 (Citron, 2023)。

AI比其他数字技术更加不公开透明 (Burrell, 2016)。黑盒子般的机器学习和AI模型非常复杂,所以我们经常无法解释或理解它们的输出结果 (Rudin & Radin, 2019)。虽然在很多情况下更加简单和易懂的统计模型能够得出同样准确的结果,但是技术公司依靠销售其受专利保护的模型的使用权而获利,用户一旦使用这些模型,就不得不相信它们,只能被迫接受无法验证其结果这一事实 (Ruddin, 2019)。如果系统出错,可能永远都不会被发现或得到纠正,公众只能承担其所造成的后果。例如:用于远程监考的人脸识别系统经常不能准确辨识某些学生或将他们的行为错误标识为可疑行为,然而学生很难质疑并改变这些结果 (Broussard, 2023)。

在教育领域高风险的决策中,允许这些令人无

法理解的模型完成重要的行政管理工作,必然意味着学校的领导和教师不能对涉及课堂和机构的决定行使判断权,提供决策理由或承担责任 (Perrotta, 2023b)。

### (三) AI未来的发展方向

#### 1. AI是否得到负责任的开发?

面对如此快速推出供学校使用的AI应用程序的局面,当务之急是要重点保证AI的开发符合伦理、保障学生权益和体现社会责任 (Holmes et al., 2022)。开发负责任的AI (Responsible AI)就是要保证产品安全和可信赖,旨在使个人、社区和社会能够从中受益,减少危害 (Responsible AI UK, 2023)。然而,到目前为止,几乎没有证据证明这些价值观在教育应用程序中得到妥善体现 (Kousa & Niemi, 2023)。遗憾的是,AIED研究者往往不重视这些问题或把它们交给教育技术行业和政策中心去处理 (Porayska-Pomsta, 2023)。因为学术界安于现状,加之教育技术公司有钱有权,因此,引导AI开发方向的是商业性目标而非教育考量 (Pea et al., 2023)。

从负责任管制的角度讲,AI公司必须承诺以透明和负责任的方式设计产品并监察、了解和减少AI在各种环境下的持续影响。尤为令人担忧的是产生“不可逆转和严重后果”的自动化决策 (Vallor, 2023)。例如:目前正在开发通过识别情绪评估一个人是否在撒谎和作弊的技术 (Ryan-Mosley, 2023)。但是,这项技术从根本上讲是不准确的,而一个不准确的判断可能会对相关人士的生活造成严重后果,如学生是否作弊或证人是否撒谎。负责任地管理AI可能会拖慢这些技术的开发进度或无限期地暂停开发。

虽然有一些项目已经针对安全和可信赖的AI开发和问责制定了相应的原则、框架或检查清单 (AI Now Institute, 2023),但是这些努力可能受到各种形式的行业游说和行为的操纵从而影响了其实施的范围或可能性 (Floridi, 2019; Shazeda, Jaźnińska, Ahlawat, Winecoff, & Wang, 2023)。如果让更多开发和使用AI的人或组织共同承担保证产品安全的责任,不由商业机构及其技术人员独自负责,这会减少上述风险 (Coldcut, 2023; Gentelet, 2024)。

实施负责任管治AI政策的诸多障碍之一是成

本。追求利润的技术公司会尽可能把更多的成本开支转移到公众身上以获得最高的投资回报率。如果要把AI置于社会监督之下，必然要求AI的所有权归公众所有或设置一套综合监管机制并且要给其足够的资金支持以保证达成既定目标。问题是钱从何处来？

这种监管与过去五十年来实行的放松管制和私有化的政策相悖，要从根本上重新思考政府与商业利益的关系。这无疑会受到只顾私利的群体的攻击，他们会认为这样一来成本高昂且扼杀创新、降低效率从而加以指责。虽然这些论调可能适用于个别情况，不具有普遍性，也不是不言而喻的事实。

从教育的角度讲，要对AI进行负责任的管治就必须做出远比现在多的承诺，只有行业发布的负责任开发AI的原则是不够的。同时，也要舍得花钱对AI在课堂上的应用效果进行持续监测，甚至在有充分理由的情况下还可能要求放缓开发进度或无限期暂停。例如：如果AI供应商未能提供足够证据证明其产品能够产生好结果，或这些产品能实现专业判断的自动化但可能产生负面后果，或它们未能妥善处理与教育直接相关的AI伦理问题，在这些情况下就不应该让AI供应商把产品引进学校。

#### 2. AI的使用是不可避免的吗？

AI产品被引进学校的的速度之快令人吃惊。如上所述，部分原因是学校面临着需采用最新科技产品以实现“现代化”的压力。目前已经有一种所谓的共识，即AI在学校中的使用是不可避免的。美国纽瓦克公立学校系统（Newark Public Schools）的教育技术总监在接受《纽约时报》（New York Times）采访时这样解释他所管辖的学区为什么要采用Khamnigo：“让我们的学生接触AI是非常重要的，因为AI不会（从学校）消失”（Singer, 2023b）。

这实际上是要求学生充当技术公司的试验对象。一开始学区只需支付很低的“入场费”，因此，尤其是那些缺乏优质学位的学区可能都愿意赌一把，希望技术创新能扭转它们差强人意的局面。然而，在下赌注之前要思考一些根本性问题，这点很重要。计算机科学家约瑟夫·维森鲍姆（Joseph Weizenbaum, 1978）在五十年前就已经表示出相关忧虑，其基本观点是：除非我们知道某种技术的使用是必要的且能够产生好结果的，否则不应该使用

任何技术。我们认为AI也不例外。

#### 3. AI的使用是必要的吗？

一百年来技术界想方设法从学校身上牟利。在20世纪20年代，机械式“教学机器”发明者声称他们能够帮助学生按照自己的进度学习并减轻教师的行政管理事务的负担（Watters, 2021）。这也是现在数字个性化学习平台的口号，因为学生无论何时何地想学习，这些平台“就在他们身边”（Boninger et al., 2019）。此外，如上所述，技术公司正迫不及待地把AI融合到个性化学习和其他平台上，开展虚拟“辅导”，生成教案和试卷，收集、整理供行政管理决策的数字。

AI程序或许能够高效地完成一些有意义的工作，尤其是各方面都非常明确的工作。但是我们并不清楚如果不使用AI以及不接受AI所产生的不良后果，我们是否能做得一样好或更好。这个问题极少被提出来讨论（Pelletier, 2023）。向学校推出的AI产品众多，但是我们目前无法肯定这些产品的使用是否真的是基于正当理由的。

#### 4. AI能产生好结果吗？

AI程序不可靠（Narayanan & Kapoor, 2023a），往往无中生有（Weise & Metz, 2023）或崩溃（Smith, 2023），而且几乎可以肯定会产生很多意料之外的后果（Holmes, 2023b）。AI程序本身之所以不稳定，部分原因要归结到它们在被使用的过程中的“学习”，所以AI程序变化很快且不可预测（Narayanan & Kapoor, 2023b）。机器学习领域的研究很少针对诸如社会需要或潜在不良影响，而是重点关注系统表现、效率和新颖性这些有商业价值和工程学意义的东西（Birhane et al., 2022）。AI程序在没有考虑公众或没有获得公众同意的情况下传播了这些内嵌的商业和工程学价值观。这是我们现在面临的一种风险。

AI算法既不客观也不中立（Crawford, 2016; Freeguard, 2020; O’Neil, 2016）。让事情变得更加复杂的是，即使我们知道AI的训练数据包含哪些假设和某一个程序是如何权衡其所使用的数据的（Rudin & Radin, 2019），无论是其开发者还是用户仍无法弄明白程序在实际使用中的基于海量数据所输出的结果（Molnar, Boninger, Noble, & Meenakshi, 2023）。结果，即使是一个了解“黑盒子”AI程序

的初始编码的人也无法解释它是如何得出这些结果的。毫无疑问，机器学习不可避免地以各种不可理解的方式产生可能不为人所理解的、不真实或不准确的结果 (Floyd, 2023)。使用这些系统，不管是教师还是行政管理人员都不能理解、解释或证明程序为什么得出这样的结论，更不用说审核或记录其有效性 (Broussard, 2023)。

我们现在正在创造这样一个世界：基于AI的学习应用程序决定学生某阶段的学习路径或剥夺他们继续求学的机会，学生对这些决定束手无策。教师和学校行政管理人员对学生或他们的家长针对AI决定的质疑也爱莫能助。学校的教学和行政管理工作离不开AI，随着AI被融合到学校日常各项工作，AI给学校的民主管治造成直接且严重的威胁 (McQuillan, Jarke, & Pargman, 2023)。从这个意义上讲，我们认为AI在学校的运用不能产生“好的”结果。

综上所述，我们没有信心得出如下结论：总体上讲AI对于学生、教师或学校而言是必不可少的或者能够产生好结果的。相反，现有的证据显示在学校全面使用AI会严重危害社会与个人的自主。采用不透明、未经验证和在很大程度上不受监管的AI系统和相关应用程序将可能迫使师生身不由己地变成教学和行政管理自动化的这场大规模试验的对象，而这场试验必定会造成意想不到的后果且可能产生不良的影响 (Taylor, 2024)。允许AI左右学校的各项工作和程序将是一场灾难，因为一旦“深陷其中”，想摆脱其影响的唯一方法是把它们彻底卸掉，不再使用任何AI系统。

## 五、给政策制定者的建议

为了预防在教学和行政管理工作中仓促使用AI所造成的不良且深远的影响，在政策制定者全面掌握AI知识及出台相关政策和法规以确保AI在学校的运用受到社会的有效监督和控制之前，我们建议学校领导暂停AI技术在学校的使用。任何AIED的开发都应该在“负责任AI”的框架下进行，与学校合作开展。为此，我们提出以下三点建议。

### （一）美国联邦政策制定者

停止把AI作为现实学校教学与行政管理转型和现代化的手段进行推广。

在建立合适的监管架构之前禁止学校采用AI教育应用程序。

制定法规禁止学校使用不向州监管机构公开工作原理的任何技术，包括“黑盒子”AI模型，除非开发商已经向监管机构提交了充分的理由和解释，说明某种技术是达成某一个清晰界定的、正当的学校目标的唯一方法并说明这种技术是如何改进现行教育实践的。

减少要求学校和教师上报数据的规定，减轻学校在行政管理事务中使用AI的压力。

### （二）美国州政策制定者

成立一个独立的政府机构负责监督供学校使用的数字教育产品的质量。这个机构的职责是在学校使用之前和使用之后定期检查和审批学校拟使用的任何数字教育产品的教学方法和程序，规定学校使用的任何数字教育产品，包括融合AI的产品的程序必须既透明又便于接受检查。

营造有利于教师能花更多时间跟学生在一起的课堂环境，如实施限制班级人数的法规，让教师不再被迫想方设法让学生安静地把时间花在使用数字产品上。

### （三）美国的学区政策制定者

不使用AI教育应用程序，直至：①对透明性和责任的严格要求被纳入技术问责制总体计划中；②向社会提供强有力的证据或详细且清楚的解释，说明使用这些应用程序的教育实践要比没有使用数字技术的教育实践效果更好。

## 原文出处

Williamson, B., Molnar, A., & Boninger, F. (2024). Time for a pause: Without effective public oversight, AI in schools will do more harm than good. Boulder, CO: National Education Policy Center. (2024-05-21) <http://nepc.colorado.edu/publication/ai>.

## 注释

① Selwyn, N. (2011). Editorial: in praise of pessimism—the need for negativity in educational technology. *British Journal of Educational Technology*, 2(5), 718. DOI: 10.1111/j.1467-8535.2011.01215.x.

## 参考文献

因版面限制本文参考文献详见以下网址：<http://nepc.colorado.edu/publication/ai>.

## 作者简介

本·威廉森 (Ben Williamson)，英国爱丁堡大学数字教育研究中心高级讲师、联执主任，《学习、媒体和技术》(Learning, Media and Technology) 联合主编。主要研究兴趣：教育技术、数据和教育政策。出版有《教育大数据：学习的数字未来、政策和实践》(Big Data in Education: The Digital Future of Learning, Policy and Practice) (2017) 和《2024世界教育年鉴：算法、自动化和人工智能时代教育的数字化》(World Yearbook of Education 2024: Digitalization of Education in the Era of Algorithms, Automation and Artificial Intelligence) (2024) (联合主编) 等著作。

亚历克斯·莫尔纳 (Alex Molnar)，美国科罗拉多大学博尔德分校研究型教授，联合创办国家教育政策中心并担任中心主任，兼任该中心教育商业主义研究室联执主任。主要研究兴趣：社会和教育政策与实践。2015年与费斯·博宁格 (Faith Boninger) 合

著《售罄：学校营销是如何威胁孩子们的福祉和影响他们的教育的》(Sold Out: How Marketing in School Threatens Children's Well-Being and Undermines their Education)。

费斯·博宁格 (Faith Boninger)，社会心理学博士，美国科罗拉多大学博尔德分校研究型教授，国家教育政策中心出版部经理，教育商业主义研究室联执主任，联合负责该研究室发布《学校商业主义趋势年度报告》(Annual Report on Schoolhouse Commercialism Trends)。

## 译者简介

肖俊洪，教授，《远程开放教育SpringerBriefs系列丛书》(SpringerBriefs in Open and Distance Education) 主编，《开放、远程和数字教育期刊》(Journal of Open, Distance, and Digital Education) 联合创刊主编。

# Time for a Pause: Without Effective Public Oversight, AI in Schools Will Do More Harm than Good

Ben Williamson<sup>1</sup>, Alex Molnar<sup>2</sup> and Faith Boninger<sup>2</sup>

(1. Centre for Research in Digital Education, University of Edinburgh, Edinburgh EH8 8AQ, UK; 2. National Education Policy Center, University of Colorado Boulder, Boulder, Colorado 80309-0249, USA)

**Abstract:** Technology companies and political agencies are promoting the use of artificial intelligence in schools, despite an inadequate research base and lacking regulatory protections. The current adoption of unregulated AI applications in schools poses a grave danger to democratic civil society and to individual freedom and liberty. Years of warnings and precedents have highlighted the risks posed by the widespread use of pre-AI digital technologies in education, which have obscured decision-making and enabled student data exploitation. Without effective public oversight, the introduction of opaque and unproven AI systems and applications will likely exacerbate these problems. This article explores the harms likely if lawmakers and others do not step in with carefully considered measures to prevent these extensive risks. The authors urge school leaders to pause the adoption of AI applications until policymakers have had sufficient time to thoroughly educate themselves and develop legislation and policies ensuring effective public oversight and control of school applications.

**Keywords:** artificial intelligence; corporate school reform; EdTech; regulation; school commercialism