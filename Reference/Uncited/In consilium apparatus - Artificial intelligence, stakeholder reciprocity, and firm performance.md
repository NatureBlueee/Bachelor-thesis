

![Elsevier logo featuring a tree and two figures.](935eed7aa61f7777f62cfc032e11bee9_img.jpg)

Elsevier logo featuring a tree and two figures.

ELSEVIER

Contents lists available at ScienceDirect

###### Journal of Business Research

journal homepage: [www.elsevier.com/locate/jbusres](http://www.elsevier.com/locate/jbusres)![Journal of Business Research cover image.](0538daaa5583c23e17db3a12f2281a55_img.jpg)

Journal of Business Research cover image.

![Check for updates icon.](4f4b52340aaccb1bcf733468dca9ee03_img.jpg)

Check for updates icon.

# In consilium apparatus: Artificial intelligence, stakeholder reciprocity, and firm performance

Douglas Bosse<sup>a</sup>, Steven Thompson<sup>a,\*,</sup>, Peter Ekman<sup>b</sup><sup>a</sup> Robins School of Business, University of Richmond, 102 UR Drive, Richmond, VA 23173, USA<sup>b</sup> Mälardalen University (www.mdh.se), Box 883, 721 23 Västerås, Sweden

## ARTICLE INFO

### Keywords:

Stakeholder theory  
Artificial intelligence  
Distributional justice  
Procedural justice  
Interactional justice  
Firm performance

## ABSTRACT

Firms are increasingly using forms of AI to serve stakeholders across various business functions, resulting in both positive and negative outcomes. Stakeholder theory explains how firms create and destroy value via their stakeholder encounters, making it an ideal foundation for understanding AI deployment on firm-level performance. As AI continues to evolve, both when it comes to the activities and roles it takes and the stakeholders it affects, the AI-stakeholder framework developed herein identifies and situates key managerial decisions related to the adoption and deployment of AI that drive the firm's likelihood of creating or destroying value through stakeholder encounters. The AI-stakeholder framework focuses on stakeholder justice and is supported by testable propositions about the conditions most likely to affect the outcomes of incorporating AI into business processes. The framework also supports future research and practical managerial guidance by articulating the challenges and potential of AI for managing stakeholder encounters.

## 1. Introduction

The recent expansion of artificial intelligence (AI) represents a fundamental change in business and management practice (Berente et al., 2021). Many AI computer programs have been granted agency, meaning they autonomously perceive environmental cues, make decisions, and take actions to pursue goals (Fleming, 2019; Aleksander, 2017; Russell & Norvig, 2016). The shift to granting agency, or decision making, to AI computer programs has far-reaching implications for business where expected gains are accompanied with risks and ethical considerations (Iansiti & Lakhani, 2020; Kaplan & Haenlein, 2019; Martin, 2018). Firms are now using forms of AI to scan their stakeholders across the range of business functions (Gregory et al., 2021; Huang & Rust, 2018; Ford, 2015) where it ideally creates value for all the stakeholders (Loureiro, Guerreiro & Tussyadiah, 2021). Accessible, for example, is a technology company that uses AI to scan websites and then convert the content to make it more accessible for people with disabilities. Unfortunately, AI can also destroy value for a firm – and at exceptionally high speed. Microsoft developed a Twitter chatbot called Tay that used machine learning to engage people through conversation. In less than 24 h, Tay learned to make misogynistic and racist remarks to users. Such a system could take just seconds to destroy a firm's

reputation and turn its stakeholders against it. As AI becomes more sophisticated, we can expect AI to replace human actors within organizations and gradually assume responsibility for more internal and external business functions (Makarius et al., 2020; von Krogh, 2018; Davenport & Kirby, 2016).

Recent research on the effects of AI on businesses and organizations emphasizes that AI is in constant development (Berente et al., 2021) which will affect different stakeholders at different points of time (Huang & Rust, 2018) and will challenge how stakeholders engage with firms (Robinson et al., 2020). As AI progresses, the effects of replacing people with AI will render stakeholder effects (cf. Markus, 2017) that in turn affect the adopting firm's performance. To develop a theoretical lens that allows for capturing the effects on firms as AI progresses, we leverage stakeholder theory as a broad foundation for examining and understanding this phenomenon. Stakeholder theory explains how firms manage value-creating relationships within a network of stakeholders (e.g., customers, employees, suppliers, financiers) who often have conflicting interests (Freeman, 1984). According to this theory, a firm can be understood as a nexus of incomplete and, at least partly, implicit contracts with parties that affect or are affected by the firm (Alchian & Demsetz, 1972; Asher, Mahoney & Mahoney, 2005). Stakeholders are assumed to willfully enter these contracts with the firm based on an

\* Corresponding author.

E-mail addresses: [dbose@richmond.edu](mailto:dbose@richmond.edu) (D. Bosse), [sthomp3@richmond.edu](mailto:sthomp3@richmond.edu) (S. Thompson), [peter.ekman@mdh.se](mailto:peter.ekman@mdh.se) (P. Ekman).

expectation that they will be treated fairly given the perceived justice norms or customs in their respective markets (Macaulay, 1963; Werder, 2011), and they reciprocate towards the firm in ways that reward it for exceptionally favorable treatment and punish it for exceptionally unfavorable treatment (Bosse, Phillips & Harrison, 2009). In aggregate, this reciprocity across the stakeholder network directly affects firm-level performance (Bosse et al., 2009). Although the extant logic of stakeholder theory has been applied to all firms independent of their technology use, the infusion of AI can be expected to change the forms and types of interactions and hence affect stakeholders differently which calls for more research on AI and stakeholders (Markus, 2017).

The purpose of this paper is to answer that call by offering a theoretical AI-stakeholder framework of key managerial decisions related to the adoption and deployment of AI that directly drive the firm's likelihood of creating or destroying value through stakeholder encounters. The applicable philosophy of science values conceptual research that "seeks to bridge existing theories in interesting ways, link work across disciplines, provide multi-level insights, and broaden the scope of our thinking" (Gilson & Goldberg, 2015: 128). Our approach is to combine two streams of research (stakeholder theory and AI) to generate both theoretical and managerial implications of using AI as a new form of information system (IS) that is granted agency (Berente et al., 2021; Di Vaio et al., 2020; Markus, 2017) – and that can subsequently become the stakeholder's mandatory way of interacting with the firm. This style of research is "focused primarily on theoretical development and does not present data and/or analysis for purposes of theory testing" (Yadav, 2010, p. 5). As such, we invoke a structured and deductive discussion on AI and its wider implication on firms and markets with an emphasis on stakeholder effects. Previous research focuses on a single specific stakeholder as for example a user (Foster, Gaschler & Guillani, 2017), customer (Kumar et al., 2019), or employee (Makarius et al., 2020). However, there is a need for understanding AI's role in the relationship between any stakeholder and firm performance (cf. Gligor, Pillai & Golgeci, 2021).

The first contribution of this paper is to provide a theoretical foundation and testable propositions that are novel for understanding key AI decisions related to where and how firms deploy AI that is linked directly to firm performance. We define the fundamental challenge facing firms seeking to utilize AI as a function of the complexities associated with perceiving stakeholder expectations (Loureiro et al., 2021), formulating a value proposition that satisfies the firm and the stakeholder, and allowing each stakeholder to positively and negatively reciprocate. Thus, *the proposed framework takes the adopting firm's perspective* and clarifies the key factors that will affect the firm. While AI may offer cost reductions, improved speed of service, and benefit from network effects, that is, learning from multiple users that can be instantly incorporated into future interactions (Hagiu & Wright, 2020), it can also present a risk in the form of misinterpretations and biases (e.g., based on unsolicited behaviors, see Perez-Vega et al., 2021) which are instantly applied in multiple subsequent user interactions. Recognizing these dynamics, scholars will identify multiple reasons for infusing this new framework into the continued research on AI. The resulting insights will be helpful in mitigating some of the historically high risks of AI mistakes (Martin, 2018), unforeseen problems (Leonardi, 2021; Gligor et al., 2021), and IT project failure (Dwivedi et al., 2015).

The second contribution is to conceptually refine and expand the recognized AI challenge of autonomously perceiving environmental cues before formulating and executing a plan of action (Gligor et al., 2021; Robinson et al., 2020; Russell & Norvig, 2016). As AI becomes more sophisticated (Makarius et al., 2020), it will increase its capacity to include in situ cues in its algorithms. In the context of stakeholder encounters, we refine the scope of AI by detailing the antecedents to value proposition development and we expand it by including the critical activities of post-encounter perception and assessment of the stakeholder's reactions to the firm. Firms that fail to deploy AI technologies

capable to capture both pre- and post-encounter information from stakeholders risk initiating a cycle of value destruction. The third contribution expands the spatial scale of our framework from a single stakeholder encounter to multiple encounters of different types; see Raisch and Krakowski (2021) on the importance of expanding spatial scale and see Berente et al. (2021) on AI as an evolving phenomenon. Allowing and responding to positive and negative reciprocal behavior is especially important and challenging for AI because it requires the detection and interpretation of stakeholder responses that may not be a sole function of a specific bilateral action or decision. We argue that perceiving and accurately interpreting reciprocal stakeholder behaviors are central AI activities that get easier for AI to perform as the firm extends its use of AI to more types of encounters with a focal stakeholder or stakeholder group. Taken together, this pragmatic refinement to the understanding of AI challenges makes our theoretical framework relevant for firms that adopt and deploy AI, as well as business and information systems (IS) scholars that study AI in competitive enterprises.

## 2. Related literature and the role of AI within organizations

The current research on AI for value creation is focused on customers, directly or indirectly, through mechanisms including complete task automation or the use of AI to augment frontline staffs' capabilities (Huang & Rust, 2020). The growing use of AI affects a variety of stakeholders (Makarius et al., 2020), which is why the focus on a 'user' should be replaced with a wider set of stakeholders (Markus, 2017). Additionally, most AI research focuses on the various operational day-to-day challenges that arise from AI and decision-making (Tarafdar, Beath & Ross, 2019; Duan, Edwards & Dwivedi, 2019; Daugherty & Wilson, 2018). By building future AI research on stakeholder theory, researchers can broaden the scope and increase the validity of AI research outcomes by incorporating various forms of stakeholder behaviors and thereby capture the aggregated business implications.

### 2.1. Artificial intelligence (AI) in firms

The ways in which AI technologies are embedded in business processes has evolved with improvements in the technologies. There is no common definition of AI (Duan et al., 2019), but it generally refers to technologies with the ability to mimic human intelligence through the use of decision trees, if-then rules, and learning algorithms. Dwivedi et al. (2019, p. 2) describe the commonality among AI definitions as "the increasing capability of machines to perform specific roles and tasks currently performed by humans within the workplace and society in general." Thus, AI can be seen as a new agent of the firm that acts autonomously and adapts based on prior stakeholder interactions. To date, research in this area has been scattered across multiple disciplines, often focusing on specific functional areas and new algorithms rather than strategic implications related to investment and firm performance (Coombs et al., 2020). That research has primarily focused on either designing new algorithms or developing better techniques for training the AI to achieve better performance on a defined task.

Some recent examples include algorithms to develop new methods or improve existing methods, such as deep convolution neural networks, recurrent deep neural networks, and extreme machine learning (Baali & Gheim, 2019; Roccetti et al., 2019; Fernandez, Salinas & Torres, 2019). In a similar vein, the proliferation of techniques and methods has led to studies focused on corralling all the various computing techniques into a cohesive framework. This includes efforts to summarize and categorize existing methods in the areas of neural networks (Hancock & Khoshgoftaar, 2020), context aware systems (Sarker, 2019; Sarker, Kayes & Watters, 2019), deep learning (Johnson & Khoshgoftaar, 2019), and support vector machines (Patwary & Wang, 2019). And, again placing emphasis on the development of new methods rather than the incorporation of the technologies into firm strategy, work has been conducted on developing frameworks to guide R&D. For example, R&D

frameworks have been put forth in the areas of deep convolutional networks, extreme machine learning, and all machine learning settings in general (Yadav & Jadhav, 2019; Ma, Wen & Yang, 2019).

Business scholars are also to an increasing degree publishing research about AI-related phenomena (Kellogg, Valentine & Christin, 2020; Lindebaum, Vesa & den Hond, 2020) where learning, scale and scope aspects are generating network effects that have been highly valuable for some firms such as Facebook, Google and Alibaba (Jansiti & Lakhani, 2020). IS researchers are entering the conversation about whether and how AI can be deployed by firms, and momentum will build as technologies advance and firms increase the extent to which they delegate knowledge tasks to machine agents due to the inherent advantages and benefits they can potentially provide (Rai, Constantinides & Sarker, 2019). This trend towards increasing the number of tasks delegated to AI agents highlights the need for organizations to identify and navigate the constantly shifting line that separates a process that can be augmented from a process that can be fully automated (Raisch & Krakowski, 2021).

To date, the more technically focused research has attempted to frame AI use in service delivery and decision support (Duan et al., 2019; van Doorn et al., 2017; Meyer et al., 2014) and some work has focused on the development of new AI technologies and the corresponding impact on the subsequent replacement of human workers (Huang & Rust, 2018). Much of that research has been based on theories such as the Technology Acceptance Model (TAM) put forth by Davis, Bogozz and Warshaw in 1989 (Gursoy et al., 2019) or the Unified Theory of Acceptance and Use of Technology (UTAUT) (Venkatesh et al., 2003; Venkatesh, Thong & Xu, 2012; 2016). While TAM and UTAUT have been used to examine many IT phenomena, some argue that traditional technology acceptance models are not well-suited to explaining user acceptance of AI (Gursoy et al., 2019; Markus, 2017). The argument being that both frameworks assume the human user has a choice of whether to adopt and learn how to use the technology, however AI can be the sole contact in some stakeholder interactions. This is what researchers have described as mandatory use (Brown et al., 2002). AI is also fundamentally different because stakeholders do not actually use the technology, it proactively interacts with them. The fundamental difference in how humans experience AI technology in comparison to other types of information technologies has been explored, and a set of qualitative and empirical studies suggest user acceptance of AI service technology is affected by six factors, namely: performance efficacy, hedonic motivation, anthropomorphism, social influence, facilitating conditions, and emotion (e.g., Lu, Cai & Gursoy, 2019). Extending that work, Gursoy et al. (2019) study the interactions among those six factors and put forth a model of 'AI device use acceptance' (AIUDA) that incorporates interaction effects among those factors. They conclude there remains a lack of understanding of how AI device use is generated or inhibited by different antecedents and the subsequent impact on firm performance has not been explored.

As AI technologies advance and become more capable of performing complex tasks, new questions will continue to arise (Berente et al., 2021). For example, replacing people (i.e., human decision makers) with AI is an intricate decision that should consider more than speed and accuracy of task performance. This is because AI is then required to execute a task and participate in a relationship with an engaged stakeholder (Perez-Vega et al., 2021). To illustrate the complexity of replacing human decision makers with AI, consider the recent trend of firms using AI to conduct initial screening interviews of job candidates. HireVue is an example of this type of technology. Job candidates are asked to go to the HireVue platform and allow access to their computer's camera and audio. The candidate then answers several questions. The role of AI is to ask questions, monitor and evaluate facial and speech patterns, and other factors. As a result, the job candidate's first formal interaction with the hiring firm is via an AI proxy. The candidate's perception of the firm, whether positive or negative, is based on their perception of that encounter. In contrast, AI-based resume scanning

software seeks to match the education, experience, qualifications, and other features of the resume with the job description but never interacts with the applicant. The fundamental distinction is that AI-based resume scanning software is used as a tool to support hiring managers and improve the speed, accuracy and efficiency of the search process whereas HireVue is performing similar tasks while replacing the hiring manager in the first face-to-face interaction and making decisions regarding candidate quality. That is, HireVue is an example of an AI that has been granted agency rights to make real-time decisions in live encounters with stakeholders.

However, HireVue is raising concerns about the role of AI in the recruiting process. For example, The Electronic Privacy Information Center (EPIC) has asked the FTC to investigate HireVue for unfair and deceptive trade practices related to its use of face scanning and speech analysis algorithms.<sup>1</sup> EPIC's concern centers on the lack of transparency over how the algorithms decide who is "employable" and who is not. Likewise, career services departments at universities are coaching students to beat the algorithm after determining HireVue was biased towards certain words and phrases as well as gestures, facial expressions, and mannerisms.<sup>2</sup> The challenges and risks do not stop there. Once deployed, AI can follow and analyze the 'digital footprints' of the employees which leads to a new level of organizational supervision and optimization (Leonardi, 2021) – a development that increases the need for ethical leadership in both business education and practice (Haenlein, Huang & Kaplan, 2022).

The increased complexity in design, use, and organizational consequences of developing algorithms and systems that make decisions with what has been considered intelligence is a major challenge for competitive firms which calls for a better understanding of the effects amongst a wider set of stakeholders (Markus, 2017). As an example, recent research on machine learning and fraud indicates that the resulting impact affects several stakeholder groups (Abbasi et al., 2012). Thus, both firms and researchers need to include a broader set of stakeholder effects when designing, implementing, utilizing, and evaluating AI.

### 2.2. AI serving stakeholders

Firms interact with their stakeholders in a variety of specified contexts. As firms seek to utilize AI in the execution of various encounters, they must accurately evaluate the capabilities of existing and emerging technologies. With respect to AI capabilities, firms are aware that existing technologies are far from the futuristic renderings of AI in science fiction. Such advanced technology is referred to as artificial general intelligence (AGI or "strong AI"), and "implies an ability to acquire and apply knowledge, and to reason and think, in a variety of domains, not just a single area" (Goertzel & Pennachin, 2007, p. 6). In contrast, current AI technologies, and those being developed, are referred to as "weak AI" (Jansiti & Lakhani, 2020). The result is a set of technologies that can replace or support human decision-makers during some phases of some moments-of-truth (MOT) encounters, i.e., interactions with a stakeholder where the firm meets, surpasses, or fails to meet the stakeholder's expectations (Carlzon & Peters, 1987). The challenge with weak AI is the technologies falls *somewhere* on the spectrum ranging from "no AI" to AGI, and are gradually progressing through intermediate steps. The relevance of stakeholder theory increases as AI evolves – through more sophisticated machine learning, neural networks and automatized decision-making – and becomes "stronger" (Berente et al.,

<sup>1</sup> Webpage visited June 30, 2019: <https://www.washingtonpost.com/technology/2019/11/06/prominent-rights-group-files-federal-complaint-against-ai-hiring-firm-hirevue-citing-unfair-deceptive-practices/>.

<sup>2</sup> Webpage visited June 30, 2019: <https://www.insidehighered.com/news/2019/11/04/ai-assessed-job-interviewing-grows-colleges-try-prepare-students/>.

**2021).** During this process, when firms increasingly seek to incorporate stronger AI into their business processes, the interactions between AI and stakeholders will become more sophisticated. The evolution of the use of AI to serve stakeholders will have, as we explain in the next section, direct implications for value creation and destruction. During AI's development, an understanding of the overall firm performance effect of AI, as well as the underlying mechanisms behind the change, is supported by a stakeholder theory perspective.

### 2.3. Stakeholder theory

Stakeholder theory views the firm as a nexus of stakeholder relationships (Freeman, 1984). A stakeholder can be engaged with the firm on behalf of (a) themselves as an individual person (e.g., an employee or a customer), (b) a group of individuals (e.g., a consumer segment or a local community), or (c) a firm (e.g., a supplier or a key account customer). Stakeholder theory thereby supports a holistic perspective on the firm and its performance which provides a broader lens for understanding AI and its effects beyond solely employees or customers (Markus, 2017). Every stakeholder who engages with the firm does so because this choice is better or equal to their next best alternative. The comparable value offered by those alternatives can be in constant motion. For this reason, managers must be continuously concerned with finding, attracting, engaging, and retaining the stakeholders they need to execute a cooperative scheme that creates enough value so that all of those stakeholders will realize a level of utility that is optimized through their association with the firm (Harrison, Bosse & Phillips, 2010).

#### 2.3.1. Bounded self-interest

Stakeholder theory recognizes that managers' value creation and allocation tasks are even more challenging because people – the stakeholders – are not only self-interested. That is, people care about more than just the absolute value they get out of any particular relationship or transaction with the firm. They also care to some extent about the value they get relative to the applicable social norms for similarly situated people (Bosse et al., 2009). This assumption about human behavior is referred to as *bounded self-interest* (Jolls, Sunstein & Thaler, 1998), and it is supported by findings in a range of related fields that show when a group of people can reward and punish each other for upholding or violating acceptable social norms, respectively, they collectively benefit from greater cooperation (Cialdini, 1984; Cropanzano & Mitchell, 2005; Fehr & Gächter, 2000).

The bounded self-interest assumption explicitly incorporates normative ethical considerations in stakeholder theory by acknowledging the importance of justice and fairness in ordinary economic settings (Phillips, 2003). Under this assumption, people primarily seek to maximize their own utility across at least three dimensions of justice: distributive, procedural, and interactional (more on these below). However, their self-interested behavior is bounded by their concerns for the justice they receive (Jolls et al., 1998). When an individual perceives the firm is treating them noticeably worse than the treatment they would expect in that setting, the individual registers this as a violation of their perceived justice norm and they seek to remedy the violation – and sometimes in unexpected ways. Likewise, when an individual perceives she/he is experiencing treatment that favorably exceeds the norm in that setting, they also see this as a norm violation that justifies an appropriate response toward the firm (Adams, 1965; Fehr & Gächter, 2000).

#### 2.3.2. Stakeholder justice

Treatment in this context consists of three dimensions of justice. *Distributional justice* refers to the amount of tangible value that is exchanged. This includes the tangible items of value that are commonly acknowledged in economics and business such as price, cost, wages, time, and quality. *Procedural justice* refers to the fairness of the processes that affect the stakeholder. For example, common expectations

regarding decision processes are that they are transparent, they incorporate input from all parties who are affected by the decision, they consider all of the relevant information that is available, and that they allow for changes ex post if the decision is widely seen as a poor one (Leventhal, Karuza & Fry, 1980). Finally, *interactional justice* refers to the way stakeholders are treated impersonally during interactions with the firm. People expect to be treated with respect and dignity because they have intrinsic value as humans (Bies & Moag, 1986). When firms treat stakeholders as instruments to be utilized for the sole benefit of other stakeholders, it is typically perceived as a violation of interactional justice.

Stakeholders continuously assess each of these dimensions of justice and update their cumulative perception of right and wrong in the focal circumstance. This means they consider and allow for tradeoffs among the three dimensions of justice (Colquitt et al., 2001). When a stakeholder perceives she has received acceptable distributive and procedural justice, for example, she might still be upset with the firm because of perceived interactional injustice. In the same way, the stakeholder's perceptions of exceptional justice in some dimension can outweigh her perceptions of injustice in another dimension (Bosse et al., 2009).

A second assumption that is critical for understanding how bounded self-interest affects stakeholder relationships and, ultimately, firm performance is that stakeholder contracts are incomplete (Asher et al., 2005). According to this assumption, the agreement (or contract) between the firm and any stakeholder cannot completely specify in advance all of the behaviors that are expected of either party under all conditions nor all of the benefits that may be realized by either party (Grossman & Hart, 1986). The result of this assumption paired with the bounded self-interest assumption is that the effort or resources a stakeholder provides to the firm is contingent on her perception of the justice or fairness underlying the firm's interactions with its stakeholders (Harrison et al., 2010).

#### 2.3.3. Stakeholder reciprocity

If the firm treats stakeholders the way most stakeholders expect to be treated, then the stakeholders generally provide the same effort and level of investment in the firm that is commonly expected in that situation. *The commonly expected responses from stakeholders are referred to as weak reciprocity* (Fehr & Gächter, 2000). A human stakeholder may react to any actions and decisions of the firm, but some of those reactions may trigger a neutral response that does not create marginal cost for the stakeholder. For example, a customer who requests a discount and is told 'no' might be disappointed, but may still purchase the item because the response was not entirely unexpected nor perceived as unfair. In contrast, customers who become aware that certain groups and communities are receiving different prices may react strongly if they believe the differences are fundamentally unjust.

*Stakeholder reactions that are beyond the norm in a given situation are referred to as strong reciprocity* (Fehr & Fischbacher, 2003). Stakeholders are willing to exhibit strong reciprocity in order to punish (negative reciprocity) or reward (positive reciprocity) a firm for violating expectations of justice. Negative reciprocity is costly to both the stakeholder and the firm that is the target of that reciprocity (Fehr & Gächter, 2000), and it serves to enforce the perceived norm of justice by punishing the violator. Stakeholders can negatively reciprocate, for example, by providing less effort or by withholding information (Anderson & Agarwal, 2011). On the other hand, if the firm allocates justice to stakeholders that is noticeably above what they expect in such situations, they can positively reciprocate in ways that benefit the firm (Bosse et al., 2009; Harrison et al., 2010). Taken in aggregate, the neutral, negative, and positive reciprocal behavior of a firm's stakeholders has a direct impact on its overall firm-level performance (Bosse et al., 2009). Irregular strong reciprocity across the stakeholder network is associated with increased variance in firm performance and is now being proposed as a foundational requirement for above-normal firm performance (Barney, 2018).

Summarizing (see Table 1), stakeholders expect three dimensions of justice, at varying weights, in their interactions with a firm. In the course of their interactions with the firm, they compare their perceptions of the justice they are receiving with their expectations, and they respond accordingly. If the absolute value of the difference between their perceptions and expectations is minimal, they behave in ways that are commonly expected in that situation (i.e., they exhibit weak reciprocity). However, if the difference is larger, they exhibit strong reciprocity (either positive or negative) to signal their pleasure or displeasure to the firm on the principle of justice. *The firm benefits from positive reciprocity from its stakeholders, collectively, so is rewarded for exceeding their expectations for justice by an amount that is noticeable by the stakeholders.* The benefits associated with exceeding stakeholder expectations much beyond this level are unlikely to fully offset the marginal costs it requires (Harrison & Bosse, 2013).

When attempting to balance perceptions and expectations for justice such that the stakeholder's perceived value is slightly greater than their expectations for value, firms contend with a number of factors that make precise estimation as much intuition as calculation. First, the relative weights or importance stakeholders assign to each type of justice can shift over time. Second, expectations can change over time based on the stakeholder's experiences with the firm, observed experiences of others with the firm, or experiences with other firms in the same industry. These dynamic weights and expectations are reflected in reciprocal stakeholder behaviors the firm must detect and subsequently respond to. We argue that these challenges, associated with perceiving those changes, are at the heart of firm decisions about adopting and deploying AI.

## 3. A stakeholder-driven framework for AI deployment

When a firm decides to have AI interact with its stakeholders, it needs to assess to what degree the AI will accommodate the phenomenon through which bounded self-interest drives reciprocal behavior and firm performance. This would be comparatively easy if the AI system could directly perceive the stakeholder's expectations of justice, perceptions of justice, and resulting behavior. However, while current forms of AI can perceive the stakeholder's behavior in a variety of ways

**Table 1**  
Summary of the adopted stakeholder theory concepts.

| Concept               | Definition                                                                                                                                                                                                                    | References                                              |
|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|
| Stakeholder           | An individual person, a group of individuals, or a firm                                                                                                                                                                       | Freeman (1984)                                          |
| Bounded self-interest | People care about the absolute value they get out of any particular relationship or transaction with a firm but they also care about the value they get relative to the applicable social norms for similarly situated people | Bosse et al. (2009), Jolls, Sunstein and Thaler (1998). |
| Justice:              | Fair treatment (in three dimensions). (a)                                                                                                                                                                                     | Leventhal, Karuza and Fry (1980), Bies and Moag (1986). |
| (a) Distributional    | The amount of tangible value that is exchanged (b)                                                                                                                                                                            |                                                         |
| (b) Procedural        | The fairness of the exchange process (c)                                                                                                                                                                                      |                                                         |
| (c) Interactional     | the way the stakeholder is treated during interactions with the firm                                                                                                                                                          |                                                         |
| Reciprocity:          | The behavioral mechanism linking stakeholder reactions to firm performance                                                                                                                                                    | Fehr and Gächter (2000), Fehr and Fischbacher (2003)    |
| (a) Weak              | The common expected response from the stakeholder                                                                                                                                                                             |                                                         |
| (b) Strong            | Positive or negative responses beyond the norm or expectation in a given situation                                                                                                                                            |                                                         |

(Berente et al., 2021) it cannot, unfortunately, directly observe the stakeholder's expectations or perceptions of justice. Instead, it must *estimate* the stakeholder's expectations and then can only *infer* her perceptions after observing her reactions to the encounter. This is in line with Russel (2016) who argues a core principle for an intelligence system should be to initially have some uncertainty regarding the stakeholder's values that it should subsequently learn through future interaction(s). This is an axiom for our AI-Stakeholder framework (Fig. 1). The framework illustrates the sequence of steps in AI-stakeholder interaction, i.e., for any given encounter, or moment of truth (MOT), that AI is used for, integrating the stakeholder's steps and the AI's steps.

The AI's steps, in chronological order, start with establishing a baseline (a priori) estimate of what the stakeholder expects in the given encounter (based on previous encounters with this and other similar stakeholders), perceiving relevant environmental pre-encounter cues about what this stakeholder is currently expecting (in situ), performing decision analysis about how to treat the stakeholder (i.e., formulating the value proposition), delivering value to the stakeholder via an encounter, and perceiving relevant post-encounter cues about this stakeholder's resultant behaviors. In this section, we develop a framework of propositions about the challenges and potential of AI in this process. Our focus, as described above, is on how AI must perceive pre-encounter cues and post-encounter stakeholder behaviors.

### 3.1. In situ stakeholder expectations

Before interacting with the stakeholder, the AI must first model the encounter using baseline estimates of what most stakeholders expect in this situation (Harrison & Bosse, 2013). This estimate is a hypothesis about what the typical stakeholders commonly expect from the firm (a priori). To the extent AI is replacing the agency of a certain associate who would otherwise be engaging in this encounter with the firm's stakeholders, that associate is the primary source of this a priori information about common stakeholder expectations. However, value propositions derived solely from historical information gathered from all stakeholders who have interacted with the firm are imprecise because they do not include stakeholder- and situation-specific information.

Stakeholder's expectations for justice can also change frequently, so refining the value proposition for a particular stakeholder in a particular context requires an updated assessment of what that stakeholder expects in the encounter (Kaplan & Haenlein, 2019). This entails perceiving relevant pre-encounter, in situ cues from the stakeholder. These perceptions must occur before the encounter and then be used to formulate a value proposition ideally tailored to trigger positive reciprocity from this stakeholder. Thus, AI that replaces a firm's (human) decision-maker must be able to perceive, as accurately as possible, the stakeholder's expectations for distributive, procedural, and interactional justice. This to accommodate the subsequent reciprocal behavior from stakeholders toward the firm.

**Proposition 1.** AI that estimate each stakeholder's in situ expectations for distributive, procedural, and interactional justice will give the firm a greater likelihood of strong positive reciprocity from stakeholders.

For the firm, the costs incurred in adopting AI that have these capabilities must be compared against the value-creation benefits of strong positive stakeholder reciprocity. Certain characteristics of the in situ information needed to accurately tailor a value proposition will arguably make automating some stakeholder encounters more complicated and expensive than others. Encounters where the value propositions are largely dictated by codifiable rules and practices are less costly to automate and require less sophisticated AI. However, in encounters where stakeholders have unspoken or unwritten rules they expect the firm to follow, the required sophistication (and hence cost) of AI increases as does the risk the AI will develop inaccurate estimates of the stakeholder's expectations.

![](55d2bfe1c3d04e86df8d7a104d802172_img.jpg)

Flowchart illustrating the encounters between stakeholders and AI designed to serve them in a specific Mot (MOT).

The process is divided into four main stages:

1. **AI definition (expanded)** leads to **AI perceives pre-encounter cues**.
2. **AI perceives pre-encounter cues** leads to **AI performs decision analysis**.
3. **AI performs decision analysis** leads to **AI takes action**.
4. **AI takes action** leads to **AI perceives post-encounter behavior**.

The flow between AI steps and stakeholder steps is as follows:

- **AI steps:** AI perceives in situ environmental cues about this stakeholder's expectations, AI computes a priori estimates of this stakeholder's expectations, and AI is given rules and resources for formulating value propositions for this MOT.
- **Stakeholder steps:** Stakeholder has expectations of justice for this MOT, and Stakeholder forms perceptions of justice received in this MOT.

The AI steps influence the stakeholder steps:

- AI perceives in situ environmental cues about this stakeholder's expectations leads to Stakeholder forms perceptions of justice received in this MOT.
- AI computes a priori estimates of this stakeholder's expectations leads to AI perceives in situ cues about this stakeholder's post-encounter behavior.
- AI is given rules and resources for formulating value propositions for this MOT leads to AI delivers value via encounter with stakeholder (MOT).
- AI delivers value via encounter with stakeholder (MOT) leads to Stakeholder forms perceptions of justice received in this MOT.

The AI steps also influence the AI steps:

- AI is given rules and resources for formulating value propositions for this MOT leads to AI formulates the value proposition.
- AI formulates the value proposition leads to AI computes a priori estimates of this stakeholder's expectations.

The AI steps also influence the AI steps:

- AI formulates the value proposition leads to AI delivers value via encounter with stakeholder (MOT).
- AI delivers value via encounter with stakeholder (MOT) leads to AI perceives in situ cues about this stakeholder's post-encounter behavior.

The AI steps also influence the AI steps:

- AI formulates the value proposition leads to AI computes a priori estimates of this stakeholder's expectations.
- AI delivers value via encounter with stakeholder (MOT) leads to AI perceives in situ cues about this stakeholder's post-encounter behavior.

Fig. 1. Encounters between stakeholders and AI designed to serve them in a specific MOT.

Implicit information that is not directly stated but is suggested through context and behavior, like facial and body language, voice tone, and word choice, is another characteristic that makes automating the stakeholder encounter via AI more difficult. Amazon has developed impressive recommender engines based on explicit data including the stakeholder's past purchases, purchases made by others who are “similar” based on personal information shared with Amazon, and products that have similar features to ones the stakeholder has purchased based on Amazon's product categorization schema. However, technologies such as Alexa cannot yet modify their recommendations to an individual based on implicit data like their mood or the tone of their voice (McLean, Osei-Frimpong & Barhost, 2021). This technical barrier is beginning to crack, though, and companies like Emotech are creating products like Olly, a voice-controlled AI assistant with algorithms that gradually teach it to be more like its owner and incorporates the ability to detect and respond to non-verbal cues and intonation. The challenges and associated costs of reaching higher levels of AI maturity (i.e., “stronger” AI) where the AI is able to pick up on these cues have been mentioned by AI researchers (Russell & Norvig, 2016). We argue they help managers identify the boundary between a stakeholder encounter that can safely be delegated to AI for automation versus one that is only appropriate for augmentation (Raisch & Krakowski, 2021).

### 3.2. A priori stakeholder expectations

The probability of generating greater total value in a market or a collective, such as a firm, is positively associated with the relevant actors' ability to engage in strong reciprocity (Bosse et al., 2009; Fehr & Gächter, 2000). This is because strong reciprocators are predisposed to cooperate with other cooperators and to punish non-cooperators (Gintis, 2000). Punishment strategies applied to those that do not uphold shared expectations for distributional, procedural, or interactional justice serve to maintain cooperative behavior and uphold social norms of justice (Clutton-Brock & Parker, 1995). For example, if actors have opportunities to punish free riders, strong reciprocators will vigorously punish any free riders even when the punishment is costly for the punisher. However, when people face no material dis-incentives to “free ride,” a collective is less likely to realize the gains that come from cooperation (Fehr & Gächter, 2000). Strong positive reciprocators also influence cooperative behavior and the resultant collective gains when they incur

their own costs to *reward* actors who treat others in exceptionally favorable ways.

Based on this logic, AI will contribute to improved firm performance when it enables actors across the stakeholder network to actively contribute to the maintenance of social justice norms through punishing and rewarding others, including the firm. An applicable social norm that is common in stakeholder-oriented firms is meritocracy (Phillips, 2003). This norm, also referred to as the equity norm, emphasizes allocating value to stakeholders according to the value of the contributions they make to the firm (Adams, 1965).

Firms that can create a pattern of positive reciprocity among their stakeholders have a sustainable value creation advantage (Harrison et al., 2010). Negative reciprocity, in the short term, is costly to the firm so it may seem paradoxical for firms to support it. However, in the long term, the firm that is responsive to negative reciprocity from stakeholders is continuously realigning its value propositions with the relevant social justice norms as signaled by its stakeholders. This phenomenon points to the frontier distinguishing between tasks that can be automated with AI from tasks that can only be augmented with AI. Both types of reciprocity must not only be allowed by the AI, but also ultimately recognized by the AI so that the a priori hypotheses about stakeholder's expectations for justice can be continuously updated after each encounter.

Like the discussion above about value proposition formulation, the possible allowable stakeholder reciprocal behaviors can range from a fixed set of behaviors to unconstrained. It is comparatively easier to design AI that offers stakeholders a finite set of positively and negatively reciprocal actions they can possibly take (Iansiti & Lakhani, 2020). The range and severity of stakeholder reactions to perceptions of the three types of justice can arguably be controlled with such a functionality. The tradeoff is that this constrains the stakeholder's potential to behave in unexpected ways. It is the unexpected behaviors from stakeholders that can spark innovative new value proposition design insights by hinting at novel or emerging sources of stakeholder value (Harrison et al., 2010). However, there can also be irrational and even hostile behaviors that should be registered (Herath & Rao, 2009). This line of logic expands the recognized scope of AI activities from autonomously perceiving environmental cues before formulating a plan of action and taking action (Russell & Norvig, 2016) to include the challenges of parsing out pre-interaction perceiving activities from post-interaction perceiving

activities.

**Proposition 2.** *Firms that deploy AI to accommodate and perceive more stakeholder reciprocity, both positive and negative, will have a greater likelihood of strong positive reciprocity from stakeholders.*

Certain characteristics of the encounter make it easier (less costly) or harder (more costly) to build into AI this ability to perceive the stakeholder's post-interaction behavior. It would be ideal if the AI could observe and perceive nuanced information about the stakeholder's expectations for distributive, procedural, and interactional justice in the encounter. What in prior research has been seen as hostile and irrational behavior can, based on stakeholder theory's focus on justice, be empirically explored and understood in a new way that elevates the outcome to the firm performance level. When stakeholder expectations are not directly observable, the next best thing is to adopt AI that recognize the post-interaction behavior of the stakeholder and then to interpret it in the context of what the AI had delivered to that stakeholder.

Given the limitations of weak AI, firms should only adopt AI such that stakeholders who would like to strongly reciprocate towards the firm in ways the AI cannot yet accommodate should have other mechanisms for sharing their intentions with the firm. Managers should be included in the encounter in these situations so that they can try to understand the expectations that were missed and to devise appropriate responses to the stakeholder. The longer-term role of the involved human agents is to assess increasing the programming functionality of the AI to expand its range of value propositions and the range of reciprocal stakeholder behaviors it can accommodate.

### 3.3. Stakeholder expectations across multiple MOT encounter types

The propositions developed thus far emphasize key challenges, costs, and benefits when firms adopt and deploy AI for automating encounters with stakeholders. The challenges, costs, and benefits all multiply when firms expand the spatial scale (Raisch & Krakowski, 2021) by seeking to use AI in multiple, sequential encounters with the same stakeholders. The primary reason is that stakeholder expectations in any given encounter can be expected to change for a variety of reasons (Loureiro et al., 2021). For example, a stakeholder's preferences in one encounter can change based on their perceptions of how the firm treated them in a different type of encounter or based on their experiences with other firms. A human agent attuned to the stakeholder can detect individual and environmental cues from across the firm and its stakeholder network (Harrison, Phillips & Freeman, 2020) – and can adjust value propositions for the stakeholder appropriately. However, many of these cues are probably not recognized for their relevance by current forms of AI.

Although the AI challenges may seem daunting, firms can conceivably leverage the comparative computational strength of computers over humans to create more value by using AI in multiple encounters with the same stakeholders. Even when encountering human agents, stakeholders do not typically indicate exactly what they expect from the firm in terms of distributive, procedural, and interactional value because this information may be difficult for the stakeholder to express. This information may also be sensitive in that its release can make the stakeholder vulnerable to exploitation (Harrison et al., 2010). However, accurate information about the nuanced differences among individual stakeholder's justice expectations is precisely the sort of information that firms can use to systematically craft the tailored value propositions that can trigger positive reciprocity. For firms that (Proposition 1) estimate each stakeholder's in situ expectations for justice in each encounter and (Proposition 2) accommodate and perceive more stakeholder reciprocity, both positive and negative, after each encounter, we argue deploying AI systems in multiple encounters with the same stakeholders – following the logic of network effects (Hagiw & Wright, 2020) – provides an opportunity for the systems to perceive subtle

changes in expectations across the multiple types of encounters that even human agents would be unlikely to perceive. Integrating this information across the multiple systems should provide unique and timely insights about how individual stakeholders will likely reciprocate after being presented specific value propositions. Of course, if the expectations for justice are imperceptible by the AI or are more dynamic (change faster) than the AI can accommodate, the value propositions formulated by the AI can destroy value for the firm by triggering negative reciprocity from stakeholders.

A current project that will put AI to the test in a multi-stakeholder context is Toyota's Woven City – a 175-acre smart city at the base of Mount Fuji in Japan that is planned to house 2,000 citizens. Toyota's CEO presented the plans at CES2020 in Las Vegas where Woven City "will create a one-of-a-kind data operating system [...] with people, buildings and vehicles. All connected and communicating – with each other – through data and sensors we will be able to test AI technology in both the virtual and physical world maximizing its potential." The city will be based on the latest knowledge in architecture, construction, mobility and communications technology which will be supported by various forms of AI, some potentially used in sync as AI platforms and others disconnected where two forms of AI interact with each other as replacement for two stakeholders, what Robinson et al. (2020) describe as InterAI. A setting like Woven City puts AI to the test. While self-driving cars powered by AI might make better decisions than humans across multiple encounters (Tian et al., 2018), developers will have to design algorithms that satisfy topics related to stakeholder expectations for justice, such as ethical decision-making (Bonnefon, Shariff & Rahwan, 2019; Sparrow & Howard, 2017) and fairness as, for example, when balancing the urgency for an ambulance on its way to the hospital with the safety of a group of children going to school. In Woven City that will strive to be sustainable, we can also expect tradeoffs in the offices where the need for heating, light or cooling may not be balanced by the availability of renewable energy or the capacity of fluid-cell energy. This in turn will have an impact on the involved stakeholders (e.g., facility managers, office tenants, and energy enterprises) that will judge each MOT and hold the involved AI accountable for favorable or negative outcomes.

As AI is used in multiple encounters with the same stakeholders, the information collected before and after each encounter can be used to update the AI estimate of that stakeholder's perception of the firm overall, and then subsequently used in formulating the value proposition for the stakeholder in different encounters.

**Proposition 3.** *Firms that integrate various AI systems to augment or automate multiple types of encounters with stakeholders will have a greater likelihood of strong positive reciprocity from stakeholders.*

As firms deploy AI in more settings, they will create a computing environment where multiple, disparate AI technologies may be interacting with the same stakeholder group and those AI technologies may be interacting with each other as well. Following the logics of the network effects and big data (Gregory et al., 2021), the variation in interaction as well as in reciprocal behaviors allows for improved learning and execution (Haley, Norvig & Pereira, 2009).

However, firms will need guidance on how to set proper controls, governance, and security protocols (Duan et al., 2019; Martin, 2018) as well as developing a collective mindfulness (Salovaara, Lyytinen & Penttinen, 2019) when adopting AI. While most studies of AI have focused on the speed and performance of the technology, more research should be done to evaluate the effectiveness of widely-used AI on the overall performance of the firm or the business unit deploying the technology.

## 4. Discussion and implications

In this conceptual paper we have introduced stakeholder theory as a foundation for expanding our understanding firms' adoption and

deployment of AI and its subsequent effect on firm performance. To accomplish this, we built a framework of novel propositions to guide firms' decisions regarding AI adoption by building on a foundation of logic largely provided by stakeholder theory and the current and future states of AI technology. This framework articulates the challenges and potential of AI for managing stakeholder encounters and an expanded sequence of steps through which this occurs. Identifying these critical points and the relationships among them provides scholars with new foundational arguments to examine more closely and provides managers with a grounded perspective of this phenomenon. Table 2 summarizes the framework's propositions with related research and managerial implications. Managers who adopt AI without assessing the solution based on the constructs and relationships in the presented framework are more likely to destroy value.

The framework builds on stakeholder theory that incorporates the multidimensional normative forms of social justice that drive stakeholder behavior. Distributive justice requires the firm to allocate rewards to stakeholders in a manner that is considered fair by the stakeholders, or else risk eventual value destruction. Distributions do not have to be equal, but the opportunity to achieve a given reward should be available to all stakeholders who deserve it. This presents an array of value proposition constraints and challenges. Procedural and interactional justice present even greater challenges. Procedural justice deals with a focal stakeholder's expectation that a decision process will be fair and transparent. While a given stakeholder may not be "happy" with the outcome, they may not be "angry" if they understand the decision process (Davenport, 2019) and believe the decision process is applied consistently to all other stakeholders (Bosse et al., 2009). This presents a conundrum for firms that give AI agency in encounters with stakeholders. While the greatest strength of AI is its ability to serve as an agent of the firm in a more cost-efficient manner than human employees (Brynjolfsson & McAfee, 2014), the greatest weakness of AI is that it generates its own decision algorithms and those algorithms may adapt over time. That is, if a dissatisfied stakeholder asks the question "Why?" the representative of the firm may not be able to answer the question because the decision algorithms of AI are by design not transparent (or static). This can be problematic in highly regulated environments such as audit, tax, and financial services where the regulations allow a certain amount of discretion but firms have to be careful not to push too far. Will future stronger AI be able to determine when it is pushing too far? Our framework argues that it should be. If AI doesn't push the boundaries at all, is it truly fulfilling its responsibilities as an agent of the firm? Whether AI pushes these "fuzzy boundaries" too hard or not hard enough, stakeholder theory predicts a greater likelihood of negative bilateral reciprocity from the focal stakeholder.

In addition, firms must decide whether and how to inform stakeholders they are interacting with AI rather than a human (Robinson et al., 2020). That decision requires the firm understand that awareness will likely trigger a change in stakeholder expectations with respect to interactional justice. For example, research has shown individuals are willing to share more data when they perceive themselves as having control over how the information is used, shared, and whether anonymity is maintained (Hajli & Lin, 2016). However, while being positive in regards to AI capabilities, it also raises ethical concerns (Haenlein et al., 2022).

The presented AI-stakeholder framework supports an understanding and a prediction of the relationships between AI and firm performance, and it elaborates with discussions of conditions that can be expected to moderate those relationships. While we have developed initial propositions that help firms determine the promises and challenges of using AI as an agent for stakeholder interactions, future research might expand this work to the context of interacting simultaneously with multiple stakeholder groups that heightens the challenge due to additional layers of complexity. As the number of stakeholder types whose interactions are handled by AI increases, the risk of missing cross-signals from other stakeholders increases. Furthermore, when multiple AI systems are

Table 2

The propositions and their implications.

| Proposition | Statement                                                                                                                                                                                                  | Implications and opportunities for research                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Implications and opportunities for management                                                                                                                                                                                                                                                                                                                                                                                                |
|-------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1           | AI that estimate each stakeholder's in situ expectations for distributive, procedural, and interactional justice will give the firm a greater likelihood of strong positive reciprocity from stakeholders. | While current research has had a focus on how AI replaces humans in current activities, it has included aspects of interactional and procedural justice. However, (a) what are the interrelationships between distributive, procedural, and interactional justice when deploying AI, (b) do the increased observability of AI procedures and other stakeholder outcomes affect the experienced justice and hence (positive/negative) reciprocity? (c) do stakeholders always expect 'optimal' justice, i.e., can AI be too instrumental?    | An adopted AI cannot only include that the right thing is done (procedural justice), and that it is done in the right way (interactional justice) for the stakeholder at hand. It must also incorporate the outcomes of all stakeholders (distributive justice).                                                                                                                                                                             |
| 2           | Firms that deploy AI to accommodate and perceive more stakeholder reciprocity, both positive and negative, will have a greater likelihood of strong positive reciprocity from stakeholders.                | Further research is needed to understand to what degree AI (a) can 'understand' the forms (positive/ negative) of reciprocity and (b) the strength of it (weak/strong)? While current research mainly has focused stakeholder reciprocity in the forms of text or talk, future research should broaden the spectrum to include all tactile dimensions as well as (c) the psychological effects of replacing humans with AI.                                                                                                                 | In AI-driven interactions, (a) AI must be able to perceive when it has done things right (positive reciprocity) and when it has done things wrong (negative reciprocity). (b) AI must also be able to identify the balance between the three forms of justice (distributive, procedural, and interactional); both for positive and negative outcomes.                                                                                        |
| 3           | Firms that integrate various AI systems to augment or automate multiple types of encounters with stakeholders will have a greater likelihood of strong positive reciprocity from stakeholders.             | Further conceptualizations are needed for (a) how different forms of AI are integrated (in a single platform and between platforms) and its effect on stakeholder reciprocity, and (b) how replacing humans with AI-augmented humans or (fully) AI affects the (i) expectations as well as (ii) reciprocity of the different stakeholders. (c) While current research mainly has followed single AI applications, future research needs to incorporate the study of firms' use of multiple AI applications (where the development of an AI- | Adopting multiple forms of AI can be expected to bring exponential effects on firm performance as well as exponential risks of failure. Integrating stakeholder information (i.e., outcomes from AI interactions) can be expected to mitigate the risks as well as bring positive leverage effects. Furthermore, combining and utilizing stakeholder (response) data brings ethical considerations that requires leadership and following up |

(continued on next page)

Table 2 (continued)

| Proposition | Statement | Implications and opportunities for research                                      | for management                                                          |
|-------------|-----------|----------------------------------------------------------------------------------|-------------------------------------------------------------------------|
|             |           | index should be prioritized), as well as when (D) AI is one form of stakeholder. | when AI makes decisions beyond (e.g., morally acceptable) expectations. |

supporting different business functions, they must be integrated in some manner to ensure each is presenting a consistent face to the stakeholder and stakeholder interactions can be optimized across the entire network of interactions. This increases the probability the firm will miss some important stakeholder expectations and, subsequently, trigger negative reciprocity. This phenomenon suggests a need for an oversight process that recognizes and responds to changes in the firm-level environment rather than just at the stakeholder level.

By developing stakeholder theory logic for the AI phenomenon, we have also helped expand the theory from one almost exclusively focused on human decision makers to one where AI is recognized as performing essentially moral deliberations (Robinson et al., 2020; Wright & Schultz, 2018; Martin, 2018). Note the domain of ethics and morality is also one in which both stakeholder and scholars are skeptical about AI (Divivedi et al., 2019; Martin, 2018; Russell & Norvig, 2016). The propositions developed in this paper characterize AI in the role of agents who serve stakeholders in moment-of-truth encounters. In that role, we explained how the AI must estimate each stakeholder's expectations for justice in the encounter and formulate a value proposition that will ideally exceed those expectations by a noticeable amount and, therefore, trigger a cycle of positive reciprocity. This behavior is analogous to the concept of moral imagination, which is the ability to perceive norms, social roles, and relationships in a given situation and to evaluate the outcomes of possible alternative actions from the stakeholder's point of view (Wehner, 1999). Without moral imagination, managers – and now AI – risk formulating value propositions that stakeholders view as unjust and consequently trigger value destruction through negative reciprocity.

## 5. Conclusion

We have in this conceptual paper introduced arguments derived from stakeholder theory to the corresponding implications for firms considering allocating agency rights to AI during stakeholder encounters. By doing this, the presented AI-stakeholder framework offers a theory-based lens for ongoing examination of the revolutionary capabilities provided by AI (Berente et al., 2021; Luiterio et al., 2021; Russell & Norvig, 2016) from the perspective of firms seeking to improve performance (cf. Lichtenhalter, 2019). Specifically, we develop propositions and provide a sampling of follow-on research opportunities that will help firms understand whether and how AI should play a role in a given stakeholder encounter or relationship. The framework presented complements prior studies of users and elevates the behaviors of any form of stakeholders as the result of expected and experienced forms of justice. Stakeholder theory offers a valuable foundation for scholars to explore all the technical, human, and social implications of AI and it does not suffer from the central challenges of user theory that are limited to one category of user (e.g., customers or employees). It is instead a more foundational theory that incorporates all types of stakeholders of a firm. Thus, the presented framework and propositions are applicable and ready for testing any sort of AI in all forms of commercial environments with various types of stakeholders.

## CREDIT authorship contribution statement

Douglas Bosse: Writing – review & editing, Writing – original draft, Conceptualization, Steven Thompson: Writing – review & editing,

Writing – original draft, Conceptualization, Peter Ekman: Writing – review & editing, Writing – original draft, Conceptualization.

## Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## References

- Abbasi, A., Albrecht, C., Vance, A., & Hansen, J. (2012). Meta-learning framework for detecting financial fraud. *MIS Quarterly*, 36(4), 1293–1327.
- Adams, J. S. (1965). Inequity in social exchange. In L. Berkowitz (Ed.), *Advances in Experimental Social Psychology* (Vol. 2, pp. 267–289). New York: Academic Press.
- Aleksander, I. (2017). Partners of humans: A realistic assessment of the role of robots in the foreseeable future. *Journal of Information Technology*, 32(1), 1–9.
- Akbari, A. A., & Demsetz, H. (1972). Production, information costs, and economic organization. *American Economic Review*, 62(5), 777–795.
- Anderson, C. L., & Agarwal, R. (2011). The digitization of healthcare: Boundary risks, emotions, and consumer willingness to disclose personal health information. *Information Systems Research*, 22(3), 469–490.
- Asher, C. C., Mahoney, J. M., & Mahoney, J. T. (2005). Towards a property rights foundation for a stakeholder theory of the firm. *Journal of Management and Governance*, 9(1), 53–72.
- Basile, M., & Gneim, N. (2019). Emotion analysis of Arabic tweets using deep learning approach. *Journal of Big Data*, 6(1), 1–12.
- Barnett, J. B. (2018). Why resource-based theory's model of profit appropriation must incorporate a stakeholder perspective. *Strategic Management Journal*, 39(13), 3305–3325.
- Berente, N., Ou, B., Recker, J., & Santham, R. (2021). Managing artificial intelligence. *MIS Quarterly*, 45(3), 1433–1450.
- Bies, R. J., & Moog, J. S. (1998). Interactional justice: Communication criteria for justice. In B. Sheppard (Ed.), *Justice in the Workplace* (pp. 197–208). Mahwah: Lawrence Erlbaum Associates.
- Bomford, J. F., Shariff, A., & Rahwan, R. (2019). The trolley, the bull bar, and why engineers should care about the ethics of autonomous cars. *Proceedings of the IEEE*, 107(3), 592–594.
- Bosse, D. A., Phillips, R. A., & Harrison, J. S. (2009). Stakeholders, reciprocity, and firm performance. *Strategic Management Journal*, 30(4), 447–456.
- Brown, S. A., Massey, A. P., Montoya-Weiss, M. M., & Burkman, J. R. (2002). Do I really have to? User acceptance of mandated technology. *European Journal of Information Systems*, 11(4), 283–295.
- Brynjolfsson, E., & McAfee, A. (2014). *The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies*. New York: Norton.
- Carlson, J. J., & Peters, T. (1987). *Moments of truth*. Cambridge: Ballinger.
- Caldini, R. B. (1984). *Influence: The Psychology of Persuasion*. New York: William Morrow and Company.
- Clutton-Brock, T. H., & Parker, G. A. (1995). Punishment in animal societies. *Nature*, 373 (6511), 209–216.
- Colquitt, J. J., Cantor, D. E., Wesson, M. J., Porter, C. O., & Ng, K. Y. (2001). Justice at the millennium: A meta-analytic review of 25 years of organizational justice research. *Journal of Applied Psychology*, 86(3), 425–445.
- Coombs, C., Holroyd, D., Tarpyn, S. K., & Barnard, S. (2020). The strategic impacts of intelligent automation for knowledge and service work: An interdisciplinary review. *Journal of Strategic Information Systems*, 29(4), [https://doi.org/10.1016/j.jis.2020.101600](#).
- Cropangan, R., & Mitchell, M. S. (2005). Social exchange theory: An interdisciplinary review. *Journal of Management*, 31, 874–900.
- Daugherity, P. R., & Wilson, H. J. (2018). *Human + Machine: Reimagining Work in the Age of AI*. Boston: Harvard Business Review Press.
- Davenport, T. H., & Kirby, J. (2016). Only Humans Need Apply: Winners & Losers in the Age of the Smart Machine. New York: Harper Business.
- Davenport, T. H. (2019). Can We Solve AI's "Trust Problem"? *MIT Sloan Management Review*, 60(3), 18–19.
- David, F. D., Baggett, R. P., & Warsaw, P. R. (1989). User acceptance of computer technology: A comparison of two theoretical models. *Management Science*, 35(8), 1082–1095.
- Di Vito, A., Palladino, R., Hassan, R., & Eschler, O. (2020). Artificial intelligence and business models in the sustainable development goals perspective: A systematic literature review. *Journal of Business Research*, 121, 283–314.
- Duan, Y., Edwards, J. S., & Devlin, Y. K. (2019). The role of intelligence for decision making in the era of Big Data–evolution, challenges and research agenda. *International Journal of Information Management*, 48, 63–71.
- Divivedi, Y. K., Wastell, D., Laumer, H. Z., Henkens, M. Z., Myers, M. D., Bunker, D., ... Srivastava, S. C. (2015). Research on information systems failures and successes: Status update and future directions. *Information Systems Frontiers*, 17(1), 143–157.
- Divivedi, Y. K., Wastell, D., Laumer, H. Z., Henkens, M. Z., Coombs, C., Crock, T., ... Williams, M. D. (2019). Artificial intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy. *International Journal of Information Management*, 57, 1–47.
- Fehr, E., & Fischbacher, U. (2003). The nature of human altruism. *Nature*, 425, 785–791.

Fehr, E., & Gächter, S. (2000). Fairness and retaliation: The economics of reciprocity. *Journal of Economic Perspectives*, 14, 159–181.

Fernández, C., Salinas, L., & Torres, C. E. (2019). A meta extreme learning machine method for forecasting financial time series. *Applied Intelligence*, 49(2), 532–554.

Fleming, P. (2019). Robots and Organization Studies: Why Robots Might Not Want to Steal Your Job. *Organization Studies*, 40(1), 23–37.

Ford, M. (2015). *The Rise of the Robots*. New York: Basic Books.

Foster, M. E., Gaschler, A., & Guillani, M. (2017). Automatically Classifying User Engagement for Dynamic Multi-party Human-Robot Interaction. *International Journal of Social Robotics*, 9, 659–674.

Freeman, R. E. (1984). *Strategic management: A stakeholder approach*. Boston: Pitman.

Gilson, L. L., & Goldberg, C. B. (2015). Editors' comment: So, what is a conceptual paper? *Group & Organization Management*, 40(2), 127–130.

Gintis, H. (2000). *Game theory evolving: A problem-centered introduction to modeling strategic behavior*. Princeton: Princeton University Press.

Gilgor, D. M., Pillai, K. G., & Golgeci, I. (2021). Theorizing the dark side of business-to-business relationships in the era of AI, big data, and blockchain. *Journal of Business Research*, 133, 79–88.

Goertzel, B., & Pennachin, C. (2007). *Artificial General Intelligence*. New York: Springer.

Gregory, R. W., Henfridsson, O., Kaganer, E., & Kyriakou, H. (2021). The role of artificial intelligence and data network effects for creating user value. *Academy of Management Review*, 46(3), 534–551.

Grossman, S. J., & Hart, O. D. (1986). The costs and benefits of ownership: A theory of vertical and lateral integration. *Journal of Political Economy*, 94(4), 691–719.

Gursoy, D., Chi, O. H., Lu, L., & Nunkoo, R. (2019). Consumers acceptance of artificially intelligent (AI) device use in service delivery. *International Journal of Information Management*, 49, 157–169.

Haenlein, M., Huang, M. H., & Kaplan, A. (2022). Guest Editorial: Business Ethics in the Era of Artificial Intelligence. *Journal of Business Ethics*, 178, 867–869.

Hagui, A., & Wright, J. (2020). When Data Creates Competitive Advantage. *Harvard Business Review*, 98(1), 94–101.

Hajli, N., & Lin, X. (2016). Exploring the Security of Information Sharing on Social Networking Sites: The Role of Perceived Control of Information. *Journal of Business Ethics*, 133, 111–123.

Haley, A., Norvig, P., & Pereira, F. (2009). The Unreasonable Effectiveness of Data. *IEEE Intelligent Systems*, 24(2), 8–12.

Hancock, J. T., & Khoshgoftaar, T. M. (2020). Survey on categorical data for neural networks. *Journal of Big Data*, 7, 1–41.

Harrison, J. S., Bosse, D. A., & Phillips, R. A. (2010). Managing for stakeholders, stakeholder utility functions, and competitive advantage. *Strategic Management Journal*, 31(1), 58–74.

Harrison, J. S., & Bosse, D. A. (2013). How much is too much? The limits to generous treatment of stakeholders. *Business Horizons*, 56(3), 313–322.

Harrison, J. S., Phillips, R. A., & Freeman, R. E. (2020). On the 2019 business roundtable “statement on the purpose of a corporation”. *Strategic Management Journal*, 46(7), 1223–1237.

Herath, T., & Rao, H. R. (2009). Protection motivation and deterrence: A framework for security policy compliance in organizations. *European Journal of Information Systems*, 18(2), 106–125.

Huang, M.-H., & Rust, R. T. (2018). Artificial Intelligence in Service. *Journal of Service Research*, 21(2), 155–172.

Huang, M.-H., & Rust, R. T. (2020). Engaged to a Robot? The Role of AI in Service. *Journal of Service Research*, 24(1), 30–41.

Iansiti, M., & Lakhani, K. R. (2020). *Competing in the Age of AI: Strategy and Leadership when Algorithms and Networks Run the World*. Boston: Harvard Business Press.

Johnson, J. M., & Khoshgoftaar, T. M. (2019). Survey on deep learning with class imbalance. *Journal of Big Data*, 6(1), 1–54.

Jolls, C., Sunstein, C. R., & Thaler, R. (1998). A behavioral approach to law and economics. *Stanford Law Review*, 50, 1471–1550.

Kaplan, A., & Haenlein, M. (2019). “Siri, Siri, in my hand: Who’s the fairest in the land?” On the interpretations, illustrations, and implications of artificial intelligence. *Business Horizons*, 62(1), 15–25.

Kellogg, K. C., Valentine, M. A., & Christin, A. (2020). Algorithms at work: The new contested terrain of control. *Academy of Management Annals*, 14(1), 366–410.

Kumar, V., Rajan, B., Venkatesan, R., & Lecinski, J. (2019). Understanding the role of artificial intelligence in personalized engagement marketing. *California Management Review*, 61(4), 135–155.

Leonardi, P. M. (2021). COVID-19 and the new technologies of organizing: Digital exhaust, digital footprints, and artificial intelligence in the wake of remote work. *Journal of Management Studies*, 58(1), 249–253.

Leventhal, G. S., Karuza, J., & Fry, W. R. (1980). Beyond justice: A theory of allocation preferences. In G. Mikula (Ed.), *Justice and Social Interaction* (pp. 167–218). New York: Springer-Verlag.

Lichtenhaler, U. (2019). An intelligence-based view of firm performance: Profiting from artificial intelligence. *Journal of Innovation Management*, 7(1), 7–20.

Lindebaum, D., Vesa, M., & den Hond, F. (2020). Insights from the machine stops to better understand rational assumptions in algorithmic decision making and its implications for organizations. *Academy of Management Review*, 45(1), 247–263.

Loureiro, S. M. C., Guerreiro, J., & Tussyadiah, I. (2021). Artificial intelligence in business: State of the art and future research agenda. *Journal of business research*, 129, 911–926.

Lu, L., Cai, R., & Gursoy, D. (2019). Developing and validating a service robot integration willingness scale. *International Journal of Hospitality Management*, 80, 36–51.

Ma, J., Wen, Y., & Yang, L. (2019). Lagrangian supervised and semi-supervised extreme learning machine. *Applied Intelligence*, 49(2), 303–318.

Macaulay, S. (1963). Non-contractual relations in business: A preliminary study. *American Sociological Review*, 28(1), 55–67.

Makarius, E. E., Mukherjee, D., Fox, J. D., & Fox, A. K. (2020). Rising with the machines: A sociotechnical framework for bringing artificial intelligence into the organization. *Journal of Business Research*, 120, 262–273.

Markus, M. L. (2017). Datiification, organizational strategy, and is research: What's the score? *Journal of Strategic Information Systems*, 26(3), 233–241.

Martin, K. (2018). Ethical Implications and Accountability of Algorithms. *Journal of Business Ethics*, 160(4), 835–850.

McLean, G., Osei-Frimpong, K., & Barhorst, J. (2021). Alexa, do voice assistants influence consumer brand engagement? Examining the role of AI powered voice assistants in influencing consumer brand engagement. *Journal of Business Research*, 124, 312–328.

Meyer, G., Adomavičius, G., Johnson, P. E., Eldriši, M., Rush, W. A., Sperl-Hillen, J. M., & O'Connor, P. J. (2014). A machine learning approach to improving digital decision making. *Information Systems Research*, 25(2), 239–263.

Pattway, M. J., & Wang, X. Z. (2019). Sensitivity analysis on initial classifier accuracy in fuzziness based semi-supervised learning. *Information Sciences*, 490, 93–112.

Perez-Vega, R., Kaartemo, V., Lages, C. R., Razavi, N. B., & Männistö, J. (2021). Reshaping the contexts of online customer engagement behavior via artificial intelligence: A conceptual framework. *Journal of Business Research*, 129, 902–910.

Phillips, R. (2003). *Stakeholder theory and organizational ethics*. San Francisco: Berrett-Koehler Publishers.

Rai, A., Constantinides, P., & Sarker, S. (2019). Editor's comments: Next-generation digital platforms: Toward human-AI hybrids. *MIS Quarterly*, 43(1), iii–x.

Raisch, S., & Krakowski, S. (2021). Artificial Intelligence and Management: The Automation-Augmentation Paradox. *Academy of Management Review*, 46(1), 192–210.

Robinson, S., Orsingher, C., Akirek, L., De Keyser, A., Giebelhausen, M., Papamichail, K. N., ... Temekar, M. S. (2020). Frontline encounters of the AI kind: An evolved service encounter framework. *Journal of Business Research*, 116, 366–376.

Rocetti, M., Delnevo, G., Casini, L., & Cappiello, G. (2019). Is bigger always better? A controversial journey to the center of machine learning design, with uses and misuses of big data for predicting water meter failures. *Journal of Big Data*, 6(1), 1–23.

Russell, S. (2016). *Should we fear supersmart robots?* (pp. 58–59). June Issue: Scientific American.

Russell, S., & Norvig, P. (2016). *Artificial Intelligence: A Modern Approach* (Third Edition). Essex: Pearson Education Ltd.

Salovaara, A., Lyytinen, K., & Penttinen, E. (2019). High reliability in digital organizing: Mindlessness, the frame problem, and digital operations. *MIS Quarterly*, 43(2), 555–578.

Sarker, I. H. (2019). Context-aware rule learning from smartphone data: Survey, challenges and future directions. *Journal of Big Data*, 6(1), 1–25.

Sarker, I. H., Kayes, A. S. M., & Watters, P. (2019). Effectiveness analysis of machine learning classification models for predicting personalized context-aware smartphone usage. *Journal of Big Data*, 6(1), 1–28.

Sparrow, R., & Howard, M. (2017). When human beings are like drunk robots: Driverless vehicles, ethics, and the future of transport. *Transportation Research Part C: Emerging Technologies*, 80, 206–215.

Tarafdar, M., Beath, C. M., & Ross, J. W. (2019). Using AI to enhance business operations. *MIT Sloan Management Review*, 11, 37–44.

Tian, Y., Pei, K., Jana, S., & Ray, B. (2018). In DeepTest: automated testing of deep-neural-network-driven autonomous cars (pp. 303–314). Sweden: Gothenburg.

Van Doorn, J., Mende, M., Noble, S. M., Hulland, J., Ostrom, A. L., Grewal, D., & Petersen, J. A. (2017). Domo argato Mr. Robot: Emergence of automated social presence in organizational frontlines and customers' service experiences. *Journal of Service Research*, 20(1), 43–58.

Venkatesh, V., Morris, M. G., Davis, G. B., & Davis, F. D. (2003). User acceptance of information technology: Toward a unified view. *MIS Quarterly*, 27(3), 425–478.

Venkatesh, V., Thong, J. Y. L., & Zu, X. (2012). Consumer acceptance and use of information technology: Extending the unified theory of use of technology. *MIS Quarterly*, 36(1), 157–178.

Venkatesh, V., Thong, J. Y. L., Zu, X. V., Thong, J. Y. L., & Zu, X. (2016). Unified theory of acceptance and use of technology: A synthesis and the road ahead. *Journal of the Association for Information Systems*, 17(5), 328–376.

von Krogh, G. (2018). Artificial intelligence in Organizations: New Opportunities for Phenomenon-Based Theorizing. *Academy of Management Discoveries*, 4(4), 404–409.

Werder, A. V. (2011). Corporate governance and stakeholder opportunism. *Organization Science*, 22(5), 1345–1358.

Werhane, P. H. (1999). *Moral imagination and management decision-making*. New York: Oxford University Press.

Wright, S. A., & Schultz, A. E. (2018). The rising tide of artificial intelligence and business automation: Developing an ethical framework. *Business Horizons*, 61(6), 823–832.

Yadav, M. S. (2010). The decline of conceptual articles and implications for knowledge development. *Journal of Marketing*, 74(1), 1–19.

Yadav, S. S., & Jadhav, S. M. (2019). Machine learning algorithms for disease prediction using IoT environment. *International Journal of Engineering and Advanced Technology*, 8(6), 4303–4307.

Doug Bosse is a Professor of Management in the Robins School of Business. His research is focused on Stakeholder Theory and how its implications can affect firm performance. His research has been published in journals such as *Strategic Management Journal*, *Academy of Management Journal*, *Journal of Management*, and *Journal of Business Venturing*.

Steve Thompson is a Professor of Analytics and Operations in the Robins School of Business and an associated Professor at Mälardalen University. His research is focused on how organizations can utilize technology and analytics to transform organization. His current work is focused on how technology and analytics can be used to achieve sustainability goals. His research has been published in journals such as *Information Systems Research*, *MIS Quarterly*, *Operations Research*, and *Manufacturing & Service Operations Management*.

Peter Ekman is Professor of Industrial Marketing at Mälardalen University and Deputy Dean at the Swedish School of Management and Information Technology at Uppsala University. He is an applied researcher that worked for several years in technical manufacturing and consulting. His current research focuses on market transformation in the context of digitalization and sustainability. His research has been published in journals such as *Global Strategy Journal*, *Industrial Marketing Management*, and *Journal of Business Research*.