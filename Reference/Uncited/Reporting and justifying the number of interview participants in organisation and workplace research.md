

# Reporting and justifying the number of interview participants in organisation and workplace research

Saunders, Mark; Townsend, Keith

DOI:  
[10.1111/1467-8551.12182](https://doi.org/10.1111/1467-8551.12182)

License:  
Other (please specify with Rights Statement)

Document Version  
Peer reviewed version

Citation for published version (Harvard):  
Saunders, M & Townsend, K 2016, 'Reporting and justifying the number of interview participants in organisation and workplace research', *British Journal of Management*, vol. 27, no. 4, pp. 836-852.  
<https://doi.org/10.1111/1467-8551.12182>

[Link to publication on Research at Birmingham portal](#)

## Publisher Rights Statement:

This is the peer reviewed version of the following article: Saunders, M. N. K. and Townsend, K. (2016), Reporting and Justifying the Number of Interview Participants in Organization and Workplace Research. *British Journal of Management*, which has been published in final form at <http://dx.doi.org/10.1111/1467-8551.12182>. This article may be used for non-commercial purposes in accordance with Wiley Terms and Conditions for Self-Archiving

### General rights

Unless a licence is specified above, all rights (including copyright and moral rights) in this document are retained by the authors and/or the copyright holders. The express permission of the copyright holder must be obtained for any use of this material other than for purposes permitted by law.

- Users may freely distribute the URL that is used to identify this publication.
- Users may download and/or print one copy of the publication from the University of Birmingham research portal for the purpose of private study or non-commercial research.
- User may use extracts from the document in line with the concept of 'fair dealing' under the Copyright, Designs and Patents Act 1988 (?)
- Users may not further distribute the material nor use it for the purposes of commercial gain.

Where a licence is displayed above, please note the terms and conditions of the licence govern your use of this document.

When citing, please reference the published version.

###### Take down policy

While the University of Birmingham exercises care and attention in making items available there are rare occasions when an item has been uploaded in error or has been deemed to be commercially or otherwise sensitive.

If you believe that this is the case for this document, please contact UBIRA@lists.bham.ac.uk providing details and we will remove access to the work immediately and investigate.

###### Reporting and justifying the number of interview participants in organisation and workplace research

**Running head:** reporting and justifying interview participant numbers

Mark N.K. Saunders, Birmingham Business School, University of Birmingham, 116 Edgbaston Park Road, Edgbaston, Birmingham, B15 2TY, UK.

Email: m.n.k.saunders@bham.ac.uk

Mark N.K. Saunders is Professor of Business Research Methods at Birmingham Business School, University of Birmingham. His research interests include research methods, organisational trust and SMEs. Mark's research has been published in a range of journals including *Human Relations*, *Journal of Small Business Management* and *Social Science and Medicine*. His textbooks include *Research Methods for Business Students* (currently in its 7<sup>th</sup> edition).

Keith Townsend, Griffith Business School, Griffith University, Nathan Campus, Kessells Road, Nathan 4111, Australia.

Email: k.townsend@griffith.edu.au

Keith Townsend is Associate Professor at Griffith University, Brisbane, Australia. His research interests focus primarily on line managers (particularly frontline managers) and employee voice, but has a keen interest in understanding approaches to qualitative research methods.

**Acknowledgement:** The researchers thank Dr Sue Ressia (Griffith University) for her assistance in collecting data for this article.

# Reporting and justifying the number of interview participants in organisation and workplace research

## Abstract

This paper examines established practice regarding the reporting, justification and number of interview participants chosen within organisation and workplace studies. For such qualitative research there is a paucity of discussion across the social sciences, the topic receiving far less attention than its centrality warrants. We analysed 798 articles published in 2003 and 2013 in ten top and second tier academic journals, identifying 248 studies using at least one type of qualitative interview. Participant numbers were contingent on characteristics of the population from which chosen and approach to analysis; but not the journal, its tier, editorial base or publication year, the interview type or its duration. Despite lack of transparency in reporting (23.4% of studies did not state participant numbers) we reveal a median of 32.5 participants, numbers ranging from one to 330, and no justification for participant numbers in over half of studies. We discuss implications and, recognising different philosophical commitments are likely to imply differing norms, offer recommendations regarding reporting, justification and number of participants. Acknowledging exceptions, dependent upon study purpose and data saliency, these include an organisation and workplace research norm of 15-60 participants, alongside credible numbers for planning interview research.

**Key words:** interview, number of participants, sample size, qualitative interview, data saturation

## Introduction

Within organisation and workplace (O&W) empirical studies, qualitative interviews are a central technique, being employed frequently (Crouch and McKenzie, 2006) as “reliable gateways” into researching organisations (Alvesson and Ashcraft, 2012, p. 240). Invariably, the utility of such qualitative research interviews depends on the participant or participants chosen in terms of their coverage, and the quality of data within their responses (Alvesson and Ashcraft, 2012). In planning and operationalising research, sufficient participants need to be identified and chosen to provide breadth, depth and saliency of data necessary for authentic analysis and reporting (Lincoln and Guba, 1985; Curtis et al., 2000), and enable new insights and rich understandings (Patton, 2015). Yet, a recent review paper for the UK’s National Centre for Research Methods (Baker and Edwards, 2012) highlights a lack of explicit discussion across the social sciences regarding how many qualitative interview participants are deemed sufficient and, along with others (Curtis et al., 2000; Robinson, 2014), we suggest that this topic has received far less attention than its centrality warrants.

What is considered methodologically valid (authentic and credible) differs between communities of qualitative scholars (Baker and Edwards, 2012) with diverging philosophical commitments (Johnson et al., 2006), research being subject to peer review, (consensual) judgement and validation (Patton, 2015) by epistemological gatekeepers (Symon and Cassell, 1999). Acknowledging this, we commence with an overview of the literature regarding the number of interview participants likely to be sufficient, reviewing both empirically grounded evidence and expert opinions. This reveals a paucity of empirical research, highlighting the need to establish accepted practices regarding the reporting, justification and number considered sufficient within the broad network of communities of those undertaking O&W

research. Within our broadly pragmatist philosophy we believe such knowledge can be useful in informing the practice of qualitative researchers, whatever their epistemological and ontological positions, providing insights into current practice and enabling recommendations to support the planning, operationalisation and reporting of future research.

Using 248 O&W empirical studies in 244 articles drawn from a sample of 10 top and second tier research journals we examine established practice judged worthy of publication. In adopting a neo-empirical inductive position we engage with the data provided in these articles as empirical evidence analysing it systematically to draw out recommendations regarding the reporting, justification and number of qualitative interview participants. Within our analysis we consider possible differences in reporting practice contingent on the journal, and its tier, editorial base and year of publication, the type of qualitative interview and interview duration, the population from which participants were chosen and approach to analysis. Finally based on the practices reviewed, and recognising differing philosophical commitments will be reflected in research practice, we offer recommendations regarding the reporting, justification and number of participants. These include both a broad range for the number of qualitative interview participants and estimates of participant numbers for planning interview research that are likely to appear credible within O&W research.

### Qualitative Research Interviewing and the Number of Participants

### *Reporting Practices for Qualitative Interviews*

Qualitative interviews are used in a variety of research designs (Crouch and McKenzie, 2006; Brinkmann and Kvale, 2015). Unlike interviewer-administered questionnaires, they comprise a relatively free-flowing interchange of views between two or, in the case of group

interviews, three or more people (Saunders et al., 2016). Such interchanges are diverse in nature, varying between disciplinary roots and dependent upon epistemological and ontological assumptions (Johnson et al., 2006), being characterised by philosophical diversity (Guba and Lincoln, 1994) and within management research methodological pluralism (Easterby-Smith et al., 2008). They also vary dependent upon the research purpose, comprising both breadth and scope (Bryman, 2012); differing in the extent to which questioning is unstructured, and the conduct and duration of the interaction between interviewer and participant(s) (Brinkmann and Kvale, 2015). In comparison to quantitative research, qualitative interviews are argued to offer greater ecological validity, providing rich insightful accounts and the ability to help make sense of complex organisational realities (Eby et al., 2009).

In reporting research drawing on qualitative interviews, researchers are expected to explain and justify their data collection and analysis transparently in relation to their purpose (Baker and Edwards, 2012; Robinson, 2014); thereby allowing users to judge its utility. Dominant publication conventions, albeit neo-positivist (Alvesson and Ashcraft, 2012), dictate a description of method, outlining concisely how participants were chosen and data collected; and providing appropriate reflexive acknowledgement regarding bias or conflicts of interest to aid transparency (Meyrick, 2006; Robinson, 2014). This implies a need to state the number and characteristics of participants interviewed (Miles et al., 2013; Patton, 2015) and reasons for their selection (Marshall et al., 2013).

Literature regarding the number of participants usually treats each interview as a discrete event involving one or more participants. Participant numbers are argued to depend on the balance between representativeness (in a loose sense) and quality of responses (Alvesson and

Ashcraft, 2012) in obtaining sufficient information. For some studies, such as where the purpose is to establish if something is possible or provide a rich account, a single qualitative interview (or case) is argued to be appropriate (Becker, 2012; Patton, 2015). For others, such as where the purpose is to establish commonalities or allow comparison, a larger number is likely to be needed (Crouch and McKenzie, 2006; Baker and Edwards, 2012). More generally, Alvesson and Ashcraft (2012) consider it important to ensure coverage through variation amongst interview participants. In particular they warn against overemphasizing richness of responses, and over reliance on participants to whom researchers have ready access, to the detriment of some form of representation. Justification for the number of participants appears therefore to be based on transparency, showing data collected are of sufficient depth to provide salient information in relation to research purpose and of sufficient breadth to allow coverage within the responses.

A cadre of researchers, noting the open-ended nature of such qualitative research, argues that ideally data collection should continue until saturation (Morse, 1994) or informal redundancy (Lincoln and Guba, 1985) is reached. For some this concept, derived originally from Grounded Theory (Glaser and Strauss, 1967), is considered the “gold standard” (for example: Guest et al., 2006, p. 60). Others however, consider the adoption of saturation as a generic quality marker as inappropriate or undesirable: where saturation is not reached, it means only the phenomenon has yet to be fully explored rather than the findings being invalid (O'Reilly and Parker, 2013). Furthermore, indicating completeness by either replication or redundancy in data collected from further participants (Bowen, 2008) implies (like sufficiency) the number cannot be resolved definitively until data collection is underway (Safman and Sobel, 2004).

Despite this, researchers are often required to estimate and provide a rationale for likely participant numbers to support resource allocation (Baker and Edwards, 2012) or requests for access (McDonald et al., 2009). At this project proposal stage, the estimate and rationale depends upon the precise research purpose (Robinson, 2014), what is likely to be judged as credible by research users and peer reviewers and what can be achieved within available time and resources (Patton, 2015). Empirically justified guidance regarding the likely number of interviews is therefore, alongside expert opinion grounded in experience, likely to be of utility.

### *Empirically justified guidance*

Research exploring interview participant numbers is limited. Our search of the extant literature in four online databases (Business Source Complete, Emerald Insight, PsycINFO and SAGE Premier) revealed seven articles. Four review the number of qualitative interview participants in published studies for specific research communities represented by peer-reviewed journals, providing insights into established norms (Safman and Sobel, 2004; Collins et al., 2006, 2007; Marshall et al., 2013). The remaining three analyse qualitative interview transcripts to establish after how many interviews saturation is likely to occur (Marshall, 1996; Guest et al., 2006; Francis et al., 2010).

Consideration of the four articles reviewing the number of participants in peer-reviewed studies highlights considerable variability and lack of transparency in reporting across the social sciences. Safman and Sobel (2004) and Collins et al. (2006) note approximately 40% of research articles reviewed (in one Health Education and four School Psychology journals respectively) did not report the number of participants. Both recommend reporting actual participant numbers (sample size), emphasising a need for completeness to allow the work to

be evaluated and, where appropriate, used as a basis for similar studies. In contrast, Collins et al. (2007) report over 98% of studies in an unspecified number of Social and Health Science journals stated the number of participants. Studies also reveal considerable variation in the number of participant interviews; Safman and Sobel (2004) and Marshall et al. (2013) both noting ranges from less than 10, to in excess of 100 interviews for communities publishing in Health Education and Information Systems journals. However only Marshall et al. (2013, p. 20), drawing on studies reporting participant numbers in five Information Systems journals, infer “collective wisdom” regarding the number of qualitative interviews. They suggest 20-30 participants for grounded theory and 15-20 participants for single case-study strategies. Poor justification of the number of participants chosen is also highlighted; Safman and Sobel (2004) commenting there is insufficient evidence to gauge whether, or to what extent, researchers in Health Education are actively designing saturation into participant selection. Similarly, Marshall et al. (2013) reveal that although many Information Systems researchers invoke the concept of data saturation, only a few provide evidence; noting also that only one cites a similar study and none cite expert opinion as justification for the number of participants.

The three studies that analyse interview transcripts to establish when saturation is likely to occur each use data drawn from homogenous populations. Guest et al. (2006), in a study of African Females with HIV, highlight the link between the researcher, methodology and research objectives and a need for sufficient interviews to enable solid understanding of the phenomenon. They note that for relatively homogenous populations, between six and 12 participants should be adequate to reach saturation. Similar numbers are reported by Francis et al. (2010) and Marshall (1996) in studies using medical leaders, practitioners and patients' relatives, these ranging from 13 to 15. Although highlighting the need for more evidence to

establish convention for different types of interview study, Francis et al. (2010) propose 10 interviews as a guide of when to start looking for saturation followed by three more to substantiate it before stopping. Notwithstanding these three studies indicating broadly similar numbers, they ignore the alternative view that saturation may not be an appropriate measure of sufficiency (O'Reilly and Parker, 2013).

### *Expert opinion*

Within extant literature on interviews and participant selection/sampling, guidance in the form of “expert voices” rarely states a precise number of participants (Baker and Edwards, 2012, p. 6). Typically, such expert opinion advises the number should not be so small that it is difficult to obtain data saturation and yet, not too large to make in-depth analysis difficult (Onwuegbuzie and Leech, 2005); highlighting factors to consider, emphasising the importance of justifying sufficiency of participant numbers and noting sampling until saturation is an ideal that works best when resources are unconstrained (Patton, 2015).

In our literature search we found eight sources that, although offering guidance regarding participant numbers in qualitative interview research, were not justified by empirical evidence. Like the empirically justified guidance reviewed, these sources between them offer a numerical range for the number of interviews in general. Adler and Adler (2012) advise the broadest range of between a dozen and 60 interviews, whereas Brinkmann and Kvale (2015) recommend between five and 25 dependent upon purpose. Bertaux (1981) suggests up to 15 participants, whilst Becker (2012) notes that one participant may be sufficient for some purposes. Such guidance highlights a range of factors that need to be considered. Kuzel (1992) recommends that where the population of interest (and the participants to be chosen) is homogenous, six to eight participants are likely to be sufficient, whereas for heterogeneous

populations 12-20 participants are likely to be needed. For ethnographic research strategies Morse (1994) recommends between 30 and 50 participants, Bernard's (2000) suggestion of around 36 participants falling within this range. For grounded theory strategies Creswell (2007) advises 20-30 participants, whilst Morse (1994) suggests approximately 35. Creswell (2007) also advises between three and five interviews per case for case-study strategies. Much of this is summarised by Saunders (2012), who notes a range of four to 12 participants as likely to be sufficient when chosen from populations considered homogeneous, and 12 to 30 participants when chosen from populations considered heterogeneous.

Drawing on combined experiences of 14 social science experts, Baker and Edwards (2012) focus on the number of qualitative interviews *per-se*. Although half their experts offer numerical examples, only two (Adler and Adler, 2012; Becker, 2012) offer recommendations, derived mainly from research experiences. All experts use the caveat, 'it depends', noting participant numbers are contingent on the research purpose and a variety of epistemological, methodological and practical issues. These relate to saturation, a community's expected norms as often represented in peer reviewers' judgements (Easterby-Smith et al., 2008), methodological perspectives, theoretical underpinnings of the research and, variability in the population. In introducing the experts' voices Baker and Edwards (2012), like others, stress the importance of transparency and taking account of practical considerations. Noting the impossibility of specifying the precise number of interviews that will be required at project inception they, like Patton (2015), highlight the need to provide an indication at project proposal stage and therefore, albeit implicitly, for numerical guidance.

Advice regarding the number of participants required for qualitative interviews is therefore often opinion, albeit drawing on experience. Like empirically justified guidance, opinions are

couched in methodological and practical caveats alluding to philosophical diversity and associated alternative knowledge assumptions, homogeneous/heterogeneous groups, and case study and ethnographic strategies. Both guidance and opinions highlight a need for transparency in reporting and justifying the sufficiency of participants, albeit not necessarily through data saturation. They also emphasise a need to work within, rarely explicit, expectations or norms. This suggests four questions regarding O&W research practice:

1. To what extent do studies that use qualitative interviews report the number of participants?
2. Where participant numbers are reported, how is their sufficiency justified?
3. Is there a discernable norm (range) for the number of participants for such studies?
4. If discernable, is the range (norm) for the number of participants contingent upon the journal, the type and duration of interview, whether participants are treated as a homogeneous or heterogeneous group, research strategy and epistemological and ontological assumptions made?

Considering our four questions, some of those using qualitative interviews might argue such a pre-occupation with justifying, or perhaps defending, methods detracts markedly from the story being told (Janesick, 2000). Perhaps some might suggest the notion of quantifying how many qualitative interviews were undertaken in a study is folly. However for others the argument is that this adds richness to the reporting (Onwuegbuzie and Leech, 2005). From our pragmatist perspective, alongside the need to explain and justify data collection and analysis (Alvesson and Ashcraft, 2012) to users of O&W research; most studies utilising qualitative interviews require a provisional decision regarding likely participant numbers at the design stage (Baker and Edwards, 2012; Robinson, 2014). Yet, with a few exceptions, research evidence upon which to base such decisions is derived across the social sciences

(rather than being specific to O&W research), recommendations rarely being justified with clear evidence. Building on Marshall et al. (2013) and others' suggestion that precedence set by similar studies published in leading journals can provide empirical guidance regarding norms and support in justifying the number of participants, we now consider our research questions in the context of O&W research.

## Method

Our research design drew on previous studies (Safman and Sobal, 2004; Marshall et al., 2013), initially selecting journals that are 'highly regarded' by Business and Management scholars and likely to contain O&W studies using qualitative interviews. We defined these as comprising journals in the constituent fields of General Management (GM); Human Resource Management and Employee Relations (HRM/ER) combined; and Organisational Studies (OS). Between them these could be considered to represent claimed expertise of specialised knowledge and, through peer review, the numbers of participants and associated justifications having been judged acceptable by editors and reviewers.

To select journals we consulted the (then current) UK based Association of Business Schools *Academic Journal Quality Guide* (Harvey et al., 2010) for those considered to publish the most original and best executed research in the field (4 and 4\*) or original and well-executed research (3). We integrated this with the two top categories in the Australian Business Deans Council *Journal Quality List* (2013): A\* the highest, and A the second highest quality. This generated 26 journals included on both lists: 12 were 4 or 4\*/A\*, 11 were 3/A and three were categorised differently with one list ranking a journal 3/A\* and the other 4 or 4\*/A. We refer to these as 'top', 'second' and 'mixed tier' respectively. Whilst acknowledging arguments

that attributing quality to journals rather than articles is non-sensical and privileges accepted methodologies (Willmott, 2011), we note such lists can still offer a good indication of whether articles within are regarded highly by their communities (Mingers and Harzing, 2007). These journals therefore can be considered to reflect well-executed practice and reporting of qualitative interviewing, within the metrics privileged by these lists.

A pragmatic decision was made to select 10 journals within our framework, twice that used by previous research within a single discipline (Marshall et al., 2013). We were interested only in journals that publish empirical articles using primary data; hence, journals like *International Journal of Management Reviews* were excluded. Our final selection criterion required selected journals to have a mix of editorial bases thereby not privileging particular scales of investigation (Easterby-Smith et al., 2009) or communities represented (Easterby-Smith et al., 2008), but facilitating comparison between what are often referred to as European and North American traditions (Grey, 2010). Using all but the final criterion we initially selected at random one journal that was top, and one that was second tier for each of the three constituent fields (GM, HRM/ER, OS). We selected at random a further journal that was mixed tier for fields other than for OS; where, as no such journals existed, we selected another second tier journal at random. Our initial selection was then assessed and found to also meet our requirement for a mix of editorial homes (Table 1).

**Table 1: Interview Articles Compared with Total Empirical Articles, 2003 and 2013 (Excludes Literature Reviews, Editor**  **Introductions)**

| Fi<br>el                                              | Journal Name Editorial |            | Journal |           | 2003      |           |           | 2013      |           | Change between 2003 and 2013 | Total     |           |           |
|-------------------------------------------------------|------------------------|------------|---------|-----------|-----------|-----------|-----------|-----------|-----------|------------------------------|-----------|-----------|-----------|
| d                                                     |                        | Base       | Ranking | Empirical | Interview | Interview | Empirical | Interview | Interview | Empirical                    | Interview | Interview | interview |
|                                                       |                        | (2013)     |         | articles  | articles  | as %      | articles  | articles  | as %      | articles                     | articles  | articles  | articles  |
|                                                       |                        |            |         |           |           |           |           |           |           |                              |           | as %      |           |
| G<br>en<br>er<br>al<br>M<br>an<br>ag<br>em<br>en<br>t | Journal of             | USA,       | Top     | 61        | 34        | 55.74     | 37        | 12        | 32.43     | -24                          | -22       | -23.31    | 46        |
|                                                       | Management             | Netherland |         |           |           |           |           |           |           |                              |           |           |           |
|                                                       | Studies                | Australia  |         |           |           |           |           |           |           |                              |           |           |           |
|                                                       | British Journal        | UK         | Mixed   | 16        | 10        | 62.50     | 38        | 15        | 39.47     | +22                          | +5        | -22.03    | 25        |
|                                                       | of                     |            |         |           |           |           |           |           |           |                              |           |           |           |
|                                                       | Management             |            |         |           |           |           |           |           |           |                              |           |           |           |
|                                                       | Asia Pacific           | Canada     | Second  | 20        | 8         | 40.00     | 45        | 8         | 17.78     | +25                          | 0         | -22.22    | 16        |
|                                                       | Journal of             |            |         |           |           |           |           |           |           |                              |           |           |           |
|                                                       | Management             |            |         |           |           |           |           |           |           |                              |           |           |           |
| H<br>um<br>an<br>R<br>es<br>ou<br>rc                  | Industrial             | USA        | Top     | 26        | 3         | 11.54     | 32        | 3         | 9.38      | +6                           | 0         | -2.16     | 6         |
|                                                       | Relations^             |            |         |           |           |           |           |           |           |                              |           |           |           |
|                                                       | Human                  |            |         | 17        | ***8      | 47.05     | 38        | 9         | 23.68     | +21                          | +1        | -23.37    | 17        |

|                | Resource     | US/UK     |        |     |       |       |     |       |       |      |     |         |     |
|----------------|--------------|-----------|--------|-----|-------|-------|-----|-------|-------|------|-----|---------|-----|
|                | Management   |           |        |     |       |       |     |       |       |      |     |         |     |
|                | Work         | UK        | Mixed  | 29  | ***16 | 55.17 | 43  | ***27 | 62.79 | +14  | +11 | +7.62   | 43  |
|                | Employment   |           |        |     |       |       |     |       |       |      |     |         |     |
|                | and Society  |           |        |     |       |       |     |       |       |      |     |         |     |
|                | Human        | Ireland,  | Second | 18  | 9     | 50.00 | 23  | 11    | 47.83 | +5   | +2  | -2.17** | 20  |
|                | Resource     | Australia |        |     |       |       |     |       |       |      |     |         |     |
|                | Management   |           |        |     |       |       |     |       |       |      |     |         |     |
|                | Journal      |           |        |     |       |       |     |       |       |      |     |         |     |
| O              | Human        | UK, USA   | Top    | 38  | ***22 | 57.89 | 53  | ***25 | 47.17 | +15  | +3  | -10.72  | 47  |
| rg<br>an<br>is | Relations    |           |        |     |       |       |     |       |       |      |     |         |     |
| at<br>io<br>n  | Group and    | USA       | Second | 16  | 1     | 6.25  | 22  | 1     | 4.55  | +6   | 0   | -1.70   | 2   |
|                | Organization |           |        |     |       |       |     |       |       |      |     |         |     |
|                | Management^  | UK        |        |     |       |       |     |       |       |      |     |         |     |
|                | Organization |           | Second | 11  | 8     | 72.73 | 21  | 14    | 66.67 | +10  | +6  | -6.06   | 22  |
| Total          |              |           |        | 252 | 119   | 45.89 | 352 | 125   | 35.17 | +100 | +6  | -10.72  | 244 |

<sup>^</sup>as there was a small number of relevant interview based articles in IR (6 of 58) and GOM (2 of 42 empirical articles) we included another journal from these categories (*Human Resource Management* and *Organization* respectively) in to the analysis.;

<sup>\*\*</sup> one article reported two separate studies;

<sup>\*\*\*</sup> one of these articles included two distinct studies

Following research considering changes over time in sample size for surveys (Baruch, 1999; Baruch and Holtom, 2008) we examined two complete published volumes a decade apart for each of the ten journals; 2013 representing the most recent complete year at the time of selection and 2003 enabling us to consider possible differences a decade earlier. Of 798 articles in these volumes, 604 were empirical, 244 using qualitative interviews as primary data (Table 1); double the articles considered by any of the four studies reviewed earlier. Of these 244 articles, 52 used mixed methods, the remaining 192 using only qualitative methods.

Development and application of themes within the data followed a 'codebook' approach (MacQueen et al., 1998; Guest et al. 2006). Initially we considered coding the epistemological and ontological positions and research strategy adopted. Yet, despite the acknowledged philosophical diversity in Business and Management research (Easterby-Smith et al., 2008), the positions adopted were mentioned extremely rarely in articles. Similarly, very few noted whether participants were treated as a homogenous or heterogeneous group. Consequently, rather than presume we could determine such positions, these data were not included. However, where we could ascertain the broad characteristics of the population from which participants were chosen and the approach to analysis, these provided limited indications homogeneity/heterogeneity. *A priori* codes therefore included the number of participants; whether participants chosen were justified (implicitly or explicitly) by reference to the study's purpose, expert opinion, previous studies and saturation; whether the population from which the participants were chosen was defined as a single organisation, multiple organisations within the same sector or multiple organisations across different sectors; the duration of interviews as reported; and whether participants were analysed as one or two or more distinct groups.

Reporting of type(s) of qualitative interview used, varied markedly between studies. We therefore developed jointly initial *in vivo* codes, using terms from the first 43 articles. These comprised articles from the 2013 volumes of *British Journal of Management* (15), *Industrial Relations* (3), and *Human Relations* (25). Codes for type of interview recorded both interview conduct (qualitative; one-to-one including face-to-face, telephone, online; group including focus group) and the degree of structure (structured, semi-structured, unstructured). We each coded separately type(s) of interview used in the next 20 articles from the 2013 volumes of the *Journal of Management Studies* (12) and *Asia Pacific Journal of Management* (8) using the schedule of definitions and compared our coding. Where there were disagreements, we debated and clarified our understandings and agreed final codes (Table 2) that were used for the remaining articles, no additional codes being necessary.

Our data comprised 248 studies (reported in 244 articles) using one or more types of qualitative interview. 24 studies were in journals with a North American editorial base, 112 in journals with a European/Australian editorial base, the remainder having mixed editorial bases (both North America and Europe/Australia). Of the studies, 197 report using one type, 47 two different types, and four using three different types of qualitative interview. The vast majority (91.1%) for which conduct was stated, were qualitative one-to-one interviews (Table 2). Of those for which degree of structure was stated, 89.3% were semi-structured (Table 2).

**Table 2: Types of Qualitative Interviews Used in Studies**

| Conduct | Structure  |            |       |     |      |
|---------|------------|------------|-------|-----|------|
|         | Not stated | Structured | Semi- | Un- | All* |

|                         | structured | structured |     |   |     |
|-------------------------|------------|------------|-----|---|-----|
| Not stated              | 32         | 0          | 11  | 1 | 44  |
| One-to-one              | 42         | 1          | 46  | 3 | 92  |
| One-to-one face-to-face | 73         | 2          | 48  | 5 | 128 |
| One-to-one telephone    | 14         | 1          | 0   | 0 | 15  |
| One-to-one online       | 1          | 0          | 0   | 0 | 1   |
| Group                   | 5          | 0          | 3   | 0 | 8   |
| Focus group             | 15         | 0          | 0   | 0 | 15  |
| All*                    | 182        | 4          | 108 | 9 | 303 |

\*197 studies used one type, 47 studies used two different types and four studies used three different types of qualitative interviews.

## Findings

### *Reporting and Justifying the Number of Participants*

Our first research question considered the extent of reporting for participant numbers. The total number of participants across all types of qualitative interviewing was reported by 190 (76.6%) of studies. A further 4.8% gave some indication of the total number of participants, whilst the remaining 18.5% did not provide any indication<sup>1</sup>. Where an indication, rather than the precise number, of participants was given, this was either a range; for example: “...the number of respondents interviewed between the companies ranging from six to 16.” (Hall et

<sup>1</sup> Percentages do not sum precisely to 100 due to rounding.

al., 2003, p. 79), or a base number that was exceeded such as: “...over a dozen interviews...” (Chen and Wilson, 2003, p. 401). Such indications were part of a description of participants’ characteristics, usually contextualised briefly in relation to the population from which they were chosen. These provided insights into the breadth and richness of data obtained and, occasionally, an implicit reason regarding why a precise number had not been stated; for example: “...personal interviews the author did over the course of a decade with numerous people at Delta from top executive level to the ramp and cabin level” (Kaufman, 2013, p. 344). The precise number of participants was reported by 76.1% of studies using one type of qualitative interview. For studies using two or more types of qualitative interview, the number of participants for each type was reported far less frequently, being 27.7% for studies using two and 25% for studies using three types.

With regard to our second research question, approximately half of studies stating the number of participants justified the sufficiency in relation to the research purpose (Table 3). For 13.7% of studies it comprised a description of participants’ characteristics relating these explicitly to the research purpose; for example: “The selection of interviewees was made on the basis of including key figures involved in the strategy creation issues” (Regnér, 2003 p. 64). More typical was an implicit justification through a description of participants’ characteristics. For 35.2% of studies, such descriptions outlined the population from which participants were chosen alongside further contextual details. These usually included an anonymised description of the organisation, sector, or range of sectors from which participants were chosen. Single organisation descriptions were usually brief, for example: “...a major British symphony orchestra” (Matlis and Lawrence, 2003, p. 113) or “...an English local authority” (Collins et al., 2013, p. 214). Those for multiple organisations and sectors were more varied, the more detailed outlining the nature and number of organisations

or sectors: “...*three product lines within the grocery supply chain, notably fish processing, vegetable processing and distribution and warehousing.*” (Thompson et al., 2013, p. 133). However, it was left to the reader to infer how those participants chosen enabled the research purpose to be met.

**Table 3: Justifications Used for Number of Participants Chosen**

| Justification used                                                                                   | Number of studies | As a percentage     |                |
|------------------------------------------------------------------------------------------------------|-------------------|---------------------|----------------|
|                                                                                                      |                   | of studies          | of all studies |
|                                                                                                      |                   | stating sample size |                |
| Meeting research purpose (explained why participants are sufficient)                                 | 26                | 13.7                | 10.5           |
| Meeting research purpose (can infer sufficiency of participants from description of characteristics) | 67                | 35.2                | 27.0           |
| <i>Meeting research purpose (sub total)</i>                                                          | 93                | 48.9                | 37.5           |
| Indicative (mean) duration of interviews                                                             | 43                | 22.6                | 17.3           |
| Indicative range of duration of interviews                                                           | 61                | 32.1                | 24.6           |
| Both indicative mean and range of duration of interviews                                             | 3                 | 1.6                 | 1.2            |

| <i>Duration of interviews (sub total)</i>                         | 101        | 53.2       | 40.7       |
|-------------------------------------------------------------------|------------|------------|------------|
| <i>Expert opinion cited</i>                                       | 5          | 2.6        | 2.0        |
| <i>Previous studies with similar number of participants cited</i> | 5          | 2.6        | 2.0        |
| Data saturation (not supported by citation)                       | 5          | 2.6        | 2.0        |
| Data saturation supported by citation                             | 3          | 1.6        | 1.2        |
| Data saturation supported by clear evidence of saturation         | -          | -          | -          |
| <i>Data saturation (sub total)</i>                                | 8          | 4.2        | 3.2        |
| None (excluding duration of interviews)                           | 97         | 51.1       | 62.5       |
| None (including duration of interviews)                           | 79         | 41.6       | 31.9       |
| <b>Total (=100%)</b>                                              | <b>190</b> | <b>190</b> | <b>248</b> |

N.B. Numbers sum to more than 100 as multiple justifications were used in some studies

Descriptions of participants' characteristics indicated how each was treated within that research; either as one or a number of discrete groups. The group or groups were usually

described briefly in relation to the research purpose, offering only implicit justifications for their selection. Creed's single group study (2003, p. 1508) of voice and silence in organisations illustrates a more detailed implicit justification: "*37 GLBT [Gay, Lesbian, Bisexual, Transgender] people who had trained for ordination by obtaining the Master of Divinity Degree and applying for admission to the formal candidacy process in their respective denominations...*"

Duration of interviews was reported by 101 studies either as an indicative (mean) value or, alternatively, a range; only three studies giving both (Table 3). Statements of duration were predominantly approximate and (although we report them in table 3 as a 'justification') appeared to form part of a contextual description of the interviewing process rather than an implicit justification of participant numbers, for example: "*Interviews which were recorded lasted about an hour were conducted between April and June 2009 in English, a language in which all respondents were fluent*" (Mustafa and Gold, 2013, p. 416), or "*...interviews took place in the offices of the interviewees or in available meeting rooms at the workplace and lasted between one and two hours*" (Lauring, 2013, p. 216).

Only 17 studies (8.9%) justified the number of participants using at least one of: demonstrating saturation, citing expert opinion, or citing precedence set by another study. No studies provided clear evidence of saturation, it being mentioned by only 4.2% of studies, (Table 5), 2.6% of studies supporting their claim by citation: "*However, we felt we did not need more, because interviews with our informants nicely fitted into the categories we built during the last phase, and we felt that category saturation had occurred (Strauss and Corbin, 1998)*" (Daudigeos, 2013 p. 731). Citing expert opinion and citing other (sometimes similar) studies, were each used as justification by 2.6% of studies. These varied in detail from

reiterating a recommendation without noting contextual differences (for example, that of Guest et al., 2006) to providing an epistemological justification within which similar O&W studies were cited; both leaving the relevance, authenticity and credibility of the work cited to be inferred by the reader. Two of the five studies citing expert opinion had 10 or fewer participants indicating such justifications, although rare, may be more likely to be provided for studies with low numbers of participants. Leitch et al. (2013, p. 353), for example note alongside discussion of their nine participants' characteristics:

“... a research approach which focuses on the development of rich descriptions and is sensitive to the 'subtleties and situated nuances of leadership practice' (Kempster and Cope, 2010, p. 11) has been adopted. However, small-scale, qualitative studies in the interpretivist tradition do not allow for generalizability; their strength lies in their capacity 'to provide insights, rich details and thick descriptions' (Jack and Anderson, 2002, p. 473).”

In summary, three quarters of O&W studies analysed state the overall number of participants precisely. Yet, where studies use more than one qualitative interviewing technique, only one-quarter state the number of participants for each technique. Justification of participants chosen is usually implicit, a description inferring they are sufficient in number to provide the depth and breadth of data to meet the research purpose. Duration of interviews is reported either as a mean value or range, providing context for the interviewing process rather than as justification for number of participants. Although some studies refer to expert opinion in relation to depth of cover as a justification, very few use this to justify number of participants. Where saturation is used as a justification, no supporting evidence is provided. Citation of previous studies, although used rarely as justification, appears to be more likely

for studies with fewer participants. We draw on these findings further in our discussion and offer recommendations regarding reporting and justifying the number of participants.

### *A Discernable Norm (Range) for the Number of Participants*

Our third research question considered whether there is a discernable norm (range) for the number of participants for (O&W) studies. Initial analysis of the overall number of qualitative interview participants in the 190 studies reporting precise numbers reveals considerable variability (Table 4). Although the median overall number of participants interviewed per study is 32.5, reported numbers range from one to 330; lower (18.75) and upper (57.25) quartiles providing an indication of the variability in number of participants considered sufficient when more extreme values are ignored. Recognising that these data are likely to represent a variety of research purposes and plurality of philosophical traditions we consider they can be used only to induce a broad overall norm for practice likely to be considered sufficient of between approximately 15 and 60 participants<sup>2</sup>.

As indicated by our fourth research question, our analysis examined whether the number of participants was contingent upon a variety of factors. There were no significant differences in the overall number of participants between journals [F(9,180)=1.679,  $p=.097$ ], tiers of journals [F(2,187)=2.392,  $p=.094$ ], editorial base [F(2,187)=0.20,  $p=.980$ ], or between the two years of publication [ $t=987$ ,  $df=125.817$ ,  $p=.325$ ]<sup>3</sup>. We also found no significant relationship between the overall number of participants and the indicative mean duration of interviews [ $t(43)=-.135$ ,  $p=.388$ ]. This indicates our proposed overall norm is not contingent on these factors; being applicable across those journals reviewed, albeit subject to refinement dependent on the research purpose and epistemological and ontological positions adopted.

<sup>2</sup> With a lower quartile of 18.75 and the upper quartile of 57.25 we have rounded our proposal for an overall norm to between 15 and 60.

<sup>3</sup> Levene's test for equality of variances [F = 4.213,  $p = .042$ ] indicated variances were not equal

**Table 4: Overall Number of Participants in Interviews by Journal**

| Field              | Journal                            | Tier   | Mean | Median | SD   | Lower Quartile | Upper Quartile | Number of studies |
|--------------------|------------------------------------|--------|------|--------|------|----------------|----------------|-------------------|
|                    |                                    |        |      |        |      |                |                |                   |
| General Management | Journal of Management Studies      | Top    | 59.4 | 40.0   | 61.0 | 13.50          | 70.50          | 37                |
|                    | British Journal of Management      | Mixed  | 38.1 | 34.0   | 28.6 | 17.25          | 53.75          | 20                |
|                    | Asia Pacific Journal of Management | Second | 46.6 | 22.0   | 48.6 | 13.00          | 87.00          | 9                 |
|                    | Industrial Relations               | Top    | 10.3 | 10.0   | 1.5  | -              | -              | 3                 |
|                    | Human Resource Management          | Top    | 28.2 | 16     | 32.5 | 3.75           | 54.00          | 6                 |
|                    | Work, Employment and Society       | Mixed  | 43.5 | 31.0   | 35.4 | 21.00          | 60.00          | 39                |
|                    | Human Resource                     | Second | 73.1 | 41.0   | 83.7 | 27.50          | 60.75          | 14                |

|                        | Management |       |       |      |       |       |       |    |
|------------------------|------------|-------|-------|------|-------|-------|-------|----|
|                        | Journal    |       |       |      |       |       |       |    |
| Organisation           | Human      | Top   | 36.9  | 30.0 | 26.1  | 17.50 | 86.75 | 41 |
|                        | Relations  |       |       |      |       |       |       |    |
| Group and Organization | Second     | 105.0 | 105.0 | N/A  | -     | -     | 2     |    |
| Management             |            |       |       |      |       |       |       |    |
| Organization           | Second     | 47.5  | 30.0  | 62.4 | 23.00 | 40.00 | 19    |    |
| All                    |            | 46.9  | 32.5* | 48.6 | 18.75 | 57.25 | 190   |    |

\* The median is calculated using the raw data from all of the journals.

Consideration of the most widely reported types of qualitative interview (one-to-one, one-to-one face-to-face, semi-structured), reveals similar median numbers of participants (30 or 30.5) for each type (Table 5); statistical analysis indicating the number of participants was not contingent on interview type. There were no significant differences in the number of participants for one-to-one interviews between the journals [F(8,57)=.801,  $p=.604$ ], the tiers of journals [F(2,63)=.517,  $p=.599$ ], the editorial bases of journals [F(2,63)=.635,  $p=.533$ ], or between the two years of publication [ $t=.373$ ,  $df=64$ ,  $p=.710$ ]. We also found no significant relationship between the number of participants for one-to-one interviews and the indicative mean duration of interviews [ $r(17)=-.017$ ,  $p=.949$ ]. For one-to-one face-to-face interviews we found no significant differences between the number of participants and the journals [F(9,75)=1.149,  $p=.340$ ], the tiers of journals [F(2,82)=2.766,  $p=.069$ ], the editorial bases of journals [F(2,82)=.309,  $p=.735$ ], or between the two years of publication [ $t=1.327$ ,  $df=83$ ,  $p=.188$ ]; and no significant relationship between the number of participants and the indicative mean duration of interviews [ $r(19)=-.274$ ,  $p=.255$ ]. For semi-structured interviews we found

no significant differences between the number of participants and the journals [F(8,71)=.854,  $p=.559$ ], the tiers of journals [F(2,69)=.159,  $p=.853$ ], the editorial bases of journals [F(2,69)=.345,  $p=.709$ ], or between the two years of publication [ $r=.744$ ,  $df=70$ ,  $p=.459$ ]; and no significant relationship between the number of participants and the indicative mean duration of interviews [ $r(22)=-.143$ ,  $p=.525$ ].

The overall number of participants is however contingent on both the broad characteristics of the population from which they were chosen and the approach to analysis. The number of participants differs significantly in relation to the characteristics of the population from which they were chosen [F(2,180)=10.578,  $p<.000$ ] (Table 5). For single organisation studies, the median number of participants interviewed (27) and variability, as indicated by the lower (15) and upper (40) quartiles, are smallest. Studies selecting participants from multiple organisations in one sector interview the largest median number (48), the lower (21.5) and upper (80.5) quartiles indicating even greater variability in participant numbers. Studies in which participants are referred to and analysed as a single group (such as managers or employees) differ significantly from those where they are analysed as two or more distinct groups [ $r=2.492$ ,  $df=187$ ,  $p=.013$ ]. For single group studies the median (29.5) number of participants is less than those analysing participants as two or more distinct groups (36.0), differences between the lower (21.5 and 21 respectively) and upper (80.5 and 63.5 respectively) quartiles indicating greater variability in the number of participants for studies analysed as two or more distinct groups. Acknowledging these data are likely to represent a variety of research purposes and plurality of philosophical positions, we believe these contingent factors still deserve consideration when making decisions regarding likely participant numbers. We therefore use the medians as a basis for initial estimates of around

30<sup>4</sup> where planning to choose participants from a single organisation or analysing them as a single group, and 50<sup>5</sup> where planning to choose participants from multiple organisations or analyse them as two or more distinct groups.

**Table 5: Overall Number of Participants Interviewed in Studies by Interview type, Population and Approach to Analysis**

|                                           | Population/<br>participant<br>analysis/<br>interview<br>type | Mean | Median | SD   | Lower<br>quartile | Upper<br>quartile | Number<br>of<br>studies |
|-------------------------------------------|--------------------------------------------------------------|------|--------|------|-------------------|-------------------|-------------------------|
| Qualitative<br>interview type             | One-to-one                                                   | 39.9 | 30     | 41.1 | 15.75             | 46.00             | 66                      |
|                                           | One-to-one<br>face-to-face                                   | 47.9 | 30     | 54.1 | 16.00             | 59.00             | 85                      |
|                                           | Semi-<br>structured                                          | 42.8 | 30.5   | 42.9 | 17.25             | 56.75             | 72                      |
| Broad<br>characteristics<br>of population | One<br>organisation                                          | 29.3 | 27.0   | 20.9 | 15.0              | 40.0              | 67                      |
|                                           | Multiple<br>organisations<br>in one sector                   | 68.7 | 48.0   | 69.5 | 21.5              | 80.5              | 53                      |
|                                           | Across<br>multiple<br>sectors                                | 49.5 | 35.0   | 43.0 | 21.0              | 63.0              | 63                      |

<sup>4</sup> We have rounded the larger (29.5) of the median numbers for participants chosen from one organisation and participants analysed as a single group to provide an estimate of 30.

<sup>5</sup> We have rounded the larger (48) of the median numbers for participants of chosen from multiple organisations in one sector or across multiple sectors, and participants analysed as two or more distinct groups to provide a planning estimate of 50.

| Approach to analysis | Single group    | 36.6 | 29.5 | 40.5 | 14.0  | 45.0  | 76  |
|----------------------|-----------------|------|------|------|-------|-------|-----|
|                      | Two or more     | 54.3 | 36.0 | 52.4 | 22.0  | 66.5  | 113 |
|                      | distinct groups |      |      |      |       |       |     |
| All studies          |                 | 46.9 | 32.5 | 48.6 | 18.75 | 57.25 | 190 |

N.B. Totals may not sum due to missing data

In summary, the number of participants chosen for studies overall, and for different types of qualitative interviews were not found to be contingent upon five factors: the journal, tier of journal, editorial base, duration of interview, or year of publication. Hence, despite the decade gap and the potential differences that journals' rankings or communities represented by reviewers and editors for such journals could have made, we feel able to utilise data about the accepted practice in these 190 studies to offer an overall norm of between 15 and 60 participants within which studies are likely to be judged sufficient. The two factors on which numbers of participants were found to be contingent provide the basis for numerical guidance when planning research. These comprise estimates of around 30 participants when chosen from a single organisation or analysed as a single group, and around 50 participants when chosen from multiple organisations or analysed as two or more groups. We draw on these, albeit with caveats, offering recommendations regarding the number of participants in our discussion.

## Discussion

Our paper is one of the first, if not the first, to examine the reporting and justification of participants and the number judged sufficient for qualitative interviews in published O&W

research. Adopting a broadly pragmatist philosophy and within this a neo-empiricist position we first draw on the findings outlined to offer four recommendations for reporting and justifying the number of participants in qualitative interviews. Subsequently a further three recommendations, albeit with caveats, are offered regarding to the number of participants likely to be judged sufficient and credible. These comprise, an overall norm well as initial estimates for numbers of participants when planning such research.

### *Reporting and Justifying the Number of Participants*

In undertaking this research we were struck by the lack of information regarding method and methodology. Despite acknowledged philosophical diversity (Johnson et al., 2006; Easterby-Smith et al., 2008), positions adopted were rarely mentioned and few studies noted whether participants were chosen from homogenous or heterogeneous groups. Whilst a transparent and convincing methods section might be considered as supporting neo-positivism (Alvesson and Ashcraft, 2012), we note it offers an opportunity for others to assess authenticity and credibility (Miles et al., 2013; Patton, 2015) and potential for informing similar studies (Safman and Sobal, 2004; Onwuegbuzie and Leech, 2007). Such transparent report of research design is predicated on there being sufficient space within journals to report methodology transparently. Yet, all but one of the O&W journals selected for this research impose a maximum length for submissions. Such limits may constrain the space that can be devoted to research design and, as part of this, reporting full details of participants. The facility for online publication of supplementary materials alongside articles, now provided by an increasing number of journals, offers a potential solution that could be utilized by authors (subject to editor agreement) to provide full details of participants as part of an expanded

methods section, providing the basis of our first recommendation for those undertaking qualitative interviews in O&W research:

1. Where space constraints prevent full and detailed reporting of research design, include an additional expanded methods section as supplementary material to be published online alongside the article.

Reporting the precise number of participants, their characteristics and those of the population from which they are chosen allows readers to understand more fully how the research was undertaken; forming an opinion regarding their authenticity and credibility and, where appropriate, the transferability of findings to other contexts (Lincoln et al., 2011). Findings reveal that although 76.6% of O&W studies using qualitative interviews report overall participant numbers precisely, nearly a fifth (18.5%) provide no indication. Although this is higher than the 60% of studies that report the number of participants by research in Heath Education (Safman and Sobel, 2004) and School Psychology (Collins et al., 2006), it is lower than the proportion (98%) found in Social and Health Sciences (Collins et al., 2007). We reiterate that a lack of basic details of method such as these detracts from transparency of the research design (Meyrick, 2006; Baker and Edwards 2012). Reflecting this, and noting that for some studies in our sample (for example: Patriotta, 2003) reporting participant details may not possible due to the complexity of the research design, length of the research process and nature of the interviews; our second recommendation is:

1. Wherever practicable, report participant numbers precisely alongside their characteristics and the population from which chosen.

For the majority of published O&W studies using qualitative interviews, justification of the number of participants, where offered, is implicit rather than explicit in relation to the

research purpose. Recognising stated publication conventions, and within this the importance of justification of methods in relation to research purpose (Robinson, 2014; Brinkmann and Kvale, 2015), our review of existing literature highlights three further ways the number of participants might be justified explicitly. These are: citing expert opinion (Marshall et al., 2013); citing precedence set by similar studies (Francis et al., 2010; Marshall et al., 2013); and by demonstrating saturation (Guest et al., 2006; Francis et al., 2010; Marshall et al., 2013). Yet, of the 190 studies that stated the number of participants, just below half justify the reasons for their choice either implicitly or explicitly in relation to research purpose; only few doing so explicitly. Two other forms of justification recommended in the advice reviewed, expert opinion and precedence set by similar studies (Baker and Edwards, 2012; Marshall et al., 2013), were mentioned very rarely in the studies considered; their use appearing more likely for those with fewer participants. We reiterate that referring to precedence set by authentic and credible studies with a similar research purpose and expert opinion can offer useful contextual justification for participants chosen.

Despite saturation being considered the gold standard by some (Guest et al., 2006) and invoked by many Information Systems' studies (Marshall et al., 2013), this was mentioned by very few O&W studies, none of which offered supporting evidence. Yet, where appropriate to the researcher's epistemological position and method (for example Grounded Theory), saturation could be utilised, providing specific insights as to sufficiency in relation to the research purpose. Noting that pressure for qualitative researchers to justify their research methods according to inappropriate criteria should be resisted (Symon and Cassell, 1999), we would encourage those using qualitative interviews in O&W research to explain their choice of participants in relation to their research purpose and philosophical position. Our third and

fourth recommendations, need therefore to be operationalized from within the context of the researcher's epistemological and ontological positions and are:

1. Explain explicitly how the participants chosen enable the research purpose to be met.
2. Consider the appropriateness of justifying the number of research participants through citing relevant expert opinion, precedence set by authentic and credible similar studies and, the meeting of data saturation (supported by clear evidence).

### *A Discernable Norm (Range) for the Number of Participants*

Our recommendations relating to numbers of participants are informed invariably by our initial neo-empiricist assumption that we could discern norms from what we observed. Within academic research such norms are operationalized by consensual agreement between reviewers and editors as gatekeepers to journals (Easterby-Smith et al., 2008). Our findings highlight that for O&W research, such agreement is not contingent upon the journal, its tier or editorial base, year of publication or the mean duration of interviews.

The data we collected reveal an overall norm of between approximately 15 and 60 participants for qualitative interviews within O&W studies. This norm indicates that the number of participants likely to be considered sufficient is both more varied and greater than the 15-30 participants suggested by empirically justified guidance (Marshall et al., 2013), and all expert opinions reviewed other than Adler and Adler (2012) who advise between 12 and 60. One interpretation of the broader range of interview participant numbers considered sufficient is that it is in part, due to the pluralist nature of O&W research. The lack of detail in the articles reviewed meant we were unable to determine the epistemological and ontological positions adopted and, invariably results in the norm induced being generic. Prior

to offering our fifth recommendation, we therefore re-emphasise the need for O&W researchers to consider the implications of their own epistemological and ontological assumptions for the number of participants, and reiterate expert opinion that one qualitative interview can be sufficient to produce salient data for some research purposes (Becker, 2012; Patton, 2015):

5. Recognise that while a norm of between 15 and 60 interview participants is likely to be considered sufficient; the actual number depends upon research purpose, saliency of data, and the researcher's epistemological and ontological positions.

Our review of empirically justified guidance and expert opinion indicated the number of participants is likely to be contingent on whether participants are selected from a homogenous or a heterogeneous population (Guest et al., 2006; Kuzel et al., 1992; Saunders, 2012). For O&W research we found the number of participants is contingent on the broad characteristics of the population from which participants are chosen and the approach to analysis, noting both factors can offer indications of homogeneity/heterogeneity. While it might be argued that such data should used to derive norms for participants from homogenous and heterogeneous populations we contend it would be unwise to base recommendations on two limited indicators.

Literature revealed the need for credible estimates of the number of participants when planning research (Patton, 2015) to support both resource allocation (Baker and Edwards, 2012) and requests for access (McDonald et al., 2009). Recognising our norm comprising a broad range is unlikely to fully address this need, and that participant numbers are contingent upon both the broad characteristics of the population from which chosen and the approach to analysis, we derive estimates from O&W practice taking both factors into account. These

comprise around 30 participants where chosen from a single organisation or analysed as a single group, and around 50 participants where chosen from multiple organisations or analysed as multiple groups. Both estimates are larger than the upper limits suggested by previous empirically justified guidance of 20 for single case studies (Marshall et al., 2013), and 25 for interview studies (Brinkmann and Kvale, 2015). The estimate for participants chosen from a single organisation or analysed as a single group is also higher than upper limits suggested by all but two experts; Morse's (1994) being 50 participants and Adler and Adler's (2012) 60 participants. As before our associated recommendations for O&W researchers, based on these estimates, do not take into account how participant numbers may be impacted upon by epistemological and ontological assumptions nor the study purpose:

1. In planning research where participants will be chosen from a single organisation, or analysed as a single group, recognise that an initial estimate of around 30 participants, whilst credible, is only an estimate.
2. In planning research where participants will be chosen from multiple organisations, or analysed as multiple groups, recognise that an initial estimate of around 50 participants whilst credible, is only an estimate.

## *Conclusion*

Our inductive analysis was hampered by a lack of information about research method, including detail regarding the number of participants, the associated justification, and participants' characteristics. Nevertheless we are still able to establish current practice and, drawing upon this, offer seven recommendations to O&W studies researchers using qualitative interviews. Our first four recommendations are made in response to this lack of

detail and reflect our position that precise reporting and, in particular justification, enables greater transparency (Baker and Edwards, 2012; Robinson, 2014) in O&W research. Whilst noting the potential issue of constraints on reporting within these recommendations we recognise that for some studies precise reporting may still not be possible, for some epistemological and ontological positions being considered unnecessary (Janesick, 2000); and that justification needs to be related explicitly to the research purpose. We also note that additional rarely used forms of justification, such as citing similar authentic and credible studies for comparative purposes and providing clear evidence of data saturation (for details see Francis et al., 2010), may be considered inappropriate.

Our remaining recommendations offer guidance regarding what is likely to be considered sufficient and credible in O&W research, based on what has been deemed acceptable by editors and reviewers. However there are limitations and caveats as sufficiency and credibility will be related to the research purpose, homogeneity/heterogeneity of the population, saliency of data and the researchers' epistemological and ontological assumptions. The implications of these aspects for norms of numbers of participants, and associated credible estimates, are aspects we believe warrants further research. Despite this we contend adopting our seven recommendations can both promote more transparent reporting and allow O&W researchers to make a more informed assessment of the number of qualitative interview participants that are likely to be sufficient and credible.

## References

Adler, P., and Adler, P. (2012). 'Expert voice'. In S. E. Baker and R. Edwards, *How many qualitative interviews are enough?* National Centre for Research Methods Review Discussion Paper, pp. 8-11. Retrieved from <http://eprints.ncrm.ac.uk/2273/> [Accessed 9 May 2015].

Alvesson, M., and Ashcraft, K. L. (2012). 'Interviews'. In G. Symon and C. Cassell (Eds.), *Qualitative organizational research: core methods and current challenges*, pp. 239-257. London: Sage.

Australian Business Deans Council (2013). *ABDC journal quality list 2013*. Retrieved from <http://www.abdc.edu.au/pages/abdc-journal-quality-list-2013.html> [Accessed 6 April 2015].

Baker, S. E., and Edwards, R. (2012). *How many qualitative interviews are enough?* National Centre for Research Methods Review Discussion Paper. Retrieved from <http://eprints.ncrm.ac.uk/2273/> [Accessed 9 May 2015].

Baruch, Y. (1999). 'Response rates in academic studies –a comparative analysis'. *Human Relations*, 52, pp. 421-430.

Baruch, Y. and Holtom, B. C. (2008). 'Survey response rates, levels and trends in organizational research'. *Human Relations*, 61, pp. 1139-1160.

Becker, H. S. (2012). 'Expert Voice'. In S. E. Baker and R. Edwards, *How many qualitative interviews are enough?* National Centre for Research Methods Review Discussion Paper, p. 15. Retrieved from <http://eprints.ncrm.ac.uk/2273/> [Accessed 9 May 2015].

Bernard, H. R. (2000). *Social research methods*. Thousand Oaks, CA: Sage.

Bertaux, D. (1981). 'From the life-history approach to the transformation of sociological practice'. In D. Bertaux (Ed.), *Biography and society: The life history approach in the social sciences*, pp. 29-45. London: Sage.

Bowen, G. A. (2008). 'Naturalistic Inquiry and the saturation concept: A research note'. *Qualitative Research*, 8, pp. 137-152.

Brinkmann, S. and Kvale, S. (2015). *InterViews*, (3rd ed.). London: Sage.

Bryman, A. (2012). 'Expert voice'. In S. E. Baker and R. Edwards, *How many qualitative interviews are enough?* National Centre for Research Methods Review Discussion Paper, pp. 18-20. Retrieved from <http://eprints.ncrm.ac.uk/2273/> [Accessed 9 May 2015].

Chen, S. and Wilson, M. (2003). 'Standardisation and localisation of Human Resource Management in Sino-foreign joint ventures'. *Asia Pacific Journal of Management*, 20, pp. 397-408.

Collins, A. M., Cartwright, S. and Hislop, D. (2013). 'Homeworking: negotiating the psychological contract'. *Human Resource Management Journal*, 23, pp. 211-225.

Collins, K. M. T., Onwuegbuzie, A. J., and Jiao, Q. G. (2006). 'Prevalence of mixed methods sampling designs in social science research'. *Evaluation and Research in Education*, 19, pp. 83-101.

Collins, K. M. T., Onwuegbuzie, A. J., and Jiao, Q. G. (2007). 'A mixed methods investigation of mixed methods sampling designs in social and health science research'. *Journal of Mixed Methods Research*, 1, pp. 267-294.

Creed, W. E. D. (2003). 'Voice lessons: Tempered radicalism and the use of voice and silence'. *Journal of Management Studies*, 40, pp. 1503-1536.

Creswell, J. (2007). *Qualitative inquiry and research design: Choosing among five approaches*. (2nd ed.). Thousand Oaks, CA: Sage.

Crouch, M. and McKenzie, H. (2006). 'The logic of small samples in interview-based qualitative research'. *Social Science Information*, 45, pp. 483-499.

Curtis, S., Gesler, W., Smith, G., and Washburn, S. (2000). 'Approaches to sampling and case selection in qualitative research: Examples in the geography of health'. *Social Science and Medicine*, 50, pp. 1001-1014.

Daudigeos, T. (2013). 'In their profession's service: How staff professionals exert influence in their organisation'. *Journal of Management Studies*, 50, pp. 722-749.

Easterby-Smith, M., Golden-Biddle, K. and Locke, K. (2008). 'Working with pluralism: determining quality in qualitative research'. *Organizational Research Methods*, 11, pp. 419-429.

Easterby-Smith, M., Li, S. and Bartunek, J. (2009). 'Research methods for organizational learning: the transatlantic gap'. *Management Learning*, 40, pp. 439-447.

Eby, L. T., Hurst, C. S. and Butts, M. M. (2009). 'Qualitative research: the redheaded stepchild in organisational and social science research'. In C. E. Lance and R. J. Vandenberg (Eds.), *Statistical and methodological myths and urban legends: Doctrine, verity and fable in organisational and social sciences*, pp. 219-246, New York: Routledge.

Francis, J. J., Johnston, M., Robertson, C., Glidewell, L., Entwistle, V., Eccles, M. and Grimshaw, M. (2010). 'What is an adequate sample size? Operationalizing data saturation for theory-based studies'. *Psychology and Health*, 25, pp. 1229-1245.

Glaser, B. and Strauss, A. (1967). *The discovery of grounded theory*. Chicago, IL: Aldine.

Grey, C. (2010). 'Organizing studies: publication, politics and polemic'. *Organization Studies*, 31, pp. 677-694.

Guba, E. and Lincoln, Y. (1994). 'Competing paradigms in qualitative research' In N. K. Denzin and Y. S. Lincoln (Eds.), *Handbook of qualitative research* pp. 105-117, Thousand Oaks, CA: Sage.

Guest, G., Bunce, A. and Johnson, L. (2006). 'How many interviews are enough?' *Field Methods*, 18, pp. 59-82.

Hall, M., Hoffmann, A., Marginson, P. and Muller, T. (2003). 'National influences on European Works Councils in UK- and US-based companies'. *Human Resource Management Journal*, 13, pp. 75-92.

Harvey, C., Kelly, A., Morris, H. and Rowlinson, M. (2010). *Academic Journal Quality Guide Version 4*. London: Association of Business Schools.

Janesick, V. J. (2000). 'The choreography of qualitative research design'. In N. K. Denzin and Y. S. Lincoln (Eds.), *Handbook of qualitative research* (2nd ed.), pp. 379-399, Thousand Oaks, CA: Sage.

Johnson, P., Buehring, A., Cassell, C. and Symon, G. (2006). 'Evaluating qualitative management research: towards a contingent criteriology'. *International Journal of Management Reviews*, 8, pp. 131-156.

Kaufman, B. E. (2003). 'Keeping the commitment model in the air during turbulent times: Employee involvement at Delta Airlines'. *Industrial Relations* 52, pp. 343-377.

Kuzel, A. (1992). 'Sampling in qualitative inquiry'. In B. Crabtree and W. Miller (Eds.), *Doing qualitative research*, pp. 31-44. Newbury Park, CA: Sage.

Lauring, J. (2013). 'International diversity management: Global ideals and local responses'. *British Journal of Management*, 24, pp. 211-224.

Leitch, C. M., McMillan, C. and Harrison, R. T. (2013). 'The development of entrepreneurial leadership: the role of human, social and institutional capital'. *British Journal of Management*, 24, pp. 347-366.

Lincoln, Y. S. and Guba, E. C (1985). *Naturalistic Inquiry*. Beverly Hills, CA: Sage.

Lincoln, Y. S., Lynham, S. A. and Guba, E. C. (2011). 'Paradigmatic controversies, contradictions and emerging confluences revisited'. In N. K. Denzin and Y. S. Lincoln (Eds.), *The Sage handbook of qualitative research* (4th ed.), pp. 97-128 Thousand Oaks, CA: Sage.

MacQueen, K. M., McLellan, E., Kay, K. and Milstein, B. (1998). 'Codebook development for qualitative analysis'. *Cultural Anthropology Methods*, 10, pp. 31-36.

Marshall, M. N. (1996). 'Sampling for qualitative research'. *Family Practice*, 13, pp. 522-525.

Marshall, B., Cardon, P., Poddar, A. and Fontenot, R. (2013). 'Does sample size matter in qualitative research: a review of qualitative interviews in IS research'. *Journal of Computer Information Systems*, 54, pp. 11-22.

Matlis, S. and Lawrence, T. B. (2003). 'Orchestral manoeuvres in the dark: Understanding failure in organizational strategizing'. *Journal of Management Studies*, 40, pp. 109-139.

McDonald, P., Townsend, K. and Waterhouse, J. (2009). 'Wrong way, go back. Negotiating access in industry based research'. In K. Townsend and J. Burgess (Eds.), *Method in the Madness: Research Stories you Won't Find in a Textbook*, pp. 119-134. Oxford: Chandos Publishing.

Meyrick, J. (2006). 'What is good qualitative research? A first step towards a comprehensive approach to finding rigour/quality'. *Journal of Health Psychology*, 11, pp. 799-808.

Miles, M. B., Huberman, A. M. and Saldaña, J. (2013). *Qualitative data analysis: A methods sourcebook* (3rd ed.). Thousand Oaks, CA: Sage.

Mingers, J., and Harzing, A. W. (2007). 'Ranking journals in Business and Management: a statistical analysis of the Harzing data set'. *European Journal of Information Systems*, 16, pp. 303-316.

Morse, J. (1994). 'Designing funded qualitative research'. In N. Denzin and Y. Lincoln (Eds.). *Handbook for qualitative research*, pp. 220-235, Thousand Oaks, CA: Sage.

Mustafa, M. and Gold, M. (2013). 'Chained to work? Strategies to manage temporal and physical boundaries among self-employed teleworkers'. *Human Resource Management Journal*, 23, pp. 413-429.

Onwuegbuzie, A. J. and Leech, N. L. (2005). 'The role of sampling in qualitative research'. *Academic Exchange*, 9, pp. 280-284.

Onwuegbuzie, A. J. and Leech, N. L. (2007). 'A call for qualitative power analyses'. *Quality and Quantity*, 41, pp. 105-121.

O'Reilly, M. and Parker, N. (2013). 'Unsatisfactory saturation: a critical exploration of the notion of the notion of saturated sample sizes in qualitative research'. *Qualitative Research*, 13, pp. 190-197.

Patriotta, G. (2003). 'Sensemaking on the shop floor: Narratives of knowledge in organizations'. *Journal of Management Studies*, 40, pp. 349-375.

Patton, M. Q. (2015). *Qualitative research and evaluation methods* (4th ed.). Thousand Oaks, CA: Sage.

Regnér, P. (2003). 'Strategy creation in the periphery: Inductive versus deductive strategy making'. *Journal of Management Studies*, 40, pp. 57-82.

Robinson, O. C. (2014). 'Sampling in interview based qualitative research: a theoretical and practical guide'. *Qualitative Research in Psychology*, 11, pp. 25-41.

Safman, R. M. and Sobal, J. (2004). 'Qualitative sample extensiveness in health education research'. *Health Education and Behavior*, 31, pp. 9-21.

Saunders, M. N. K. (2012). 'Choosing research participants'. In G. Symon and C. Cassell (Eds.), *Qualitative organizational research: core methods and current challenges*, pp. 35-52, London: Sage.

Saunders, M., Lewis, P. and Thornhill, A. (2016). Research methods for business students (7th ed.). Harlow: Pearson.

Symon, G. and Cassell, C. (1999) 'Barriers to innovation in research practice'. In M. Cunha and C. Marques (Eds.), *Readings in organizational science: Organizational change in a changing context* pp. 387-398, Lisbon: ISPA.

Thompson, P., Newsome, K. and Commander, J. (2013). 'Good when they want to be': Migrant workers in the supermarket supply chain. *Human Resource Management Journal*, 23, pp. 129-143.

Willmott, H. (2011). 'Journal list fetishism and the perversion of scholarship: Reactivity and the ABS list'. *Organization*, 18, pp. 429-442.