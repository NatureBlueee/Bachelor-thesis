

![Elsevier logo](935eed7aa61f7777f62cfc032e11bee9_img.jpg)

Elsevier logo

ELSEVIER

Contents lists available at ScienceDirect

## Computers in Human Behavior

journal homepage: [www.elsevier.com/locate/comphumbeh](http://www.elsevier.com/locate/comphumbeh)![Cover image of Computers in Human Behavior journal](0538daaa5583c23e17db3a12f2281a55_img.jpg)

Cover image of Computers in Human Behavior journal

![Check for updates icon](4f4b52340aaccb1bcf733468dca9ee03_img.jpg)

Check for updates icon

# Allying with AI? Reactions toward human-based, AI/ML-based, and augmented hiring processes

Manuel F. Gonzalez<sup>a,\*,1</sup>, Weiwei Liu<sup>b,c</sup>, Lei Shirase<sup>d</sup>, David L. Tomczak<sup>b,e</sup>, Carmen E. Lobbe<sup>b</sup>, Richard Justenhoven<sup>b,f</sup>, Nicholas R. Martin<sup>b</sup><sup>a</sup> Department of Education Leadership, Management and Policy, Seton Hall University, 419 Jubilee Hall, 400 South Orange Ave., South Orange, NJ, 07079, USA<sup>b</sup> Aon Assessment Solutions, One Liberty Street, New York, NY, 10006, USA<sup>c</sup> Department of Psychology, 150 W. University Blvd., Melbourne, FL, 32901, USA<sup>d</sup> Takeda Pharmaceuticals, 1200 Lakeside Dr., Bannockburn, IL, 60015, USA<sup>e</sup> Department of Organizational Sciences and Communication, The George Washington University, 600 21st St. NW, Washington, DC, 20052, USA<sup>f</sup> Department of Education and Psychology, Freie Universität Berlin, Habelschwerdter Allee 45, 14195, Berlin, Germany

## ARTICLE INFO

### Keywords:

Applicant reactions

Artificial intelligence

Machine learning

Employee selection

Augmented approach

Familiarity

## A B S T R A C T

While many organizations' hiring practices now incorporate artificial intelligence (AI) and machine learning (ML), research suggests that job applicants may react negatively toward AI/ML-based selection practices. In the current research, we thus examined how organizations might mitigate adverse reactions toward AI/ML-based selection processes. In two between-subjects experiments, we recruited online samples of participants (undergraduate students and Prolific panelists, respectively) and presented them with vignettes representing various selection systems and measured participants' reactions to them. In Study 1, we manipulated (a) whether the system was managed by a human decision-maker, by AI/ML, or a combination of both (an "augmented" approach), and (b) the selection stage (screening, final stage). Results indicated that participants generally reacted more favorably toward augmented and human-based approaches, relative to AI/ML-based approaches, and further depended on participants' pre-existing familiarity levels with AI. In Study 2, we sought to replicate our findings within a specific process (selecting hotel managers) and application method (handling interview recordings). We found again that reactions toward the augmented approach generally depended on participants' familiarity levels with AI. Our findings have implications for how (and for whom) organizations should implement AI/ML-based practices.

## 1. Introduction

Many organizations have integrated artificial intelligence and machine learning into their hiring processes (Equal Employment Opportunity Commission, 2016; Stephan et al., 2017). Artificial intelligence (AI) is a discipline focused on simulating human intelligence in technology (Nilsson, 2014), and includes machine learning (ML), a subdiscipline

focusing on developing algorithmic models that learn and improve over time (Alpaydin, 2020). In hiring and recruitment, AI/ML can fill functions such as designing job descriptions, marketing vacancies to potential candidates, providing technical support through chat bots, and streamlining selection decisions (Albert, 2019; Black & van Esch, 2020). Regarding the latter, organizations can use AI/ML to gather, synthesize, and analyze large volumes of applicant data (e.g., Das et al., 2018), thus

\* Corresponding author.

E-mail addresses: [manuel.gonzalez@shu.edu](mailto:manuel.gonzalez@shu.edu) (M.F. Gonzalez), [weiwei.liu@aon.com](mailto:weiwei.liu@aon.com) (W. Liu), [Lei.shirase@takeda.com](mailto:lei.shirase@takeda.com) (L. Shirase), [david.tomczak@aon.com](mailto:david.tomczak@aon.com) (D.L. Tomczak), [lobbe.carmen@gmail.com](mailto:lobbe.carmen@gmail.com) (C.E. Lobbe), [richard.justenhoven@aon.com](mailto:richard.justenhoven@aon.com) (R. Justenhoven), [nicholas.martin@aon.com](mailto:nicholas.martin@aon.com) (N.R. Martin).

<sup>1</sup> Present Address: Department of Education Leadership, Management and Policy, Seton Hall University, 419 Jubilee Hall, 400 South Orange Ave., South Orange, NJ, 07079, USA.

allowing for semi- or fully-automated selection processes such as interviews (Langer et al., 2019), resume screening (Sajjadiani et al., 2019), and decision-making (Lawrence, 1991).

In the current research, we focus on the use of AI/ML for facilitating pre-employment selection decisions.<sup>2</sup> While AI/ML is increasingly common in this area, there are several potential challenges surrounding its use. We focus on one particular challenge supported by a growing body of research: people typically react negatively toward these AI/ML-based processes, compared to when the same processes are managed by a human (e.g., Gonzalez et al., 2019; Langer et al., 2019; Mirowska, 2020). Such negative reactions – particularly from job applicants – could undermine the potential benefits of AI/ML.

Despite growing evidence that job applicants react adversely to AI/ML-based selection processes, there is far less research on how to improve these reactions. Researchers often consider other areas such as identifying and removing bias from algorithms (e.g., Lee, 2018; Sun et al., 2019), or improving reactions from stakeholders who use the algorithm (for a review, see Langer et al., 2021). These other research areas have important implications for effectively developing and implementing AI/ML. However, without the acceptance of job applicants, even objectively fair AI/ML-based hiring systems may lose their utility. It is thus important to identify how to implement AI/ML-based selection processes without undermining candidate experiences.

In the current research, we explored what factors influence applicant reactions toward AI/ML. To do so, we conducted two experiments. In the first experiment, we examined whether reactions toward AI/ML are influenced by (a) whether AI/ML supplements human decision-makers (an “augmented” selection approach), rather than *replacing* them (a fully-automated approach),<sup>3</sup> and (b) the selection stage in which the approach is used (screening versus final stage). In the second experiment, we again examined the augmented approach, this time examining a specific application of AI/ML within a specific industry. In both experiments, we further examined applicants’ familiarity with AI as a moderating individual difference factor.

Through our research, we offer several contributions. First, we contribute to research on applicant reactions and related theories such as organizational justice theory (Colquitt et al., 2005), self-determination theory (Gagné & Deci, 2005), and signaling theory (Bangert et al., 2012) by examining how aspects of technology-based hiring processes influence applicant reactions, and the possible boundary conditions of these influences. Second, we contribute to the explainable artificial intelligence (XAI) literature. Researchers, practitioners, and the broader public have scrutinized the transparency and interpretability of AI/ML-based systems (e.g., Adadi & Berrada, 2018; Langer et al., 2021). XAI scholars seek to enhance the fairness and acceptance of these systems by developing mechanisms for transparency. We contribute to the XAI literature by examining how people react to one such factor that may influence transparency: the presence of human decision-makers, who can interpret and explain predictions from AI/ML (Langer et al., 2021).

Lastly, we contribute to the human-automation interaction literature. Advanced technologies have expanded to activities previously

done by managers (e.g., project management systems, Auth et al., 2019), team members (e.g., autopilotting systems, Hoeff et al., 2006; see also, Seeber et al., 2020), and even customers (e.g., automated personal assistants, Olsen & Malizia, 2011), leading researchers to explore what factors shape technology interactions at work (e.g., Landers & Marin, 2021). We add to these efforts by examining how job applicants react to automated hiring processes, and what factors shape these reactions. Furthermore, by examining augmented hiring processes, we contribute insights into how people perceive and react to human-computer interactions at work (i.e., between hiring professionals and AI/ML).

## 2. Theoretical background and hypotheses

### 2.1. Artificial intelligence and machine learning in employee selection

AI/ML can facilitate selection decisions in various ways. Organizations can use AI/ML to analyze and synthesize information from complex datasets that would otherwise be difficult to manage, such as those including data from many job candidates, containing many variables, and/or continuously receiving new data points (e.g., Oswald et al., 2020; Sinar, 2015). One can also use AI/ML to analyze data that were previously cumbersome to subject to quantitative analyses, such as open-ended texts, audio, and video. For example, natural language processing (NLP) can be used to extract variables from open-ended data, such as qualifications from résumés (Indira & Kumar, 2016) or personality profiles from social media data (Alexander et al., 2020; Park et al., 2015), which one can then use to predict key outcomes (e.g., turnover; Sajjadiani et al., 2019). Furthermore, AI/ML algorithms offer more flexibility than traditional analytical approaches (e.g., linear regression, analysis of variance) because they can identify complex, nonlinear relationships to explain greater variance in outcomes (e.g., Putka et al., 2018). AI/ML can thus facilitate or entirely automate selection decisions by optimizing how to weigh and combine variables. In sum, AI/ML offers increased efficiency – e.g., saved time, money, and effort – and (potentially) greater predictive capabilities. Many organizations have thus integrated AI/ML into their talent assessment processes (Stephan et al., 2017).

However, AI/ML-based processes can also involve several practical and ethical challenges. First, even rigorously developed algorithms can have “black-box” issues, in which it is unclear whether, how, or why specific variables contributed to a given prediction (e.g., Adadi & Berrada, 2018). Because AI/ML algorithms often form predictions in complex ways, these predictions can become challenging for end-users – such as hiring managers, applicants, or, in a legal challenge, a judge – to fully comprehend. Second, and relatedly, while there are new approaches for enhancing the transparency of AI/ML algorithms (for a recent discussion, see Langer et al., 2021), they can limit the algorithm’s flexibility (i.e., complex predictions are harder to explain; Adadi & Berrada, 2018; Breiman, 2001).

Third, if not carefully developed, AI/ML can potentially perpetuate (or even magnify) bias in selection procedures. AI/ML-based selection tools are often marketed as reducing bias by removing the need for human decision-makers. However, bias can still exist in the data used to train AI/ML algorithms (Ntoutsi et al., 2019). For example, imagine an organization where supervisors tend to evaluate male employees more favorably than female employees. If that organization trained an AI/ML algorithm to predict job performance, operationalized as supervisory ratings, the algorithm may associate employee sex (or predictors that correlate with sex) with “successful” performance. By using this algorithm for hiring, the organization may potentially introduce systematic bias against female applicants. Similar concerns exist for factors such as ethnicity (e.g., Ntoutsi et al., 2019) and physical/mental health (e.g., Weintraub, 2017).

Addressing the above challenges is critical for developing ethical and fair AI/ML systems. Furthermore, as with any selection approach, organizations must provide evidence for the validity of AI/ML tools before

<sup>2</sup> We acknowledge that organizations can use AI/ML in other, relatively benign ways such as customizing user interfaces to individual applicants or automating technical support delivery (e.g., through chat bots). While job applicants may react toward each of these applications of AI/ML in idiosyncratic ways, we chose to focus particularly on the application of AI/ML to selection decision-making. We believe that applicants’ reactions toward AI/ML will be magnified in this context because of the potential for AI/ML to influence high-stakes outcomes. We nevertheless encourage researchers to examine applicant reactions toward these other applications of AI/ML.

<sup>3</sup> The augmented approach has also been referred to elsewhere as “collaborative intelligence” (e.g., Epstein, 2015; Goldberg, 2019; Paschen et al., 2020; Wilson & Daugherty, 2018).

using them in high-stakes selection contexts EEOC (1978); SIOP (2018). Yet, even if such algorithms are properly validated and ethically designed, a fourth challenge remains: job applicants tend to react unfavorably toward AI/ML.

### 2.2. Applicant reactions toward artificial intelligence and machine learning

People have historically distrusted automated, intelligent decision-making systems such as AI/ML (Armer, 1995); organizational contexts are no exception (for a review, see Glikson & Woolley, 2020). Applicants can learn in several ways that an organization is using AI/ML in its hiring process. For example, organizations may ask applicants to complete assessments that are commonly known to utilize AI/ML (e.g., asynchronous video interviews, CV scraping platforms; Laurim et al., 2021), or organizations may be legally required to disclose their use of AI/ML for collecting or analyzing certain types of data (e.g., General Data Protection Regulation, 2016).

Increasing evidence suggests that job applicants tend to react unfavorably toward automated processes, such as those using AI/ML. As examples, highly-automated hiring processes can lead people to perceive less control and fairness, greater ambiguity and privacy concerns, and less acceptance of the process, relative to human-led processes (e.g., Gonzalez et al., 2019; Langer et al., 2019, 2020). The use of AI/ML thus appears to influence how applicants experience the hiring process. We draw from organizational justice theories (Colquitt et al., 2005), self-determination theory (Deci & Ryan, 2017), and signaling theory (Bangerter et al., 2012; Spence, 1973) to discuss why the use of AI/ML could influence specific types of applicant reactions, and will then hypothesize about factors that may further shape these reactions.

According to organizational justice theories, job applicants navigate hiring processes with various fairness criteria in mind (Gilliland, 1993), such as those pertaining to fair procedures (i.e., procedural justice; Leventhal, 1980; Thibaut & Walker, 1975) and interpersonal treatment (interactional justice; Bies, 2015). These criteria serve as heuristics for applicants (Lind, 2001), by which to judge the subsequent fairness of their outcomes (Brockner & Wiesenfeld, 1996) and whether the organization values them (Lind & Tyler, 1988). We next focus on procedural justice, and will later return to discuss interactional justice.

Job applicants may perceive AI/ML-based selection processes as violating several criteria for *procedural justice*. Applicants may perceive AI/ML-based decisions as inaccurate, simply due to unfamiliarity with how AI/ML works (Johnson & Verdichio, 2017). Often, applicants will have limited information about how AI/ML is being used in the process (e.g., which computational approach or data points are being used) or may not comprehend such information when it is available (for an analogous discussion, see Xu, 2019). In such cases, applicants may rely on heuristics (i.e., mental shortcuts) to evaluate the fairness of AI/ML, such as by drawing from negative examples of AI in the media (Garvey, 2019). Likewise, applicants may perceive AI/ML-based systems as less transparent and less fair due to a lack of familiarity with them (Johnson & Verdichio, 2017), relative to human-led systems, which people are typically more familiar with. Furthermore, by automating the process, applicants may perceive fewer opportunities for process control, such as appealing decisions or voicing their opinions (Leventhal, 1980). For example, using asynchronous video interviews with a fixed set of questions and a restricted timeframe does not allow participants to clarify questions or provide feedback (e.g., Basch & Melchers, 2019). For all these reasons, applicants may perceive automated systems as less procedurally just than human-led systems. This is potentially ironic; AI/ML operate on a standardized set of variables, and such standardization typically yields more valid hiring decisions than more subjective approaches that human decision-makers may sometimes use (Highhouse, 1997; Kuncel et al., 2013).

Drawing from self-determination theory (SDT; Deci & Ryan, 2012), AI/ML-based processes may demotivate applicants by undermining

fundamental needs for relatedness (i.e., human interaction and connection), competence (i.e., feeling capable, demonstrating one's abilities) and autonomy (i.e., exercising free will and choice). By automating the hiring process, organizations may deprive applicants from interacting with organizational members, thus impeding applicants' needs for *relatedness* and, in turn, demotivating them. According to organizational justice theories, such depersonalization may also foster concerns about whether the organization adhered to norms of *interactional justice* (Bies & Moag, 1986), which pertain to respectful treatment. Indeed, Gonzalez et al. (2019) found that people were more concerned about interpersonal factors relating to AI/ML-based selection processes than procedural factors.

Automated hiring processes may also hinder *autonomy* and *competence* needs. As mentioned earlier, applicants may perceive automated systems as depriving them of voice or appeal opportunities (see Schlicker et al., 2021, for analogous findings outside of the hiring context), which are forms of process control (Tyler et al., 1985). Furthermore, AI/ML-based hiring procedures often use a standardized set of information to make decisions, whereas human decision-makers can account for additional applicant information (Kuncel et al., 2013) and can be influenced by applicants through impression management (Gilmore & Ferris, 1989). Applicants may thus feel that they have less autonomy and fewer opportunities to show their competence in automated systems, relative to human-led systems. For similar reasons, applicants may perceive that they have less *control* over their outcomes in automated systems.

Lastly, job applicants may form impressions of the organization's culture and values based on the extent to which hiring processes use AI/ML. According to signaling theory (Bangerter et al., 2012; Goldberg & Allen, 2008; Spence, 1973), selection practices can (intentionally or unintentionally) signal to applicants what the organization does and does not value. On the one hand, by leveraging advanced technologies such as AI/ML, organizations may signal that they value *innovation*. Analogously, recent findings by Folger et al. (2021) indicate that job applicants perceive digital selection methods (i.e., those that more strongly leverage digital communication technologies) as innovative, which in turn predicted applicants' attraction to the organization. Applicants may thus perceive organizations using AI/ML as innovative.

On the other hand, applicants may perceive organizations that use AI/ML as outsourcing their selection processes to technology, rather than investing human time and effort in it. Indeed, Stone et al. (2015) noted that cost-cutting and efficiency often become the primary goals of organizations that use AI/ML for hiring purposes, rather than increasing the effectiveness of the candidates that are attracted and retained. It is possible that such values of efficiency over human capital may permeate through AI/ML-based hiring processes and be detected by applicants (for similar arguments, see Acikgoz et al., 2020; Folger et al., 2021). Applicants may thus infer that the organization does not *value its human capital* (during the hiring process or in general) and may subsequently react unfavorably toward the organization.

### 2.3. Implications of applicant reactions toward AI/ML

Applicant reactions can have important implications for organizations that use AI/ML-based selection processes. In general, applicant reactions can spillover to hiring outcomes (for a review, see Truxillo & Bauer, 2011), such as how applicants evaluate the organization (Bauer et al., 1998), their attraction to the organization and intentions to continue pursuing the position (Smither et al., 1993), and their intentions to accept a job offer from the organization or recommend the organization to others (Hausknecht et al., 2004). By evoking unfavorable applicant reactions, AI/ML-based hiring processes may thus negatively affect important hiring-related outcomes. Indeed, research suggests that people are less willing to apply to and pursue jobs when their data will be reviewed by an AI-based evaluator, relative to a human (Mirowska, 2020; Wesche & Sonderregger, 2021). Furthermore, adverse

reactions toward AI/ML may leave a “bad taste” for *hired* applicants by affecting their behavioral intentions after the process (Gonzalez et al., 2019). By influencing the outcomes above, unfavorable applicant reactions may undermine the potential efficiency and validity gains promised by AI/ML. It is thus important to understand how organizations can mitigate adverse applicant reactions toward AI/ML.

We examined one such strategy in the current research. Specifically, we examined the use of augmented selection processes, in which AI/ML *augments* (rather than *replaces*) human decision-making.<sup>4</sup> However, while we examine whether this strategy influences applicants’ acceptance of AI/ML, we note that organizations must also *earn* this acceptance by using AI/ML solutions that adhere to legal and ethical fairness standards.

### 2.4. Improving reactions to AI/ML: augmented selection processes

Selection processes can involve *both* human decision-makers and AI/ML. Specifically, automation can exist along a continuum (Parasuraman et al., 2000). At lower levels of automation, human decision-makers (e.g., recruiters, hiring managers) primarily lead and manage the hiring process, whereas at higher levels of automation, the process is entirely (or almost entirely) automated (e.g., using automated recruitment systems, communications, assessments, and decision-making approaches). We note, however, that even highly automated processes have at least some degree of human involvement, such as in developing the algorithm, deciding which data to feed into it, or supervising the learning of the algorithm. In the current research, we examine augmented selection processes, which fall between these two extremes.

We conceptualize *augmented selection processes* as those in which AI/ML is used to enhance human decision-making about job applicants – such as who should advance in the hiring process or receive a job offer – as well as enhance the activities of human decision-makers during the process – such as data collection and analysis. For example, an organization could use AI/ML to generate an assessment profile for each job candidate and provide initial hiring recommendations, which a human decision-maker can review and consider when arriving at a final decision about the candidates. As another example, organizations could use AI/ML to synthesize the ratings of multiple human decision-makers (e.g., an interview panel, assessment center raters). To our knowledge, there is limited empirical work on this topic, although researchers have described such approaches as a newer form of *collective intelligence* (e.g., Malone & Bernstein, 2015; Verhulst, 2018) – an emergent form of intelligence, coordinated group behavior that generalizes to a variety of tasks (Woolley et al., 2010).<sup>5</sup>

Augmented approaches may have several organizational benefits. They can allow for a balance between (a) the intuition and creativity of human decision-makers, and (b) the analytical decision-making and capacity to handle complex data typically offered by AI/ML (Jarrahi,

2018). Furthermore, organizational decision-making (selection or otherwise) often involves uncertainty from limited information and many possible response options (Milliken, 1987), which AI/ML can help to reduce such as by narrowing down the number of suitable options to consider (Jarrahi, 2018) or identifying the most critical criteria for matching a candidate to a given role (Koivunen et al., 2019). Lastly, in augmented approaches, human decision-makers and AI/ML could serve as “checks” on one another. For example, human decision-makers can monitor algorithmic recommendations for bias or inaccuracy, whereas algorithms can flag human-based hiring decisions that do not align with selection criteria (Koivunen et al., 2019).

We examined another possible benefit of augmented selection processes: that they might alleviate unfavorable applicant reactions, relative to strictly AI/ML-based processes. For several reasons, we expected that people would react more favorably toward augmented processes than AI/ML-based processes. First, human decision-makers can take in a larger array of information than AI/ML, which operates on a fixed set of variables. Job candidates may perceive the former as a “safeguard” against the latter because a human decision-maker can detect additional, idiosyncratic information that AI/ML may not be programmed to consider. Job candidates may thus perceive the selection process as more thorough and hence procedurally fairer (Leventhal, 1980). Second, by including a human decision-maker, AI/ML-based processes may feel less depersonalizing to applicants and better satisfy their fundamental needs for relatedness and social contact (Deci and Ryan, 2017; Gagné & Deci, 2005). In doing so, augmented processes may also convey respect toward applicants, thus enhancing interactional justice (Bies & Moag, 1986).

Third, job applicants may experience greater satisfaction of autonomy and competence needs and may perceive more control in augmented processes, rather than solely AI/ML-based processes. Specifically, the presence of a human decision-maker can enable applicants to communicate more information about themselves, engage in impression management (i.e., try to influence the decision-maker; Gilmore & Ferris, 1989), and ask questions *in situ*. Lastly, organizations that use augmented processes may simultaneously signal to applicants that they value innovation – through using advanced technologies like AI/ML – and that they care about their human capital and prospective employees – through investing human time and effort in the process – whereas AI/ML-based and human-based processes may only signal one of these values, respectively. Applicants may thus form more favorable perceptions of the organization’s culture during augmented processes.

**Hypothesis 1.** Augmented and human-based selection processes will generally lead to more favorable reactions than AI/ML-based selection processes (H1a), with the exception that augmented and AI/ML-based processes will lead to greater perceived innovation than human-based processes (H1b).

### 2.5. Improving applicant reactions: does selection stage matter?

We examined whether reactions to augmented, AI/ML-based, and human-based selection processes depend on the stage of the selection process. Organizations may not uniformly automate every selection stage. For example, they may rely more heavily on automation early on, where there is a larger volume of job applicants (Liem et al., 2018). Once the applicant pool is narrowed down, organizations may then invest more human resources (i.e., rely less heavily on automation) to assess applicants before making final selection decisions. We thus examined how automation affects applicant reactions at earlier versus later stages of the selection process.

We propose that the selection approach (augmented, AI/ML-based, or human-based) should more strongly affect applicant reactions at later stages of the process. According to referent cognitions theory (Cropanzano & Folger, 1989; Folger, 1987) and relative deprivation theory (Crosby, 1976), one’s proximity to a desirable outcome (e.g.,

<sup>4</sup> Prior to the studies reported in this manuscript, we conducted a preliminary, within-subjects experiment in which we manipulated (a) whether an augmented approach was used for making selection decisions (relative to a solely human-based or AI/ML-based approach), and, (b) if AI/ML was used, whether participants believed that it was developed using a theory-driven approach or a data-driven approach. Findings from this preliminary study provided some initial evidence that an augmented approach – especially if theory-driven rather than data-driven – could mitigate adverse reactions toward AI/ML. We omit this preliminary study here due to manuscript length and methodological concerns raised during the peer review process, but we wish to make readers aware of the study for the purpose of transparency and because its findings led us to focus on the augmented approach in Studies 1 and 2. Readers interested in learning more about these research findings should contact the corresponding author.

<sup>5</sup> We selected the terminology of “augmented” hiring processes, rather than “collective intelligence,” because collective intelligence refers to a conceptually broader phenomenon that can occur with or without AI/ML.

receiving a job offer) influences their sensitivity to fairness-related information. Specifically, the (un)fairness of a procedure more strongly influences people who are closer to (rather than farther from) a desirable outcome. At closer proximity, perceived unfairness triggers referent cognitions, that is, cognitive simulations of more favorable (in this case, fairer) alternatives that one could have experienced (Kahneman & Tversky, 1982). Furthermore, this process can occur even before the outcome has been allocated (van den Bos & van Prooijen, 2001).

We posit that applicants in later stages of the selection process have closer proximity to the desirable outcome (i.e., the potential job offer) than applicants in earlier stages. Relative to applicants in earlier stages, we thus suggest that applicants at later stages should be more sensitive to the fairness of the procedure, may generate more referent cognitions about fairer hiring procedures that may have been experienced in the past, and ultimately react more strongly to the degree of automation in the process.

**Hypothesis 2.** The effects of the selection process (augmented, human-based, AI/ML-based) on applicant reactions will be more pronounced at later stages of the selection process, relative to earlier stages of the selection process.

### 2.6. How familiar are applicants with AI?

We also evaluated whether familiarity with AI influences how applicants react to human-based, AI/ML-based, and augmented approaches. We established earlier that distrust toward AI may partly stem from a general lack of understanding from the broader public regarding how AI/ML operates (Johnson & Verdicchio, 2017), which contributes to perceptions of AI as opaque. This sense of opacity may lead applicants to perceive AI/ML-based processes as unfair, and thus adversely react to it. Conversely, applicants who have greater familiarity with AI may feel less uncertainty about the process, because they have fewer misconceptions about AI.

People who are less familiar with AI/ML may also make potentially erroneous inferences about it, leading to adverse reactions within AI/ML-based systems. People often underestimate the degree of human involvement in designing and using AI (Johnson & Verdicchio, 2017). In other words, while AI can operate autonomously, people can mistakenly interpret this autonomy to mean that humans cannot control AI. People even anthropomorphize AI, such as by making attributions about its agency or morality (Shank & DeSanti, 2018). The tendency to make these inferences may stem from sensationalized coverage about AI/ML from entertainment and news media (e.g., as an agentic force that could overthrow or control humanity, or as a perpetrator of bias or prejudice; Garvey, 2019). People who are less familiar with AI/ML may use these media portrayals as heuristics by which to judge AI/ML and the entities that use them (Garvey, 2019) – in our research, the hiring organization. Conversely, those who are more familiar with AI/ML may be less prone to making such erroneous assumptions.

**Hypothesis 3.** The effects of the selection process (augmented, human-based, AI/ML-based) on applicant reactions will be more pronounced among people who have lower levels of familiarity with AI/ML, relative to people who have higher levels of familiarity with AI/ML.

## 3. Study 1

### 3.1. Study 1 method

In Study 1, we used a 3 (selection approach: human-based, AI/ML-based, augmented) x 2 (selection stage: screening, final) between-subjects experimental design to test **Hypothesis 1** – regarding the different selection approaches – and **Hypotheses 2** and **3** – concerning the moderating effects of the selection stage and applicants’ familiarity with AI/ML. All study materials are provided as a supplement to this manuscript.

#### 3.1.1. Participants

An *a priori* power analysis for a two-way ANOVA indicated that a sample size of 158 participants was needed to detect a medium-sized effect ( $f = 0.25$ ,  $\alpha = 0.05$ , power = .80, numerator  $df = 2$ ,  $k = 6$ ; Paul et al., 2007). We raised this number to 180, or approximately 30 participants per experimental condition. We recruited 264 undergraduate psychology and management students from a medium-sized university in the northeastern United States, who participated for course credit. All participants had previous or current work experience. We excluded 80 participants who failed at least two out of three attention and manipulation check questions (described below), and we excluded one participant for indicating at the end of the study that they did not have work experience, yielding a final sample of 183 participants (46.5% male; 42.6% Asian, 22.4% White, 21.9% Hispanic/Latinx, 6.0% Black/African American;  $M_{age} = 20.77$ ,  $sd = 4.14$ ;  $M_{work experience} = 3.44$  years,  $sd = 3.82$ ).

#### 3.1.2. Procedure

Participants read that we were examining attitudinal and behavioral reactions to workplace events, and that they would read and answer questions about a short, multi-part scenario describing such an event. In the scenario, participants imagined applying for a mid-level job at a multinational company, and that the current selection stage would involve completing a job knowledge test and an asynchronous video interview.<sup>6</sup> We manipulated the selection approach and selection stage in a randomized order. We manipulated the *selection approach* by stating that either (a) a hiring manager (human-based), (b) the company’s AI algorithms (AI/ML-based), or (c) a hiring manager working in combination with the company’s AI algorithms (augmented) would score and analyze the assessments. We manipulated the *selection stage* by stating that the assessment data would help the company to either (a) make an initial screening decision, at which point they would either move on in the process or be screened out, or (b) make a final hiring decision, at which point they would either receive a job offer or not.<sup>7</sup>

After the scenario, participants answered three attention and manipulation check questions, followed by several applicant reaction measures, which we organize into process-directed, company-directed, and self-directed reactions, presented in a randomized order. Participants then completed two single-item outcome measures in a randomized order, where they rated how favorably they viewed the company and their likelihood of accepting a job offer from the company. Lastly, participants completed measures of individual differences (including their familiarity with AI/ML) and a demographics questionnaire.

#### 3.1.3. Measures

As an attention check, participants indicated which assessments they would complete in the scenario from a list of four options (job knowledge test, video interview, role play simulation, personality test). In the selection approach and selection stage manipulation checks, participants indicated all the ways that their assessments would be scored and analyzed (i.e., hiring manager, artificial intelligence algorithms, random selection, and/or the company’s CEO) and how their assessment data

<sup>6</sup> We chose job knowledge tests and asynchronous video interviews for the scenario because (a) both assessments can be administered by human decision-makers, AI/ML-based tools, or both, and (b) participants should be familiar with the concepts of job knowledge tests and interviews, even if they have no formal experience completing them.

<sup>7</sup> The selection stage manipulation was thus designed to influence perceived psychological proximity (i.e., distant versus close) to the potential job offer. Researchers have used vignettes to successfully manipulate similar variables, such as another’s proximity to completing a task (Cryder et al., 2013), and one’s psychological and physical proximity to another person (Mencí & May 2009). We thus deemed the vignette approach as suitable for manipulating the selection stage.

would be used (final hiring decision, initial screening decision, determine which department to place the participant in, provide skill-related feedback), respectively.<sup>8</sup>

We measured participants' reactions toward the hiring process, the company, and themselves. For *process-directed reactions*, we measured *procedural justice* with seven items (e.g., "I would be able to express my views and feelings";  $\alpha = 0.83$ ) and *interpersonal justice* – a form of interactional justice – using three items (e.g., "I would be treated with dignity";  $\alpha = 0.95$ ), both from Colquitt (2001). For *company-directed reactions*, we measured (a) *perceived fit* with three items from Cable and DeRue (2002); e.g., "My personal values would match the organization's values and culture";  $\alpha = 0.91$ ), (b) perceptions of the company's culture as *supportive* (e.g., "The company's leadership would consider my personal feelings before acting";  $\alpha = .85$ ) and valuing *personal recognition* (e.g., "The company's leadership would acknowledge improvement in my quality of work";  $\alpha = .87$ ) with three items each from Shao et al. (2012), and (c) *perceived innovation climate* using four items from Campbell et al. (2014); e.g., "This organization would be open and responsive to change";  $\alpha = 0.86$ ).

For *self-directed reactions*, we measured (a) *perceived control* using four items from Saks and Ashforth (1999); e.g., "Obtaining this job would be totally within my control";  $\alpha = 0.72$ ), (b) *applicant self-efficacy* using five items from Tay et al. (2006); e.g., "I would be able to prepare for this type of hiring process";  $\alpha = 0.88$ ), and (c) *basic need satisfaction* using nine items from Sheldon et al. (2001), with three items each measuring need satisfaction for *autonomy* (e.g., "I could freely do things my own way";  $\alpha = 0.87$ ), *competence* (e.g., "I am capable in what I do";  $\alpha = 0.82$ ), and *relatedness* (e.g., "I have a sense of contact with other people";  $\alpha = 0.92$ ).

Participants completed two single-item measures assessing *outcomes of the selection process*. Participants *evaluated the company* by rating it from 1 to 5 stars. This type of measure has been used in past research for evaluating organizations, has been found to correlate with other measures of organizational effectiveness (Boyne et al., 2011), and has high fidelity to common ratings that are used in customer and employee reviews of organizations through third party services, such as Yelp and Glassdoor. Participants also read a brief epilogue in which they imaged receiving an attractive offer from the company, and similarly attractive offers from other companies. Participants used a slider tool to indicate their *likelihood of accepting the offer* from the focal company (i.e., 0%–100%).<sup>9</sup>

Lastly, participants completed three individual difference measures: a 2-item measure of *familiarity with AI* adapted from Gonzalez et al. (2019; e.g., "How much do you know about artificial intelligence?";  $\alpha = 0.71$ ), a 10-item measure of *trait curiosity* from Kashdan et al. (2009; e.g., "I prefer jobs that are excitingly unpredictable";  $\alpha = 0.88$ ), and a 7-item measure of *trait anxiety* from Bieling et al. (1998, based on Spielberger et al., 1985; e.g., "I feel nervous and restless";  $\alpha = 0.91$ ). The former

<sup>8</sup> We re-ran our analyses by excluding participants who did not pass all attention and manipulation checks. Most of our findings that were originally statistically significant either remained statistically significant or became non-significant but had similar sized effects. The only exception was procedural justice, which no longer appeared to be affected by the selectin approach. For statistical power, we retained our original inclusion criteria.

<sup>9</sup> While the single-item format of the *offer acceptance* measure could raise concerns about its validity, we note that the measure was moderately correlated with our other outcome measure, evaluations of the organization ( $r = 0.52$ ,  $p < .001$ ), and correlated most strongly with this other outcome measure, relative to any other reaction measures.

measure allowed us to test our third hypothesis, regarding the moderating effects of applicants' familiarity levels with AI/ML. We included the latter two measures for exploratory, internal research.<sup>10</sup> Together with the AI familiarity measure, we asked participants descriptive questions about (a) their opinions about AI in organizations and in society, (b) whether they worked in IT, computer science, or a related field, and (c) whether they trust AI to make important work decisions, and why. These items were ultimately not analyzed. Descriptive statistics and correlations are presented in the supplemental materials.

We conducted CFA to rule out common method variance as an issue in our measures. We estimated a model with separate latent variables for each applicant reaction (11 total). We omitted the two outcome variables (i.e., likelihood of accepting an offer, rating of the company) which were single-item measures. We allowed latent variables to covary, given that each reaction refers to the same event (i.e., our manipulations). The model fit the data reasonably well. All indices but the CFI met recommended fit thresholds: CFI = 0.892, RMSEA = 0.064 (90% CI [0.058, 0.070]), SRMR = 0.067. While the CFI fell below the minimum recommended threshold for model fit (i.e., CFI  $\ge 0.90$ ), we note that CFI penalizes models that estimate many parameters (Kenny, 2020), and is less suitable for evaluating confirmatory models than RMSEA (Rigdon, 1996). We next estimated a model with all the reaction items loading on a single common method variance factor, which fit the data poorly: CFI = 0.498, RMSEA = 0.133 [90% CI [0.129, 0.138)], SRMR = 0.105. We thus determined that it was appropriate to separately analyze scores from each applicant reaction measure.<sup>11</sup>

### 3.2. Study 1 results

#### 3.2.1. Manipulation checks

We first conducted Chi square analyses to examine the effect of each manipulation on responses to the manipulation check questions. Supporting the validity of our manipulations, most participants correctly answered the selection approach manipulation check,  $X^2(6) = 286.61$ ,  $p < .001$ , and the selection stage manipulation check,  $X^2(2) = 129.95$ ,  $p < .001$ . Each manipulation did not affect its non-respective manipulation check, supporting the independence of the manipulations.

#### 3.2.2. *Process-directed reactions*

We tested our hypotheses using general linear modeling (GLM), with the selection approach and selection stage entered as categorical independent variables, and familiarity with AI entered as a continuous independent variable (Cohen & Cohen, 1983). For all post-hoc analyses, we applied Bonferroni corrections to account for familywise error rates.

We found significant main effects of the selection approach on procedural justice,  $F(2, 170) = 5.05$ ,  $p = .007$ ,  $\eta_p^2 = 0.06$ , and interactional justice,  $F(2, 171) = 4.69$ ,  $p = .010$ ,  $\eta_p^2 = 0.05$ . Participants perceived more procedural and interactional justice during the human-based ( $M = 3.17$  and  $M = 3.79$ , respectively) than during the AI/ML-based approach ( $M = 2.78$  and  $M = 3.27$ ;  $p = .015$  and  $p = .009$ ). Furthermore, participants perceived the augmented approach as having greater levels of procedural justice ( $M = 3.15$ ,  $p = .027$ ), but similar levels of interactional justice ( $M = 3.62$ ,  $p = .135$ ), relative to the AI/ML-based

<sup>10</sup> Specifically, several of the authors developed pre-employment assessments outside of the current research that tap into constructs such as trait anxiety and curiosity, among others. We thus included basic psychological measures of these two constructs to gain initial insights into the suitability of these pre-employment assessments for future applicant reactions research. We report significant findings from these analyses in the supplemental materials.

<sup>11</sup> For more specific information regarding our CFA findings, please contact the first author.

approach. The human-based and augmented approaches did not significantly differ in either analysis ( $p = 1.00$  and  $p = 1.00$ ).<sup>12</sup> AI familiarity also had a significant main effect on procedural justice,  $F(1, 170) = 4.23$ ,  $p = .041$ ,  $\eta_p^2 = 0.02$ , such that higher familiarity levels were associated with greater procedural justice perceptions. We found no other main or interaction effects of the selection stage, nor of familiarity. Thus, regarding process-directed reactions, we found mixed support for Hypothesis 1a (i.e., full support for procedural justice, partial support for interactional justice) and no support for Hypotheses 2 or 3.

#### 3.2.3. Company-directed reactions

The selection approach significantly affected perceptions of the company as having a supportive culture,  $F(2, 171) = 5.32$ ,  $p = .006$ ,  $\eta_p^2 = 0.06$ , and valuing personal recognition,  $F(2, 171) = 8.42$ ,  $p < .001$ ,  $\eta_p^2 = 0.09$ . Participants perceived companies using a human-based approach as being more supportive ( $M = 3.27$ ) and as more strongly valuing personal recognition ( $M = 3.78$ ) than companies using an AI/ML-based approach ( $M = 2.79$ ,  $p = .006$ ;  $M = 3.20$ ,  $p < .001$ , respectively). Furthermore, relative to the augmented approach, companies using the human-based approach were perceived as being similarly supportive ( $M = 2.93$ ,  $p = .082$ ) and as more strongly valuing personal recognition ( $M = 3.40$ ,  $p = .029$ ). The augmented and AI/ML-based approaches did not significantly differ ( $p = 1.00$  and  $p = .560$ , respectively). These findings partially support Hypothesis 1a—the human-based approach, but not the augmented approach, led to more favorable company-directed reactions than the AI/ML-based approach.

Contrary to Hypothesis 1a and 1b, the selection approach did not significantly affect perceived fit nor perceived innovation climate,  $Fs \le 2.69$ ,  $ps \ge .071$ ,  $\eta_p^2 \le 0.03$ .<sup>13</sup> Contrary to Hypotheses 2 and 3, neither the selection stage nor familiarity with AI had any significant main or interactive effects on company-directed reactions. Hypothesis 1a thus received partial support for the company-directed reactions, whereas Hypotheses 1b, 2 and 3 were not supported.

#### 3.2.4. Self-directed reactions

The selection approach significantly affected perceived control,  $F(2, 171) = 3.64$ ,  $p = .028$ ,  $\eta_p^2 = 0.04$ , self-efficacy,  $F(2, 171) = 7.33$ ,  $p = .001$ ,  $\eta_p^2 = 0.08$ , and need for competence,  $F(2, 171) = 3.58$ ,  $p = .030$ ,  $\eta_p^2 = 0.04$ . Participants perceived more control, self-efficacy, and competence during the human-based approach ( $M = 3.15$ ,  $M = 3.73$ , and  $M = 3.70$ , respectively), relative to the AI/ML-based approach ( $M = 2.81$ ,  $M = 3.14$ , and  $M = 3.32$ , respectively;  $ps \le .036$ ). Furthermore, the augmented approach led to higher levels of self-efficacy ( $M = 3.58$ ,  $p = .019$ ) than the AI/ML-based approach, but led to similar levels of control and competence as the AI/ML-based approach ( $M = 3.08$ , and  $M = 3.62$ , respectively;  $p = .141$  and  $p = .141$ ). The human-based and augmented approaches did not significantly differ for these three variables (all  $ps = 1.00$ ). Thus, Hypothesis 1a received full support for self-efficacy, and partial support for perceived control and need for competence.

We found one significant main effect each of the selection stage and familiarity with AI. Specifically, the selection stage significantly affected self-efficacy,  $F(1, 171) = 4.86$ ,  $p = .029$ ,  $\eta_p^2 = 0.03$ , such that participants experienced greater self-efficacy in the final selection stage ( $M = 3.63$ ) than in the screening stage ( $M = 3.33$ ). Familiarity with AI had a

significant main effect on need for competence,  $F(1, 171) = 6.12$ ,  $p = .014$ ,  $\eta_p^2 = 0.04$ , such that higher levels of familiarity were associated with greater satisfaction of competence needs.

The selection stage and selection approach significantly interacted to affect perceived control,  $F(2, 171) = 5.58$ ,  $p = .004$ ,  $\eta_p^2 = 0.06$ . Supporting Hypothesis 2, follow-up one-way ANOVAs revealed that the selection approach significantly affected perceived control in the final stage condition,  $F(2, 85) = 9.26$ ,  $p < .001$ ,  $\eta_p^2 = 0.18$ , but not in the screening condition,  $F(2, 92) = 0.16$ ,  $p = .854$ ,  $\eta_p^2 = 0.003$  (see Fig. 1). Probing the final stage condition further, participants perceived greater control in both the human-based ( $M = 3.34$ ) and augmented approaches ( $M = 3.20$ ), relative to the AI/ML-based approach ( $M = 2.61$ ,  $p < .001$  and  $p = .005$ , respectively). The former two approaches did not significantly differ ( $p = 1.00$ ). We found no other significant main or interactive effects on the self-directed reactions. Thus, for self-directed reactions, we found partial support for Hypotheses 1a and 2, and no support for Hypothesis 3.

#### 3.2.5. Outcomes of the hiring process

The selection approach significantly affected evaluations of the company,  $F(2, 171) = 6.78$ ,  $p = .001$ ,  $\eta_p^2 = 0.07$ , and the likelihood of accepting an offer from the company,  $F(2, 171) = 5.93$ ,  $p = .003$ ,  $\eta_p^2 = 0.07$ . Regarding evaluations of the company, partially supporting Hypothesis 1a, participants evaluated organizations more favorably following a human-based approach ( $M = 3.85$ ) than following an AI/ML-based approach ( $M = 3.40$ ,  $p = .001$ ). However, the augmented approach fell in the middle ( $M = 3.57$ ) and did not significantly differ from either of the other approaches ( $p = .075$  and  $p = .546$ , respectively).

Regarding the likelihood of accepting an offer, supporting Hypothesis 1a, participants were significantly more likely to accept the offer after a human-based approach ( $M = 67.93\%$ ) or an augmented approach ( $M = 65.74\%$ ), relative to an AI/ML-based approach ( $M = 57.28\%$ ,  $p = .004$  and  $p = .035$ , respectively). The former two approaches did not significantly differ ( $p = 1.00$ ). The selection stage did not have significant main effects or interactions on either outcome,  $Fs \le 1.78$ ,  $ps \ge .184$ ,  $\eta_p^2 = 0.01$ . Familiarity with AI had a significant main effect on the likelihood of accepting an offer from the company,  $F(2, 171) = 5.21$ ,  $p = .024$ ,  $\eta_p^2 = 0.03$ , but not on evaluations of the company,  $F(2, 171) = 1.59$ ,  $p = .209$ ,  $\eta_p^2 = 0.01$ .

![Bar chart showing the interaction between selection approach (Human, AI, Human+AI) and selection stage (Screening, Final Stage) on perceived control. The Y-axis is Perceived Control (1.00 to 5.00). For the Human approach, perceived control is highest in the Final Stage (approx. 3.4) and lowest in the Screening Stage (approx. 2.8). For the AI approach, perceived control is highest in the Screening Stage (approx. 3.1) and lowest in the Final Stage (approx. 2.8). For the Human+AI approach, perceived control is highest in the Final Stage (approx. 3.3) and lowest in the Screening Stage (approx. 2.9). Error bars represent 95% CI.](bd4e83fb391d10fb5f7595a1cf5f3f05_img.jpg)

Bar chart showing the interaction between selection approach (Human, AI, Human+AI) and selection stage (Screening, Final Stage) on perceived control. The Y-axis is Perceived Control (1.00 to 5.00). For the Human approach, perceived control is highest in the Final Stage (approx. 3.4) and lowest in the Screening Stage (approx. 2.8). For the AI approach, perceived control is highest in the Screening Stage (approx. 3.1) and lowest in the Final Stage (approx. 2.8). For the Human+AI approach, perceived control is highest in the Final Stage (approx. 3.3) and lowest in the Screening Stage (approx. 2.9). Error bars represent 95% CI.

Fig. 1. Interaction between selection approach and selection stage on perceived control.

<sup>12</sup> While we report  $p$ -values of 1.00, these likely reflect either (a) decimal rounding ( $p > .999$ ) and/or (b) artifacts of statistical corrections for familywise error in post-hoc analyses.  $p$ -values theoretically cannot equal exactly 1.00 or 0.00, and so readers should thus  $p$ -values of 1.00 as *approaching* 1.00, rather than being exactly 1.00.

<sup>13</sup> For concision, we provide the max.  $F$ , min.  $\eta_p^2$  to describe multiple non-significant effects. We provide the cleaned data files for each study with our key variables on the Open Science Framework: [https://osf.io/z9gsa/?view\\_only=ccc8956f354a466db08c18ff27b2c94f](https://osf.io/z9gsa/?view_only=ccc8956f354a466db08c18ff27b2c94f).

Lastly, we found significant two-way interactions between AI familiarity and the selection approach on evaluations of the company,  $F(2, 171) = 3.10, p = .048, \eta_p^2 = 0.04$ , and the likelihood of accepting an offer,  $F(2, 171) = 5.54, p = .005, \eta_p^2 = 0.06$  (see Fig. 2A and B, respectively). Simple slopes analyses revealed that AI familiarity significantly and positively predicted evaluations of organizations using an AI/ML-based approach,  $B = 0.33, s.e. = 0.14, p = .018$ , but did not significantly predict evaluations of organizations using a human-based approach,  $B = -0.15, s.e. = 0.12, p = .207$ , nor an augmented approach,  $B = 0.14, s.e. = 0.13, p = .269$ . Likewise, AI familiarity significantly and positively predicted the likelihood of accepting a job offer after an AI/ML-based approach,  $B = 12.46, s.e. = 3.16, p < .001$ , but did not significantly predict the likelihood of accepting offers after a human-based approach,  $B = -3.17, s.e. = 3.06, p = .305$ , nor an augmented approach,  $B = 3.96, s.e. = 3.56, p = .270$ .

Visual inspection of Fig. 2 reveals that the selection approach seemed to affect participants who had lower levels of familiarity with AI in a similar manner to what we found earlier in our results, whereas the selection approach did not seem to affect participants who had higher levels of familiarity with AI. In other words, higher levels of familiarity with AI seemed to mitigate any differences in reactions toward the various selection approaches. In sum, for outcomes of the hiring process, we found partial support for Hypothesis 1a, full support for Hypothesis 3, and no support for Hypothesis 2.

### 3.3. Study 1 discussion

Study 1 revealed several themes. First, we found partial support for Hypothesis 1a, that augmented and human-based selection approaches lead to more favorable reactions than a solely AI/ML-based approach. Our findings depended on the type of reaction studied. Regarding process-directed reactions, partially supporting an organizational justice view, augmented approaches enhanced reactions toward AI/ML with regards to procedural justice, but not interactional justice. Importantly, in both cases, the augmented approach performed similar to the human-based approach (i.e., did not significantly differ) with regard to both types of fairness reactions. Our data thus suggest that the use of AI/ML alone may lead job applicants to feel disrespected and draw unfavorable inferences about the fairness of the hiring process.

The human-based approach generally led to more favorable self-directed reactions – namely, perceived control, self-efficacy, and competence need satisfaction, but not autonomy or relatedness need satisfaction. Furthermore, the augmented approach improved self-efficacy, relative to the AI/ML-based approach, although it did not differ from either approach for the remaining self-directed reactions, contrary to hypotheses. Our data suggest that applicants may perceive AI/ML as hindering their capacity to convey their skills and abilities during the hiring process (relating to competence needs), but not necessarily as hindering their freedom of choice or capacity for human connection (relating to autonomy and relatedness, respectively). Participants' reduced sense of control and self-efficacy during AI/ML-based processes may likewise stem from perceiving inadequate opportunity to demonstrate their competence.

Similarly, augmented and human-based approaches led to a greater likelihood of accepting a job offer from the company, relative to a purely AI/ML-based approach, which corroborates past research indicating that applicant reactions (both in general and toward AI/ML in particular) can spillover to influence their behavioral intentions toward the hiring organization (e.g., Gonzalez et al., 2019; Mirowska, 2020; Smither et al., 1993; Stoughton et al., 2015). Our findings raise the question of whether there are hidden costs to using AI/ML during hiring. In other words, while AI/ML-based selection tools offer the potential for improved efficiency and prediction (assuming the algorithms are appropriately developed and validated), adverse applicant reactions could potentially undermine their utility if not managed properly.

Contrary to our predictions, the augmented approach did not

improve perceptions of the company, nor evaluations of the company. Specifically, the AI/ML-based approach led to poorer perceptions and evaluations of the company than the human-based approach, whereas the augmented approach generally did not differ from either approach. The sole exception was with perceptions of the company as valuing personal recognition, in which the augmented and AI/ML-based approaches were similarly viewed unfavorably. We theorized that the inclusion of a human decision-maker would signal to applicants that the organization values its human capital by investing human resources in the process, rather than fully automating it. Possibly, the augmented approach may also send mixed signals to applicants by including human decision-makers, on the one hand, and by automating parts of the human decision-maker's role, on the other hand. Nevertheless, it is worth noting that, more often than not, the augmented approach and the human-based approach yielded similar levels of company-directed reactions (i.e., they did not differ from one another in the reactions evoked).

While we offer the above explanation post-hoc, it is nevertheless a potentially interesting question worth investigating: How do applicants respond to competing signals in the hiring process? Furthermore, while the selection approach may not directly affect some of these company-directed reactions, they may indirectly affect them through proximal reactions, such as process-directed and self-directed reactions. We thus offer a second research direction: How do reactions to AI/ML and augmented approaches unfold over time?

Second, except for self-efficacy, we did not find support for Hypothesis 2, that the selection approach would more strongly affect reactions at later stages of the process, relative to earlier stages. One possible reason for our findings is that participants did not receive an unfavorable outcome, whereas the influence of fairness information often depends on outcome favorability (Brockner & Wiesenfeld, 1996). Specifically, the receipt of an undesirable outcome typically triggers a search to understand why the outcome occurred (Lind & Van den Bos, 2002), with fairness information indicating the outcome's legitimacy. In Study 1, we measured reactions before the hiring decision occurred and after participants received an offer from the organization, thus potentially reducing the need for an informational search. We were particularly interested in cases where applicants received a hiring offer, as this type of situation would enable applicants to have the most opportunity to impact the organization (i.e., by accepting or rejecting their job offer). However, we recognize the absence of an undesirable outcome as a limitation and suggest that future research investigate contexts in which applicants are turned down from a hiring offer.

Lastly, in line with past research (Gonzalez et al., 2019), our findings suggest that the effects of the various selection approaches on applicant reactions partly depends on applicants' familiarity levels with AI. Among people who are more familiar with AI, their evaluations and behaviors at the end of an AI/ML-based or augmented selection process seemed no different than that of a human-based process. It was particularly among people with less familiarity that the selection approach seemed to matter. Practically, our findings indicate that organizations should consider the base rate of applicants in their pool who are familiar with AI/ML when deciding whether to implement AI/ML-based selection processes. For example, the use of AI/ML may lead to fewer concerns during selection processes in which the applicant pool consists of mostly data scientists or information technology experts.

A potential limitation in Study 1 pertains to the level of abstraction at which we examined our research questions. In our vignette, we manipulated the selection approach by broadly describing one of the approaches as being used to make a selection decision, rather than describing how the approach is being used with greater specificity. Our scenarios were also broad in that we did not focus on a specific industry, job type, or applicant population. We intentionally focused on this higher level of abstraction so our findings could generalize beyond one specific selection context or application of AI/ML. However, we recognize that such a broad level of abstraction could limit the ecological

![Figure 2 shows two line graphs illustrating the two-way interaction between selection approach (Human, AI/ML, Augmented) and self-reported familiarity with AI (Low Familiarity with AI, High Familiarity with AI). Graph (A) is 'Rating of Organization', showing ratings (1 to 5) for each selection approach. Graph (B) is 'Likelihood of Accepting Offer', showing likelihoods (30 to 100) for each selection approach. In both graphs, the likelihood of accepting the offer generally increases with familiarity with AI, and the rating of the organization generally increases with familiarity with AI, but the interaction effect is significant, showing that the increase in likelihood and rating is stronger for the Augmented approach compared to Human and AI/ML approaches.](91be14371a97fb5ce9eeb29ae18d07c3_img.jpg)

Figure 2 shows two line graphs illustrating the two-way interaction between selection approach (Human, AI/ML, Augmented) and self-reported familiarity with AI (Low Familiarity with AI, High Familiarity with AI). Graph (A) is 'Rating of Organization', showing ratings (1 to 5) for each selection approach. Graph (B) is 'Likelihood of Accepting Offer', showing likelihoods (30 to 100) for each selection approach. In both graphs, the likelihood of accepting the offer generally increases with familiarity with AI, and the rating of the organization generally increases with familiarity with AI, but the interaction effect is significant, showing that the increase in likelihood and rating is stronger for the Augmented approach compared to Human and AI/ML approaches.

Fig. 2. Two-way interaction between selection approach and self-reported familiarity with AI on (a) participants' evaluations of the organization in the scenario and (b) the likelihood that participants would accept a job offer from the organization.

validity of our findings, in that the contexts we examined are more abstract than what job applicants would actually encounter. We sought to address these limitations in Study 2 by testing our hypotheses in a richer, more specific context.

## 4. Study 2

Study 2 serves as a “use case,” in which we examined how people react toward a specific application of the augmented, human-based, and AI/ML-based approaches in a specific industry. We examined how people react when these selection approaches are used to score and interpret asynchronous video interview data. In asynchronous video interviews (AVIs), applicants record themselves answering structured interview questions, without the presence or involvement of an interviewer Lukacik et al. (2022). AVIs have become popular because they reduce time demands for human resource managers and recruiters, who otherwise would conduct the interviews, and they offer applicants flexibility to complete the interview on their own time – although research suggests that reactions toward AVIs may vary (Basch & Melchers, 2019). AVIs have become a ripe area to apply AI/ML (e.g., Suen et al., 2019), which job applicants have also become aware of (Laurim et al., 2021). For example, organizations can extract unique data points from interview recordings, such as verbal and nonverbal behavior, which can be linked to constructs from which to base hiring decisions on (Lukacik, Bourdage, & Roulin, 2022).

We examined our hypotheses within the context of the hospitality industry by designing vignettes describing a selection process for a hotel manager role. We focused on the hospitality industry for two reasons. First, the nature of work in industries such as hospitality typically emphasize human interaction, and thus the augmented approach could be particularly beneficial for hospitality organizations that seek to use AI/ML while still signaling their value of human connection. Second, organizations within the hospitality industry are already adopting technologies such as AVIs and AI/ML in their hiring processes (Albert, 2019), thus aiding the ecological validity of Study 2. Lastly, to further enhance ecological validity, we recruited participants who worked in industries relevant to the position described in our vignette.

### 4.1. Study 2 method

In Study 2, we examined the human-based, AI/ML-based, and

augmented approaches (Hypothesis 1a) within a hiring process for hospitality managers. Based on our findings from Study 1, we again explored whether familiarity with AI moderated reactions to these approaches (Hypothesis 3). We tested our hypotheses in a 3-level between-subjects experimental design (selection approach: human-based, AI/ML-based, augmented).

#### 4.1.1. Participants

An *a priori* power analysis for a one-way ANOVA with a 3-level factor indicated that we needed a minimum sample size of 158 participants to detect a medium-sized interaction effect ( $f = 0.25$ ,  $\alpha = 0.05$ , power = .80, numerator  $df = 2$ ,  $k = 3$ ; Faul et al., 2007). We recruited 223 US-based adults from Prolific, an online research recruitment platform, who participated in exchange for \$2.38. Because our study context pertained to selecting managers in the hospitality industry, we recruited participants who had current or past work experience in either Hospitality/Tourism or Marketing/Sales.<sup>14</sup> Sixty participants failed our manipulation check question and were excluded from analyses; yielding a final sample size of 163 (42.3% male, 55.8% female, 1.8% other; 75.5% Caucasian;  $M_{age} = 32.50$  years,  $sd = 9.88$ ;  $M_{work experience} = 12.71$  years,  $sd = 9.55$ ).

#### 4.1.2. Procedure and measures

We told participants that we were studying hiring processes in the hospitality industry, and that they would imagine themselves as job applicants. To help participants envision themselves in the scenario, we asked participants to recall a time when they applied for a job, including their thoughts and feelings during that experience. Participants then proceeded to read the hiring scenario, where they read that they applied to several jobs and came across a listing for a position that they would be qualified for. Participants received a full job description for the role of a hotel manager, which we adapted from publicly available templates on

<sup>14</sup> As indicated in our experimental materials, the focal job in the study involved several skills relevant to marketing and sales, such as building relationships with vendors and clients, and selling hospitality services. We thus felt that sales and marketing were applicable industries to recruit participants from, in addition to hospitality and tourism.

Indeed.com.<sup>15</sup> The job description included information about the employer, responsibilities and requirements for the role, an equal employment opportunity statement, and a salary range.<sup>16</sup> After reviewing the job description, participants read that a couple weeks later they had moved on to the next stage of the hiring process for multiple companies, including the one they had just read about.

Participants then read that the company asked them to complete an asynchronous video interview, and received instructions about the interview procedure. We manipulated the selection approach in the instructions by describing the next steps for the interview. Participants in the AI/ML condition read that their interview responses would be transcribed and evaluated using trained AI/ML algorithms, which would automatically determine whether they can move on to the next selection stage. Participants in the human condition read that an experienced member of the hiring team would review and evaluate their responses and decide whether they can move to the next stage. Participants in the augmented condition read that their responses would be transcribed and evaluated using AI/ML, and that an experienced member of the hiring team would review the results of this analysis and decide whether they can move to the next stage.

Next, participants completed dependent measures of their reactions to the hiring process. For simplicity, we measured two of each class of reactions in a randomized order, using the same measures from Study 1. Specifically, we measured *procedural* and *interactional justice* as our process-directed reactions (Colquitt, 2001), perceptions of the company as having a *supportive culture* and as *valuing personal recognition* as our company-directed reactions (Shao et al., 2012), and *perceived control* (Saks & Ashforth, 1999) and *self-efficacy* (Tay et al., 2006) as self-directed reactions. Following these measures, participants reported their *likelihood of accepting an offer* from the company, using the same slider measure from Study 1, followed by an open-ended question regarding their thoughts and reactions from the scenario.

Following the dependent measures, participants completed a manipulation check. They indicated whether their interview recordings would be reviewed by AI/ML algorithms, a member of the hiring team, both, or neither. Lastly, participants completed the AI *familiarity* measure from Study 1 and a demographic questionnaire.

We again conducted CFA, modeling each applicant reaction as a separate latent variable, excluding the single-item likelihood of acceptance variable (6 latent variables total). The model fit the data reasonably well, CFI = 0.924, RMSEA = 0.077 (90% CI [0.067, 0.087]), SRMR = 0.068. An alternative, single factor model fit the data poorly, CFI = 0.603, RMSEA = 0.170 (90% CI [0.162, 0.179]), SRMR = 0.105, enabling us to separately analyze scores from each measure.

### 4.2. Study 2 results

#### 4.2.1. Manipulation check

Supporting the validity of our manipulation, most participants correctly answered the manipulation check, based on their experimental condition,  $\chi^2(6) = 188.81, p < .001$ .

#### 4.2.2. Process-directed reactions

We tested Hypotheses 1a and 3 using GLM, with the selection approach entered as a categorical independent variable, and familiarity with AI entered as a continuous independent variable. As with Study 1, we applied Bonferroni corrections for all post-hoc analyses. We did not find any significant main or interactive effects on perceptions of procedural justice or interactional justice,  $Fs \le 2.57, ps \ge .080, \eta^2 \le 0.03$ .

Hypotheses 1a and 3 were thus not supported. These findings run contrary to those of Study 1, in which we found significant effects of the selection approach on both justice perceptions.

#### 4.2.3. Company-directed reactions

The selection approach significantly affected perceptions of the company as valuing personal recognition,  $F(2, 157) = 4.67, p = .011, \eta^2 = 0.06$ , but not perceived supportive culture,  $F(2, 157) = 2.04, p = .133, \eta^2 = 0.05$ . Participants perceived companies using a human-based approach as more strongly valuing personal recognition ( $M = 3.47$ ) than companies using an AI/ML-based approach ( $M = 2.95, p = .009$ ). The augmented approach fell in the middle and did not differ from the other approaches ( $M = 3.30, p = 1.00$  and  $p = .210$ , respectively). These findings partially support Hypothesis 1a and partially replicate Study 1, with two exceptions. First, whereas the human-based approach led to greater perceived value of personal recognition than the augmented approach in Study 1, we found here that participants viewed companies using either approach as similarly valuing personal recognition. Second, the selection approach similarly affected both types of culture perceptions in Study 1, whereas here we found that the selection approach only affected perceptions of the company as valuing personal recognition. Familiarity with AI did not have any significant main effects,  $Fs \le 0.69, ps \ge .408, \eta^2 \le 0.004$ .

The selection approach and familiarity with AI significantly interacted to affect perceptions of the company as valuing personal recognition,  $F(2, 157) = 5.46, p = .005, \eta^2 = 0.07$ , and as having a supportive culture,  $F(2, 157) = 4.42, p = .017, \eta^2 = 0.05$  (see Fig. 3A and B, respectively). Simple slopes analyses revealed that AI familiarity (a) significantly and positively predicted both types of reactions in AI/ML-based approaches (personal recognition:  $B = 0.471, se = 0.193, p = .018$ ; supportive:  $B = 0.450, se = 0.176, p = .014$ ), (b) did not predict either reaction in human-based approaches (personal recognition:  $B = 0.141, se = 0.154, p = .365$ ; supportive:  $B = 0.268, se = 0.179, p = .138$ ), and (c) significantly and negatively predicted perceptions of valuing personal recognition in augmented approaches ( $B = -.518, se = 0.253, p = .047$ ), but did not predict perceptions of a supportive culture ( $B = -.427, se = 0.247, p = .091$ ).

We probed both interactions further by examining regions of significance (Preacher et al., 2006), in which we examined over what levels of familiarity with AI each pair of selection approaches significantly differed from each other. To interpret the following statistics, we remind readers that AI familiarity scores ranged from 1 to 5 ( $M = 2.23, sd = 0.68$ ). Regarding perceptions of the company as valuing personal recognition, both the human-based and augmented approaches led to more favorable perceptions than the AI/ML-based approach for participants who had lower levels of familiarity with AI (i.e.,  $X_{familiarity} \le 2.61$  and 2.18, respectively). Furthermore, the augmented approach led to less favorable perceptions than both the human-based and AI/ML-based approaches for participants who had roughly average or higher levels of familiarity (i.e.,  $X_{familiarity} \ge 2.61$  and 3.28, respectively). The human-based and AI/ML-based approaches did not differ at higher familiarity levels.

Regarding perceptions of the company as having a supportive culture, the human-based approach led to more favorable perceptions than the AI/ML-based approach for participants who had moderately lower levels of familiarity ( $1.52 \le X_{familiarity} \le 2.24$ ). The augmented approach led to more favorable perceptions than the AI/ML-based approach for participants who had much lower levels of familiarity ( $X_{familiarity} \le 1.70$ ). In contrast, the augmented approach led to less favorable perceptions than both the human-based and AI/ML-based approaches for participants who had roughly average or higher levels of familiarity ( $X_{familiarity} \ge 2.45$  and 2.84, respectively).

Our examination of company-directed reactions suggests that both the human-based and augmented approaches yielded more favorable perceptions of the company's culture than AI/ML-based approaches, particularly for people who have less familiarity with AI. However, our

<sup>15</sup> Templates can be found at <https://www.indeed.com/hire/job-descripti on/hotel-manager?hl=en&co=US>.

<sup>16</sup> The salary range is based on salary estimates for a hotel manager from Glassdoor.com: [https://www.glassdoor.com/Salaries/hotel-manager-salary -SRCH\\_K00,13.htm](https://www.glassdoor.com/Salaries/hotel-manager-salary -SRCH_K00,13.htm).

![Figure 3 shows four plots (A, B, C, D) illustrating interactions between selection approach (AI, Human, Augmented) and familiarity levels (2sd, 1sd, M, +1sd, +2sd) on perceived recognition culture, perceived supportive culture, perceived control, and self-efficacy. The plots show lines representing different selection approaches (AI, Human, Augmented) and colored regions indicating areas of significance. Arrows indicate the direction of the region of significance. The minimum AI familiarity score is 1. The legend indicates AI (solid line), Human (dashed line), and Augmented (dotted line).](10c82dcc5f2c237961329dd29d65859c_img.jpg)

Figure 3 shows four plots (A, B, C, D) illustrating interactions between selection approach (AI, Human, Augmented) and familiarity levels (2sd, 1sd, M, +1sd, +2sd) on perceived recognition culture, perceived supportive culture, perceived control, and self-efficacy. The plots show lines representing different selection approaches (AI, Human, Augmented) and colored regions indicating areas of significance. Arrows indicate the direction of the region of significance. The minimum AI familiarity score is 1. The legend indicates AI (solid line), Human (dashed line), and Augmented (dotted line).

Fig. 3. Interactions between the selection approach and familiarity levels with AI. Regions of significance are overlaid on each plot, indicating values of familiarity over which pairs of slopes significantly differ. Colored lines indicate where region of significance starts/ends. Arrows of the same color indicate the direction the region extends in. Red = human vs. augmented. Blue = augmented vs. AI. Gold = human vs. AI. While the X-axis includes -2sd (familiarity = 0.82), note that the minimum AI familiarity score is 1. <sup>a</sup> Not shown in Fig. 3D is the region of significance between the augmented and AI conditions at AI familiarity  $\ge 4.14$  ( $>3\text{sd}$ ).

data also suggest that highly familiar individuals do not perceive companies that use human-based or AI/ML-based approaches differently, and perceive companies *less* favorably when they use an augmented approach. Our findings thus partially support **Hypothesis 3**. These findings also differ from Study 1, in which no interactions were found for company-directed reactions.

#### 4.2.4. Self-directed reactions

The selection approach significantly affected self-efficacy,  $F(2, 157) = 3.41$ ,  $p = .036$ ,  $\eta_p^2 = 0.04$ , but not perceived control,  $F(2, 157) = 1.06$ ,  $p = .350$ ,  $\eta_p^2 = 0.01$ . Participants reported greater self-efficacy when a human-based approach was used ( $M = 3.48$ ), relative to an AI/ML-based approach ( $M = 2.98$ ,  $p = .030$ ), whereas the augmented approach fell in the middle and did not differ from either of the other approaches ( $M = 3.21$ ,  $p = .616$  and  $p = .870$ , respectively). These findings partially support Hypothesis 1a and partially replicate Study 1, similar to the company-directed reactions discussed previously. Familiarity with AI did not have significant main effects on either of these reactions,  $Fs \le 3.37$ ,  $ps \ge .068$ ,  $\eta_p^2 \le 0.02$ .

The selection approach and AI familiarity significantly interacted to affect perceived control,  $F(2, 157) = 4.38$ ,  $p = .014$ ,  $\eta_p^2 = 0.05$  (see Fig. 3C). Simple slopes analyses revealed that familiarity with AI significantly and positively predicted perceived control in both the AI/ML-based ( $B = 0.564$ ,  $se = 0.183$ ,  $p = .003$ ) and human-based approaches ( $B = 0.315$ ,  $se = 0.152$ ,  $p = .042$ ), but not in the augmented approach ( $B = -0.276$ ,  $se = 0.236$ ,  $p = .248$ ). When examining regions of significance, the augmented approach led to significantly more perceived control than the AI/ML-based approach for participants who had much lower levels of familiarity with AI ( $X_{\text{familiarity}} \le 1.70$ ). The augmented approach also led to significantly less perceived control than both the human-based and AI/ML-based approaches for participants who had moderate or higher levels of familiarity with AI ( $X_{\text{familiarity}} \ge 2.73$  and  $2.94$ , respectively). The slopes for the human-based and AI/ML-based approaches did not differ at any level of AI familiarity.

We also found a near-significant interaction effect on self-efficacy,  $F(2, 157) = 2.98$ ,  $p = .054$ ,  $\eta_p^2 = 0.04$  (see Fig. 3D). Because the  $p$ -value for this interaction fell close to the accepted threshold for statistical significance, we probed the interaction pattern further. Simple slopes

indicated that AI familiarity did not significantly predict self-efficacy in any condition ( $ps \ge .099$ ). When examining regions of significance, the human-based approach led to significantly higher self-efficacy than the AI/ML-based approach for participants with lower levels of familiarity with AI ( $X_{familiarity} \le 2.59$ ). Like the previous interactions, the augmented approach led to significantly lower self-efficacy than the human-based approach for participants who had moderate to high levels of AI familiarity ( $X_{familiarity} \ge 2.52$ ). Lastly, relative to the AI/ML-based approach, the augmented approach led to significantly higher self-efficacy for participants who had much lower levels of familiarity with AI ( $X_{familiarity} \le 1.76$ ), and led to significantly lower self-efficacy for participants for had much higher levels of familiarity ( $X_{familiarity} \ge 4.14$ ).

In sum, we find a similar pattern for self-directed reactions as we did for company-directed reactions. The augmented approach generally led to more favorable self-directed reactions than the AI/ML-based approach for people who were less familiar with AI. Conversely, the augmented approach led to less favorable self-directed reactions than the AI/ML-based and human-based approaches for people who were more familiar with AI. Our findings thus partially support **Hypothesis 3**. Again, these findings differ from Study 1, in which no interactions were found for self-directed reactions.

#### 4.2.5. Outcomes of the hiring process

The selection approach significantly affected the likelihood of accepting a job offer from the company,  $F(2, 156) = 4.18, p = .017, \eta^2 = 0.05$ . Replicating our findings from Study 1, the human-based approach led to significantly stronger intentions to accept the offer ( $M = 63.71$ ) than the AI/ML-based approach ( $M = 51.74, p = .013$ ). Unlike Study 1, in which the augmented approach had led to stronger intentions than the AI/ML-based approach, the augmented approach now fell in the middle and did not differ from the two other approaches ( $M = 57.84, p = .625$  and  $p = .556$ , respectively). Hypothesis 1a was thus partially supported. Furthermore, we did not find main or interactive effects for AI familiarity,  $Fs \le 1.77, ps \ge .185, \eta^2 \le 0.01$ . **Hypothesis 3** was thus not supported, nor did we replicate the interaction pattern found in Study 1, although the interactions discussed earlier in this section partially resemble those obtained in Study 1.

### 4.3. Study 2 discussion

The results of Study 2 provide additional nuance regarding our earlier findings. First, we again found that the selection approach used for making selection decisions – in this case, whether an applicant moves on to the next stage of the process – can influence applicant reactions and outcomes of the hiring process. The human-based approach led to more favorable reactions and a higher likelihood of accepting the company’s offer, relative to the AI/ML-based approach. The augmented approach generally emerged as a “middle ground” in terms of applicant reactions, in that it did not differ from the other two approaches.

Second, as hypothesized, we found that participants who were less familiar with AI tended to view the human-based and augmented approaches more favorably than the AI/ML-based approach. However, we also unexpectedly found that participants who were more familiar with AI generally viewed the augmented approach less favorably than the human-based approach and, in some cases, the AI/ML-based approach. Possibly, those who are more familiar with AI may view either a human decision-maker or an AI/ML algorithm as sufficient for handling asynchronous video data, whereas the combination of the two may be viewed as excessive, redundant, or as indicating that either the human or AI-driven part of the process was not well-implemented (e.g., human decision-makers being added to the process because the AI has flaws). By including multiple decision-making approaches, these participants may have seen the process as too complicated, and hence souring their views of the company and hindering their perceptions of control and self-efficacy. Interestingly, this interaction pattern did not emerge for procedural or interactional justice, nor for intentions to accept a job offer

from the company. Our findings thus highlight that (a) different applicant reactions are not interchangeable, and that (b) reactions during the process do not always translate into behaviors after the process.

A final takeaway from Study 2 pertains to the importance of context. While we generally show a consistent pattern across both studies regarding how applicants react toward the different selection approaches, our findings became more complex in Study 2 by examining these approaches in a specific context and application method (i.e., handling AVI data). For example, this time we found that the selection approach did not affect process-focused reactions, and we found cases in which the augmented approach was viewed less favorably than the AI/ML-based approach (i.e., at higher levels of familiarity with AI). The complexity of our findings highlights the myriad influences of the surrounding context on applicant reactions toward AI/ML (for a broader discussion of context and replication, see [Iso-Ahola, 2020](#)).

## 5. General discussion

Across two studies, we experimentally investigated several factors that may influence applicant reactions toward AI/ML-based hiring processes. We first explored two aspects of developing and implementing AI/ML: (a) by using an augmented selection approach – in which AI/ML is used to augment (rather than replace) human decision-making – and (b) the selection stage in which any given approach was used. We then further explored reactions to the augmented approach within a specific context and application method. Across both studies, we also investigated applicants’ familiarity with AI as a moderating factor. While we discussed specific findings in each study’s discussion section, we focus here on broader themes.

First, our findings generally support the use of the augmented approach as a way for organizations to use AI/ML during employee selection in a manner that is more palatable to job applicants. Research indicates that AI/ML-based processes are typically evaluated unfavorably (Langer et al., 2019), are viewed as unfair and depersonalizing (Gonzalez et al., 2019), and can undermine intentions to pursue working for the companies that use these processes (Mirowska, 2020), compared to the same process being managed by a human decision-maker. Job applicants may perceive human decision-makers as “safeguards” against AI/ML, in that human decision-makers may notice idiosyncratic information that the algorithm would not otherwise detect, and they may allow candidates to have more personalized interactions and exercise control during the selection process. Indeed, we found evidence that augmented approaches can sometimes be perceived as procedurally fairer, foster greater perceptions of self-efficacy, and increase the likelihood that a selection approach will result in an accepted job offer.

We advocate for organizations that use AI/ML for streamlining the personnel decision-making process to consider the extent to which human decision-makers are included in these processes, as well as whether applicants are aware of this inclusion. In doing so, we also urge organizations to reflect on the context in which they might plan to use an augmented approach, such as generally what level of experience their applicants tend to have with AI, the industry norms surrounding technology use, and for what application method AI/ML is being used (e.g., analyzing asynchronous video interview data). There might be specific points in the hiring process in which human involvement (or knowledge of such involvement) is particularly critical. For example, our Study 1 data indicate that augmented approaches can help to enhance applicants’ perceptions of control in the final selection stage. Likewise, researchers have found that job applicants are particularly sensitive to the use of AI during interview processes (e.g., Folger, Brosi, Stumpf-Wollersheim, & Welpe, 2021; Wesche & Sonderegger, 2021), which suggests that augmented approaches may serve as a potential solution (although we will caveat this possibility by discussing familiarity with AI shortly).

Second, researchers may consider whether the issue of adverse reactions necessarily lies with making AI/ML more explainable, rather

than familiarizing stakeholders with how AI/ML works and why it is being used. Our data suggest that greater familiarity may lead people to react similarly toward AI/ML-based, augmented, and human-based selection processes in some cases, although we also show in Study 2 that this can backfire for the augmented approach in certain contexts. Familiarizing stakeholders may take some of the “mystique” out of AI/ML (Johnson & Verdichio, 2017), and may help stakeholders to overcome potential misconceptions about it, such as those coming from popular media (Garvey, 2019). Thus, while there are increasing calls for interdisciplinary collaboration in AI research (Langer et al., 2021), we add our own call for increased engagement with members of the broader public from researchers who study AI.

Third, our research highlights the potential communicative signals that come from using AI/ML in hiring processes. Specifically, consistent with signaling theory (Bangerter et al., 2012; Spence, 1973), our data reveal that applicants can extract a wide array of information from companies employing AI/ML and/or human decision-makers in their hiring processes. These inferences include those pertaining to the organization’s culture (e.g., their value of their human capital) and those pertaining to aspects of the procedure itself (e.g., its accuracy, potential for process control, likelihood of respectful treatment). Organizations may consider what signals they are sending through different aspects of their hiring processes, such as what technologies are used and who is (or is not) involved in the process, and develop strategies to manage applicants’ perceptions throughout the process.

Fourth, the effects of the augmented approach generally do not seem to depend on the selection stage of the process. This finding is particularly important, in that it suggests that adverse reactions toward AI/ML can emerge in both early and late selection stages alike. These findings run contrary to referent cognitions theory (Folger, 1987), in that applicants’ proximity to the final selection decision did not influence how strongly they reacted toward varying degrees of automation (which, as we found, can covary with perceptions of unfairness). Organizations may more frequently use AI/ML at earlier selection stages because it can more efficiently analyze and screen out large volumes of applicants than a human decision-maker (Liem et al., 2018). Based on our findings, we suggest that organizations that use AI/ML consider having safeguards in place even at earlier stages of the process, such as by appointing a human decision-maker (e.g., an HR manager or recruiter) to interface with applicants at these earlier stages, especially if these applicants have less familiarity with AI/ML.

Lastly, we found that there is a great deal of nuance in how people react toward AI/ML-based, human-based, and augmented selection processes. Specifically, our data suggest that reactions toward these different processes depends on (a) the specific type of reaction being considered (e.g., process-directed, company-directed, self-directed), (b) aspects of the applicant, such as their familiarity or experience with AI/ML, and (c) the broader context and manner in which the process is applied. While research in this area is still young, relative to the broader applicant reactions literature (although, see Bauer et al., 2011; McCarthy et al., 2017), we suggest that research should explore more of these nuances and their respective underlying mechanisms. Doing so will enable organizations to better design selection systems that foster a pleasant candidate experience. For example, reactions toward solely AI/ML-based processes may be less of a concern when recruiting applicants who have backgrounds in computer/data science or information technology, given that people with greater familiarity about AI reacted no differently toward fully-automated and human-based systems in our research.

### 5.1. Limitations and future research

Here we cover four limitations of the current research. First, we relied on self-reported measures in our experiments, which may make our data susceptible to biases such as social desirability (Fisher, 1993) and careless responding (e.g., Curran, 2016; Kam & Meyer, 2015). We

believe that self-report measures were necessary to use, given that many applicant reactions are perceptual in nature, and would be difficult to assess without directly asking participants. Because participants responded to hypothetical scenarios, and because we did not ask overly sensitive questions, we do not believe that our study introduced excessive pressure to engage in response distortion. Furthermore, we took steps to detect and remove data from potentially inattentive or careless respondents in all our studies. Second, our studies were all cross-sectional, which could inflate relationships between our variables due to common method variance (Podsakoff et al., 2003). While our dependent variables were generally correlated with one another (see supplemental materials for correlations), many of these relationships were small to medium in size, and we observed different patterns of results across the various reactions, indicating that each of measure should be assessing a different construct.

Third, we used vignettes in our studies, which could arguably reduce the external validity of our findings. Participants imagined themselves in a hiring process, rather than experiencing a situation with greater fidelity to the factors we sought to manipulate. As a result, there could be alternative explanations for why participants responded to our measures the way they did, such as by relying more heavily on their preconceived notions of AI than they normally would in an actual situation involving AI. Despite this limitation, vignette methodologies also lend two major advantages (for a full discussion, see Aguinis & Bradley, 2014). Specifically, (a) vignettes often allow for a better examination of causality for phenomena that are difficult to naturally observe (i.e., applicant reactions in the middle of a hiring process), and (b) vignettes offer high levels of internal validity by allowing researchers to better control for potential confounding variables. Furthermore, our findings comparing the solely human-based and AI/ML-based approaches generally align with those of the extant research on reactions toward AI (for a review, see Langer & Landers, 2021). We thus believe that our use of vignettes in the current research was appropriate, though we encourage future research that uses additional methodologies.

Lastly, in both studies, we collected data from non-applicant samples (undergraduate students in Study 1 and Prolific panelists in Study 2), which may affect our ability to generalize to a job applicant population. However, we screened out prospective participants who lacked current or past work experience and, in Study 2, recruited participants with experience working (and thus applying for jobs) in the same industrial context in which we tested our hypotheses, which should increase the representativeness of our samples. Nevertheless, based on these limitations, future research should include other methodologies (e.g., longitudinal, experience sampling, field experiments), measurement approaches (e.g., multi-source, behavioral, or physiological data), and samples (i.e., actual job applicants). Additionally, future research could recruit samples that are diverse in other ways, such as in their socioeconomic status, education levels, professions, or nationalities, which will further enhance generalizability.

We also offer future research directions, beyond those described earlier. First, while we examined augmented selection approaches in a broad sense, researchers may consider investigating different types of augmented approaches. Augmented processes can take on many forms. As examples, organizations could (a) delegate communications to recruiters or hiring managers, and automate the rest of the process using AI/ML, (b) have human decision-makers primarily manage the hiring process, and use AI/ML to synthesize data and generate candidate profiles for human decision-makers to act upon, or (c) use AI/ML to integrate judgments from several human decision-makers, and generate a final decision based on those judgments (e.g., synthesizing ratings from a panel of interviewers). Researchers may consider exploring how applicants react to these different processes, and what mechanisms drive these reactions.

Second, while we examined the effects of the augmented approach on applicant reactions, research is also needed regarding the implications of the augmented approach for selection system validity. It is

possible that a human decision-maker could potentially introduce bias into the process, such as by disregarding the recommendations of the AI/ML system and instead relying on their intuition. This possibility opens up a new array of potential research questions, such as what factors might influence how human decision-makers interact with AI/ML-based selection tools to make decisions, what factors affect the ultimate decision quality in augmented approaches, and whether (and under what conditions) an augmented approach leads to better, worse, or similar validity as their solely human-based or AI/ML-based equivalent.<sup>17</sup>

Third, while the augmented approach generally yielded more favorable applicant reactions than a purely AI/ML-based approach, the human-based approach led to the most favorable reactions. Researchers may consider exploring additional ways to use AI/ML during the hiring process in a way that is equally palatable to applicants as a solely human-based approach. Lastly, by examining different types of applicant reactions, we found that one size does not fit all; augmented approaches improved reactions toward AI/ML in some cases, and fared no better in other cases. Researchers may benefit from further exploring what aspects of the selection process influence these different reactions, and why.

### 5.2. Conclusions

We encourage organizations that use AI/ML in employee selection to consider the suitability of an augmented approach for their hiring systems. In appropriate contexts, including human decision-makers in the process may signal to applicants that they are being treated fairly and respectfully, and that they have some control over their outcomes in the process. We encourage researchers and practitioners to partner in further exploring the possible synergies between humans and AI/ML, so as to benefit organizations, employees, and applicants alike.

## Author credit statement

**Manuel F. Gonzalez:** Conceptualization, Data curation, Investigation, Methodology, Project administration, Writing – original draft, Writing – review & editing, **Weiwai Liu:** Conceptualization, Methodology, Writing – review & editing, **Lei Shirase:** Conceptualization, Methodology, Writing – review & editing, **David L. Tomczak:** Conceptualization, Methodology, Writing – review & editing, **Carmen E. Lobbe:** Conceptualization, Methodology, Writing – review & editing, **Richard Justenhoven:** Conceptualization, Supervision, **Nicholas R. Martin:** Conceptualization, Supervision

## Declaration of competing interest

All authors of the current manuscript confirm that they have no competing interests to report regarding the reported research and its findings.

## Appendix A. Supplementary data

Supplementary data to this article can be found online at <https://doi.org/10.1016/j.chb.2022.107179>.

## References

- Acikgoz, Y., Davison, K. H., Compagnone, M., & Laske, M. (2020). Justice perceptions of artificial intelligence in selection. *International Journal of Selection and Assessment*, 28, 399–416. <https://doi.org/10.1111/ijsa.12306>
- Adadi, A., & Berrada, M. (2018). Peeking inside the black-box: A survey on explainable artificial intelligence (XAI). *IEEE Access*, 6, 52138–52160. <https://doi.org/10.1109/ACCESS.2018.2870052>
- Aguinis, H., & Bradley, K. J. (2014). Best practice recommendations for designing and implementing experimental vignette methodology studies. *Organizational Research Methods*, 17(4), 351–371.
- Albert, E. T. (2019). AI in talent acquisition: A review of AI-applications used in recruitment and selection. *Strategic HR Review*, 18(5), 215–221. <https://doi.org/10.1108/SHR-04-2019-0024>
- Alexander, L., Mulfinger, E., & Oswald, F. L. (2020). Using big data and machine learning in personality measurement: Opportunities and challenges. *European Journal of Personality*, 34(5), 632–648. <https://doi.org/10.1002/per.2305>
- Alpaydin, E. (2020). *Introduction to machine learning* (4th ed.). The MIT Press.
- Armer, P. (1995). Attitudes toward intelligent machines. In E. Feigenbaum, & J. Feldman (Eds.), *Computers and thought* (pp. 389–405). McGraw-Hill.
- Auth, G., Jokisch, O., & Durk, C. (2019). Revisiting automated project management in the digital age – a survey of AI approaches. *Online Journal of Applied Knowledge Management*, 7(1), 27–39. <https://doi.org/10.36965/OJAKM.2019.7.127-39>
- Bangerter, A., Roulin, N., & König, C. J. (2012). Personnel selection as a signaling game. *Journal of Applied Psychology*, 97, 719–738. <https://doi.org/10.1037/a0026078>
- Basch, J. M., & Melchers, K. G. (2019). Fair and flexible?! Explanations can improve applicant reactions toward asynchronous video interviews. *Personnel Assessment and Decisions*, 5(3). <https://doi.org/10.25035/pad.2019.03.002>. Article 2.
- Bauer, T. N., Maertz, J. C. P., Dolen, M. R., & Campion, M. A. (1998). Longitudinal assessment of applicant reactions to employment testing and test outcome feedback. *Journal of Applied Psychology*, 83, 892–903. <https://doi.org/10.1037/0021-9010.83.6.892>
- Bauer, T. N., Truxillo, D., Mack, K., & Costa, A. B. (2011). Applicant reactions to technology-based selection: What we know so far. In N. T. Tippins, S. Adler, & A. I. Kraut (Eds.), *Technology enhanced assessment of talent* (pp. 190–223). Jossey-Bass.
- Bieling, P. J., Antony, M. M., & Swinson, R. P. (1998). The state-trait anxiety inventory, trait version: Structure and content re-examined. *Behaviour Research and Therapy*, 36 (7–8), 777–788.
- Bies, R. J. (2015). Interactional justice: Looking backward, looking forward. In R. S. Cropanzano, & M. L. Ambrose (Eds.), *The Oxford handbook of justice in the workplace*. Oxford University Press.
- Bies, R. J., & Moagi, J. F. (1986). Interactional justice: Communication criteria of fairness. In R. J. Lewicki, B. H. Sheppard, & M. H. Bazerman (Eds.), *Research on negotiations in organizations* (Vol. 1, pp. 43–55). JAI Press.
- Black, J. S., & van Esch, P. (2020). AI-enabled recruiting: What is it and how should a manager use it? *Business Horizons*, 63(2), 215–226. <https://doi.org/10.1016/j.bushor.2019.12.001>
- van den Bos, K., & van Prooijen, J.-W. (2001). Referent Cognitions Theory: The role of closeness of reference points in the psychology of voice. *Journal of Personality and Social Psychology*, 81(4), 616–626. <https://doi.org/10.1037/0022-3514.81.4.616>
- Boyne, G. A., John, P., James, O., & Petrovsky, N. (2011). Top management turnover and organizational performance: A test of a contingency model. *Public Administration Review*, 71(4), 572–581. <https://doi.org/10.1111/j.1540-6210.2011.02389.x>
- Breiman, L. (2001). Statistical modeling: The two cultures. *Statistical Science*, 16(3), 199–215.
- Brockner, J., & Wiesenfeld, B. (1996). An integrative framework for explaining reactions to decisions: Interactive effects of outcomes and procedures. *Psychological Bulletin*, 120, 189–208.
- Cable, D. M., & DeRue, D. S. (2002). The convergent and discriminant validity of subjective fit perceptions. *Journal of Applied Psychology*, 87(5), 875–884.
- Campbell, J. W., Im, T., & Jeong, J. (2014). Internal efficiency and turnover intention: Evidence from local government in South Korea. *Public Personnel Management*, 43(2), 259–282.
- Cohen, J., & Cohen, P. (1983). *Applied multiple regression/correlation analysis for the behavioral sciences* (2nd ed.). Lawrence Erlbaum Associates.
- Colquitt, J. A. (2001). On the dimensionality of organizational justice: A construct validation of a measure. *Journal of Applied Psychology*, 86(3), 386–400.
- Colquitt, J. A., Greenberg, J., & Zapata-Phelan, C. P. (2005). What is organizational justice? A historical overview. In J. Greenberg, & J. A. Colquitt (Eds.), *Handbook of organizational justice* (pp. 3–58). Lawrence Erlbaum Associates.
- Cropanzano, R., & Folger, R. (1989). Referent cognitions and task decision autonomy: Beyond equity theory. *Journal of Applied Psychology*, 74(2), 293–299.
- Crosby, P. J. (1976). A model of egoistic relative deprivation. *Psychological Review*, 83, 85–113.
- Cryder, C. E., Loewenstein, G., & Seltman, H. (2013). Goal gradient in helping behavior. *Journal of Experimental Social Psychology*, 49(6), 1078–1083. <https://doi.org/10.1016/j.jesp.2013.07.003>
- Curran, P. G. (2016). Methods for the detection of carelessly invalid responses in survey data. *Journal of Experimental Social Psychology*, 66, 4–19. <https://doi.org/10.1016/j.jesp.2015.07.006>
- Das, P., Pandey, M., & Rautaray, S. S. (2018). A CV parser model using entity extraction process and big data tools. *L.J. Information Technology and Computer Science*, 9, 21–31. <https://doi.org/10.5815/ijtits.2018.09.03>
- Deci, E. L., & Ryan, R. M. (2012). Self-determination theory. In P. M. Van Lange, A. W. Kruglanski, & E. T. Higgins (Eds.), *Handbook of theories of social psychology* (Vol. 1, pp. 416–436). Sage Publications, Ltd.
- Deci, E. L., & Ryan, A. M. (2017). Self-determination theory in work organizations: The state of a science. *Annual Review of Organizational Psychology and Organizational Behavior*, 4, 19–43. <https://doi.org/10.1146/annurev-orgpsych-032516-113108>
- Epstein, S. L. (2015). Wanted: Collaborative intelligence. *Artificial Intelligence*, 221(1), 36–45. <https://doi.org/10.1016/j.artint.2014.12.006>
- Equal Employment Opportunity Commission. (1978). *Uniform Guidelines on Employee Selection Procedures* 28 C.F.R. § 5.14.

<sup>17</sup> We are grateful to an anonymous reviewer for raising this point.

Equal Employment Opportunity Commission. (2016). *Big data in the workplace: Examining implications for equal employment opportunity law*.

Faul, F., Erdfelder, E., Lang, A., & Buchner, A. (2007). G\*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. *Behavior Research Methods*, 39, 175–191.

Fisher, R. J. (1993). Social desirability bias and the validity of indirect questioning. *Journal of Consumer Research*, 20, 303–315.

Folger, N., Brosi, P., Stumpf-Wollersheim, J., & Welpe, I. M. (2021). Applicant reactions to digital selection methods: A signaling perspective on innovativeness and procedural justice. *Journal of Business and Psychology*, 2020.100770/101089-021-09770-3

Folger, R. (1987). Reformulating the preconditions of resentment: A referent cognitions model. In J. C. Masters, & W. P. Smith (Eds.), *Social comparison, social justice, and relative deprivation* (pp. 183–215). Lawrence Erlbaum Associates, Publishers.

Gagné, M., & Deci, E. L. (2005). Self-determination theory and work motivation. *Journal of Organizational Behavior*, 26(4), 331–362.

Garvey, S. C. (2019). Hypothesis: Is “terminator syndrome” a barrier to democratizing artificial intelligence and public engagement in digital health? *Journal of Integrative Biology*, 23(7), 362–363. <https://doi.org/10.1089/oml.2019.0070>

General Data Protection Regulation. (2016). Regulation (EU) 2016/679 of the European Parliament and of the council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46. *Official Journal of the European Union*, 59, 1–88.

Gilliland, S. W. (1993). The perceived fairness of selection systems: An organizational justice perspective. *Academy of Management Review*, 18, 694–734.

Gilmore, D. C., & Ferris, G. R. (1989). The effects of applicant impression management tactics on interviewer judgments. *Journal of Management*, 4, 557–564.

Glikson, E., & Woolley, A. W. (2020). Human trust in artificial intelligence: Review of empirical research. *The Academy of Management Annals*, 14(2), 627–660. <https://doi.org/10.5465/annals.2018.0057>

Goldberg, K. (2019). Robots and the return to collaborative intelligence. *Nature Machine Intelligence*, (1), 2–4. <https://doi.org/10.1038/s42256-018-0008-x>

Goldberg, C. B., & Allen, D. G. (2008). Black and white and read all over: Race differences in reactions to recruitment web sites. *Human Resource Management*, 47, 217–236.

Gonzalez, M. F., Capman, J. F., Oswald, F. L., Theys, E. R., & Tomczak, D. L. (2019). Where's the I-O? Artificial intelligence and machine learning in talent management systems. *Personnel Assessment and Decisions*, 5(3), 33–44.

Hausknecht, J. P., Day, D. V., & Thomas, S. C. (2004). Applicant reactions to selection procedures: An updated model and meta-analysis. *Personnel Psychology*, 57, 639–683.

Highhouse, S. (1997). Understanding and improving job-finalist choice: The relevance of behavioral decision research. *Human Resource Management Review*, 7, 449–470. [https://doi.org/10.1016/S1053-4822\(97\)90029-2](https://doi.org/10.1016/S1053-4822(97)90029-2)

Hoeff, R. M., Kochan, J. A., & Jentsch, F. (2006). Automated systems in the cockpit: Is the autopilot, “george,” a team member? In C. Bowers, E. Salas, & F. Jentsch (Eds.), *Creating high-tech teams: Practical guidance on work performance and technology* (pp. 243–259). <https://doi.org/10.1037/11263-011>

S. L. S. Indira, D. N. V., & Kumar, R. K. (2016). Profile screening and recommendation using natural language processing (NLP) and leveraging Hadoop framework for Bigdata International Journal of Computer Science and Information Security, 14, 799–811.

Iso-Ahola, S. E. (2020). Replication and the establishment of scientific truth. *Frontiers in Psychology*, 11. <https://doi.org/10.3389/fpsyg.2020.02183>. Article 2183.

Jarrahi, M. S. (2018). Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making. *Business Horizons*, 61, 577–586. <https://doi.org/10.1016/j.bushor.2018.03.007>

Johnson, D. G., & Verdichio, M. (2017). Reframing AI discourse. *Minds & Machines*, 27, 575–590.

Kahneman, D., & Tversky, A. (1982). Availability and the simulation heuristic. In D. Kahneman, P. Slovic, & A. Tversky (Eds.), *Judgment under uncertainty: Heuristics and biases* (pp. 201–208). Oxford University Press.

Kam, C. C. S., & Meyer, J. P. (2015). How careless responding and acquiescence response bias can influence construct dimensionality: The case of job satisfaction. *Organizational Research Methods*, 18(3), 512–541. <https://doi.org/10.1177/109442811571894>. July 1, 2015.

Kashdan, T. B., Gallagher, M. W., Silva, P. J., Winterstein, B. P., Breen, W. E., Terhar, D., & Steger, M. F. (2009). The Curiosity and Exploration Inventory-II: Development, factor structure, and psychometrics. *Journal of Research in Personality*, 43, 987–998.

Kenny, D. A. (2020). Measuring model fit. Retrieved from <http://avidakenny.net/c/mfit.htm>.

Koivunen, S., Olsson, T., Olshannikova, E., & Lindberg, A. (2019). Understanding decision-making in recruitment: Opportunities and challenges for information technology. *PACM on Human-Computer Interaction*, 3. <https://doi.org/10.1145/3361123>. Article 242.

Kuncel, N. R., Klieger, D. M., Connelly, B. S., & Ones, D. S. (2013). Mechanical versus clinical data combination in selection and admissions decisions: A meta-analysis. *Journal of Applied Psychology*, 98(6), 1060–1072.

Landers, R. N., & Marin, S. (2021). Theory and technology in organizational psychology: A review of technology integration paradigms and their effects on the validity of theory. *Annual Review of Organizational Psychology and Organizational Behavior*, 8, 235–258. <https://doi.org/10.1146/annurev-orgpsych-012420-060843>

Langer, M., König, C. J., & Papathanasiou, M. (2019). Highly automated job interviews: Acceptance under the influence of stakes. *International Journal of Selection & Assessment*, 27, 217–234.

Langer, M., König, C. J., Sanchez, D. R. P., & Samadi, S. (2020). Highly automated interviews: Applicant reactions and the organizational context. *Journal of Managerial Psychology*, 35(4), 301–314. <https://doi.org/10.1108/JMP-09-2018-0402>

Langer, M., & Landers, R. N. (2021). The future of artificial intelligence at work: A review on effects of decision automation and augmentation on workers targeted by algorithms and third-party observers. *Computers in Human Behavior*, 123, 106878. <https://doi.org/10.1016/j.chb.2021.106878>

Langer, M., Oster, D., Speith, T., Hermanns, H., Kastner, L., Schmidt, E., Sesing, A., & Baum, K. (2021). What do we want from explainable artificial intelligence (XAI)? – a stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research. *Artificial Intelligence*, 296. <https://doi.org/10.1016/j.artint.2021.103473>. Article 103473.

Laurim, V., Arpacı, S., Prommegger, B., & Krcmar, H. (2021). Computer, whom should I hire? – acceptance criteria for artificial intelligence in the recruitment process. In *Proceedings of the 54th Hawaii international conference on system sciences*. <https://doi.org/10.24251/HICSS.2021.668>

Lawrence, T. (1991). Impacts of artificial intelligence on organizational decision-making. *Journal of Behavioral Decision Making*, 4(3), 195–214. <https://doi.org/10.1002/jbdm.3960040306>

Lee, N. T. (2018). Detecting racial bias in algorithms and machine learning. *Journal of Information, Communication and Ethics in Society*, 16(3), 252–260. <https://doi.org/10.1108/JICES-06-2018-0056>

Leventhal, G. S. (1980). What should be done with equity theory? New approaches to the study of fairness in social relationships. In K. J. Gerger, M. S. Greenberg, & R. H. Willis (Eds.), *Social exchange: Advances in theory and research* (pp. 27–55). Plenum.

Liem, C. C. S., Langer, M., Demetriou, A., Hiemstra, A. M. F., Sukma Wicaksana, A., Born, M. P., & König, C. J. (2018). Psychology meets machine learning: Interdisciplinary perspectives on algorithmic job candidate screening. In H. J. Escalante, S. Escalera, I. Guyon, X. Baró, Y. Güçlütürk, U. Güçlü, & M. van Gerven (Eds.), *Explainable and interpretable models in computer vision and machine learning* (pp. 197–253). Springer International Publishing. [https://doi.org/10.1007/978-3-319-98131-4\\_9](https://doi.org/10.1007/978-3-319-98131-4_9)

Lind, E. A. (2001). Fairness heuristic theory: Justice judgments as pivotal cognitions in organizational relations. In J. Greenberg, & R. Cropanzano (Eds.), *Advances in organizational justice* (pp. 56–88). Stanford University Press.

Lind, E. A., & Tyler, T. R. (1988). *The social psychology of procedural justice*. Plenum.

Lind, E. A., & Van den Bos, K. (2002). When fairness works: Toward a general theory of uncertainty management. *Research in Organizational Behavior*, 24, 181–223.

Lukacik, E., Bourdage, J. S., & Roulin, N. (2022). Into the void: A conceptual model and research agenda for the design and use of asynchronous video interviews. *Human Resource Management Review*, 32(1), 100789. <https://doi.org/10.1016/j.hrmr.2020.100789>

Malone, T. W., & Bernstein, M. S. (2015). Introduction. In T. W. Malone, & M. S. Bernstein (Eds.), *Handbook of collective intelligence* (pp. 1–14). The MIT Press.

McCarthy, J. M., Bauer, T. N., Truxillo, D., Anderson, N., Costa, A. C., & Ahmed, S. M. (2017). Applicant perspectives during selection: A review addressing “so what?”, “what’s new?”, and “where to next?”. *Journal of Management*, 43(6), 1693–1725.

Mencel, J., & May, D. R. (2009). The effects of proximity and empathy on ethical decision-making: An exploratory investigation. *Journal of Business Ethics*, 85(2), 201–226. <https://doi.org/10.1007/s10551-008-9765-5>

Milliken, F. J. (1987). Three types of perceived uncertainty about the environment: State, effect, and response uncertainty. *Academy of Management Review*, 12(1), 133–143.

Mirowska, A. (2020). AI evaluation in selection: Effects on application and pursuit intentions. *Personnel Psychology*, 19(3), 142–149. <https://doi.org/10.1027/1866-5888/a000258>

Nilsson, N. J. (2014). *Principles of artificial intelligence*. Morgan Kaufman Publishers, Inc.

Noutsi, E., Fafalios, P., Gadiraju, U., Iosifidis, V., Nejdi, W., Vidal, M., Ruggieri, S., Turini, F., Papadopoulos, S., Krasanakis, E., Kompatisari, I., Kinder-Kurlanda, K., Wagner, C., Karimi, F., Fernandez, M., Alani, H., Berendt, B., Kruegel, T., Heinze, C., ... Staab, S. (2019). Bias in data-driven artificial intelligence systems—an introductory survey. *WIREs Data Mining and Knowledge Discovery*, 10(3). <https://doi.org/10.1002/widm.1356>

Olsen, K. A., & Malizia, A. (2011). Automated personal assistants. *Computer*, 44(11), 110–112. <https://doi.org/10.1109/MC.2011.329>

Oswald, F. L., Behrend, T. S., Putka, D. J., & Sinar, E. F. (2020). Big data in industrial-organizational psychology and human resource management: Forward progress for organizational research and practice. *Annual Review of Organizational Psychology and Organizational Behavior*, 7, 505–533. <https://doi.org/10.1146/annurev-orgpsych-032117-104553>

Parasuraman, R., Sheridan, T. B., & Wickens, C. D. (2000). A model for types and levels of human interaction with automation. *IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans*, 30(3), 286–297. <https://doi.org/10.1109/3468.844354>

Park, G., Schwarz, H. A., Eichstaedt, J. C., Kern, M. L., Kosinski, M., Stillwell, D. M., Ungar, L. H., & Seligman, M. E. P. (2015). Automatic personality assessment through social media language. *Journal of Personality and Social Psychology*, 108, 934–952. <https://doi.org/10.1037/ppp000020>

Paschen, J., Wilson, M., & Ferreira, J. J. (2020). Collaborative intelligence: How human and artificial intelligence create value along the B2B sales funnel. *Business Horizons*, 63(3), 403–414. <https://doi.org/10.1016/j.bushor.2020.01.003>

Podsakoff, P. M., MacKenzie, S. B., Lee, J. Y., & Podsakoff, N. P. (2003). Common method biases in behavioral research: A critical review of the literature and recommended remedies. *Journal of Applied Psychology*, 88(5), 879–903.

Preacher, K. J., Curran, P. J., & Bauer, D. J. (2006). Computational tools for probing interactions in multiple linear regression, multilevel modeling, and latent curve analysis. *Journal of Educational and Behavioral Statistics, 31*(3), 437–448. <https://doi.org/10.1177/1094428117697041>

Putka, D. J., Beatty, A. S., & Reeder, M. C. (2018). Modern prediction methods: New perspectives on a common problem. *Organizational Research Methods, 21*, 689–732. <https://doi.org/10.1177/1094428117697041>

Rigdon, E. E. (1996). CFI versus RMSEA: A comparison of two fit indexes for structural equation modeling. *Structural Equation Modeling: A Multidisciplinary Journal, 3*(4), 369–379. <https://doi.org/10.1080/10705519609540052>

Sajjadiani, S., Sojourner, A. J., Kammerer-Mueller, J. D., & Mykerezi, E. (2019). Using machine learning to translate applicant work history into predictors of performance and turnover. *Journal of Applied Psychology, 104*, 1207–1225. <https://doi.org/10.1037/apl0000405>

Saks, A. M., & Ashforth, B. E. (1999). Effects of individual differences and job search behaviors on the employment status of recent university graduates. *Journal of Vocational Behavior, 54*(2), 335–349.

Schlicker, N., Langer, M., Ötting, S. K., Baum, K., König, C. J., & Wallach, D. (2021). What to expect from opening up 'black boxes'? Comparing perceptions of justice between human and automated agents. *Computers in Human Behavior, 122*, 106837. <https://doi.org/10.1016/j.chb.2021.106837>

Seebert, I., Bittner, E., Briggs, R. O., de Vreede, T., de Vreede, G.-J., Elkins, A., Maier, R., Merz, A. B., Oeste-Reiß, S., Randrup, N., Schwabe, G., & Söllner, M. (2020). Machines as teammates: A research agenda on AI in team collaboration. *Information & Management, 57*. <https://doi.org/10.1016/j.im.2019.103174>

Shank, D. B., & DeSanti, A. (2018). Contributions of morality and mind to artificial intelligence after real-world moral violations. *Computers in Human Behavior, 86*, 401–411.

Shao, Z., Feng, Y., & Liu, L. (2012). The mediating effect of organizational culture and knowledge sharing on transformational leadership and Enterprise Resource Planning systems success: An empirical study in China. *Computers in Human Behavior, 28*(6), 2400–2413.

Sheldon, K. M., Elliot, A. J., Kim, Y., & Kasser, T. (2001). What is satisfying about satisfying events? Testing 10 candidate psychological needs. *Journal of Personality and Social Psychology, 80*(2), 325–339.

Sinar, E. F. (2015). Data visualization. In S. Tonidandel, E. B. King, & J. M. Cortina (Eds.), *Big data at work: The data science revolution and organizational psychology* (pp. 115–157). Routledge.

Smithers, J. W., Reilly, R. R., Millsap, R. E., Pearlman, K., & Stoffly, R. W. (1993). Applicant reactions to selection procedures. *Personnel Psychology, 46*, 49–76.

Society for Industrial and Organizational Psychology. (2018). *Principles for the validation and use of personnel selection procedures* (5th). Cambridge University Press.

Spence, M. (1973). Job market signaling. *Quarterly Journal of Economics, 87*, 355–374.

Spielberger, C. D., Johnson, E. H., Russell, S. F., Crane, R. J., Jacobs, G. A., & Worden, T. J. (1985). The experience and expression of anger: Construction and validation of an anger expression scale. In M. A. Cheney, & R. H. Rosenman (Eds.), *Anger and hostility in cardiovascular and behavioral disorders* (pp. 5–30). McGraw-Hill.

Stephan, M., Brown, D., & Erickson, R. (2017). *Talent acquisition: Enter the cognitive recruiter*. <https://www2.deloitte.com/insights/us/en/focus/human-capital-trends/2017/predictive-hiring-talent-acquisition.html>. February 28.

Stone, D. L., Deadrick, D. L., Lukaszewski, K. M., & Johnson, R. (2015). The influence of technology on the future of human resource management. *Human Resource Management Review, 25*(2), 216–231. <https://doi.org/10.1016/j.hrmr.2015.01.002>

Stoughton, J. W., Thompson, L. F., & Meade, A. W. (2015). Examining applicant reactions to the use of social networking websites in pre-employment screening. *Journal of Business and Psychology, 30*(1), 73–88. <https://doi.org/10.1007/s10869-013-9333-6>

Suen, H.-Y., Chen, M. Y.-C., & Lu, S.-H. (2019). Does the use of synchrony and artificial intelligence in video interviews affect interview ratings and applicant attitudes? *Computers in Human Behavior, 98*, 93–101. <https://doi.org/10.1016/j.chb.2019.04.012>

Sun, T., Gait, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.-W., & Wang, W. Y. (2019). In *Proceedings of the association for computational linguistics (ACL)*. Florence, Italy: Mitigating Gender Bias in Natural Language Processing: Literature Review.

Tay, C., Ang, S., & Van Dyne, L. (2006). Personality, biographical characteristics, and job interview success: A longitudinal study of the mediating effects of interviewing self-efficacy and the moderating effects of internal locus of causality. *Journal of Applied Psychology, 91*(2), 446–454.

Thibaut, J., & Walker, L. (1975). *Procedural justice: A psychological analysis*. Erlbaum.

Truxillo, D., & Bauer, T. N. (2011). Applicant reactions to organizations and selection systems. In S. Zedeck (Ed.), *APA handbook of industrial and organizational psychology* (Vol. 2, pp. 379–397). American Psychological Association.

Tyler, T. R., Rasinski, K. A., & Spodick, N. (1985). Influence of voice on satisfaction with leaders: Exploring the meaning of process control. *Journal of Personality and Social Psychology, 48*(1), 72–81. <https://doi.org/10.1037/0022-3514.48.1.72>

Verhulst, S. G. (2018). Where and when AI and CI meet: Exploring the intersection of artificial and collective intelligence towards the goal of innovating how we govern. *AI & Society, 33*, 293–297.

Weintraub, A. (2017). Consider HIPAA when using A.I. & machine learning. *MedStack*. <https://medstack.co/blog/consider-hipaa-using-machine-learning/>

Wesche, J. S., & Sonderegger, A. (2021). Repelled at first sight? Expectations and intentions for job-seekers reading about AI selection in job advertisements. *Computers in Human Behavior, 125*, 106931. <https://doi.org/10.1016/j.chb.2021.106931>

Wilson, H. J., & Daugherty, P. R. (2018). Collaborative intelligence: Humans and AI are joining forces. *Harvard Business Review, 96*(4), 114–123.

Woolley, A. W., Chabris, C. F., Pentland, A., Hashmi, N., & Malone, T. W. (2010). Evidence for a collective intelligence factor in the performance of human groups. *Science, 330*(6004), 686–688. <https://doi.org/10.1126/science.1193147>

Xu, W. (2019). Toward human-centered AI: A perspective from human-computer interaction. *ACM, 26*(4), 42–46. <https://doi.org/10.1145/3328485>