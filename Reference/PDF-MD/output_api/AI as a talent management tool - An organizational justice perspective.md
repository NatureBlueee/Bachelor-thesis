

## Journal Pre-proof

AI as a talent management tool: An organizational justice perspective

Nathan Bennett, Christopher L. Martin

![Cover image of the journal Business Horizons, featuring a building and pink blossoms.](64662465bba247703fdec49c8f3309f9_img.jpg)

Cover image of the journal Business Horizons, featuring a building and pink blossoms.

PII: S0007-6813(25)00049-7

DOI: <https://doi.org/10.1016/j.bushor.2025.03.005>

Reference: BUSHOR 2058

To appear in: *Business Horizons*

Please cite this article as: Bennett N. & Martin C.L., AI as a talent management tool: An organizational justice perspective *Business Horizons*, <https://doi.org/10.1016/j.bushor.2025.03.005>.

This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.

© 2025 Kelley School of Business, Indiana University. Published by Elsevier Inc. All rights are reserved, including those for text and data mining, AI training, and similar technologies.

# **AI as a talent management tool: An organizational justice perspective**

Nathan Bennett \*  
Robinson College of Business  
Georgia State University  
35 Broad Street, NW  
Atlanta, GA 30303  
USA  
nate@gsu.edu

Christopher L. Martin  
College of Business  
Louisiana Tech University  
P.O. Box 10318  
Ruston, LA 71272  
USA  
cmartin@ltech.edu

\*Corresponding author

# **AI as a talent management tool:** **An organizational justice perspective**

## **Abstract**

This paper offers a discussion of the challenges organizational decision makers face because of the emerging role of artificial intelligence (AI) as a talent management tool. Already, AI has become a critical element in customer decision-making, as witnessed on customer-facing platforms like Netflix and Amazon. Companies are rapidly adopting AI to improve talent management practices such as recruitment, selection, and performance reviews. As the use of AI becomes more widespread, managers will need to be prepared to contend with what could be significant concerns voiced by the job applicants and employees whose livelihoods could be impacted. There is already anecdotal evidence to suggest employees are troubled with transparency, fairness, and the potential for bias in AI algorithms and as a result may see its use as unfair. It is well-established by decades of research that employees who experience or witness unfair treatment display undesirable attitudinal and behavioral characteristics. To assist managers in their efforts to deploy AI in a manner employees will embrace rather than resist, organizational justice theory is used to develop recommendations for anticipating, understanding, and addressing employee reactions to talent management decisions based on AI.

**KEYWORDS:** Artificial intelligence; Talent management; Justice and AI

## 1. Artificial intelligence as a talent management tool

By learning decision-makers' needs and wants, processing large volumes of data effectively, and considering many possible scenarios, AI technology offers the potential to understand customers, markets, and internal business processes more deeply. In our personal lives, we comfortably rely on algorithms to recommend movies on Netflix or products from Amazon. Netflix reports that 80% of viewer selections are based on its recommendation engine; 35% of purchases on the Amazon platform are of AI-recommended products (Krysiak, 2024; Harrigan, 2022). Nearly half of US drivers report they have become dependent on GPS to reach their destinations (United Tires Library, 2024).

Anticipation regarding the introduction of AI to the workplace has been voiced for some time (see Bennett, Lemoine, & Molnar, 2024; Holloway & Hand, 1988) and its use is now emerging as an undeniable element in the lives of managers and employees. It has been reported that perhaps as many as 70% of employers plan to adopt AI as a recruiting tool – without human oversight (Dungan, 2024). Why businesses and individuals are rapidly getting into the AI game is easy to understand. Who among us is not interested in making quicker, better decisions? This temptation in the face of considerable hype around the potential value created through the clever use of AI makes it essential for leaders to develop and act on an understanding of the ways job applicants and current employees perceive its fairness. Caution is warranted; the last thing a manager needs is another kerfuffle such as that surrounding current efforts to reign in remote work (Mortenson, 2023).

The potential workplace applications of AI are vast, and as a result, its implications for job applicants and employees are possibly profound (Leyer & Schneider, 2021). Not surprisingly, employees are expressing concern about the ways employers are considering using AI to make decisions about human resource management actions (Rainie, Anderson, McClain, Vogels, & Gelles-Watnick, 2023). Of particular interest is the degree to which human decision makers remain in the loop. In some cases, potential exists for AI to entirely replace a human decision maker. Once trained, an algorithm can operate without supervision to quickly identify optimal outcomes. In other cases, the intent of AI implementation is to offer decision makers a powerful tool designed to inform their expert judgements. The adoption of AI and the role of the human decision maker presents an interesting irony when it comes to justice judgements. On the one hand, an argument in support of AI is that algorithms can be developed that are free from the bias a human would inevitably introduce. On the other, that much of what AI 'does' is obscured from view leads employees to distrust unless a human is in position to supervise the process and to intervene where appropriate. Of course, this sets aside entirely the reality that early results of AI deployment demonstrate that so far experts have failed to train bias out of the tool.

The observation that AI has incredible potential to disrupt how organizations operate and leaders lead is well-known and well-understood by both practicing managers and academic scholars. Thus, it is important to be clear about what this article offers. We provide a structured, theory-based approach that forward-thinking managers can adopt to increase employees' likelihood of accepting decisions informed by AI. This serves as a resource that can guide decision-makers as they try to cope with the momentum of the AI movement and the inevitable pressure not to fall behind in exploiting the tool. Managers need a foundation for constructively pausing to appreciate how employees will react when AI is employed as a talent management tool.

### 1.1. Talent management

Talent management has been a common phrase for more than two decades - since McKinsey & Company opined about the then burgeoning war over talent being waged among employers (Handfield-Jones, Michaels, & Axelrod (2001)).<sup>1</sup> The term quickly gained acceptance; it implied employees were a resource that could be a source of competitive advantage and thus should be optimized (e.g., Barney, 1991). Many businesses are now angling for early mover advantage in such optimization efforts as they deploy AI to assist in talent management (McKinsey & Company, 2023; Stone, Lukaszewski, & Johnson, 2024). AI can be used to automate repetitive tasks such as resume screening, freeing recruiters up for more strategic work. In theory, AI algorithms can also be trained to quickly analyze resumes and job descriptions free of the influence of unconscious bias a human might otherwise introduce to the process. In learning and development, predictive analytics driven by AI can analyze employee data to forecast future career paths and recommend development plans to help employees reach their full potential. AI is also changing performance management through continuous feedback and performance monitoring. By focusing on data and metrics, AI should lead to more objective performance evaluations. Finally, AI is being deployed so that managers have a better understanding of areas of concern regarding employee engagement and retention. For example, sentiment analysis is used to identify issues impacting morale.

These AI tools for talent management are evolving quickly and gaining traction in the workplace (Gelinas, Sadreddin, & Vahidov, 2022). IBM, Oracle, and SAP are among many deploying AI solutions to improve their client's talent management outcomes. At Oracle, offerings branded as AI for Talent have been produced that automate tasks throughout the recruiting process and algorithms developed to produce estimates of a candidate's fit with open positions. According to the company's website, SAP's Business AI "makes every process more efficient, every decision more data-driven, and every employee more capable." (<https://tinyurl.com/ydydwacf>). Companies that provide talent to employers are also developing AI tools in creating their offerings. For example, companies such as Beamery, Workday, and HireVue employ AI to enhance learning and development, performance management, and recruiting processes. Beamery claims its TalentGPT can improve "every stage of the talent management lifecycle" by offering "personalized conversational experiences" and that it enables "better decisions throughout the talent lifecycle." (<https://beamery.com/>). These efforts are sometimes subject to controversy. HireVue, for example, has been decried as unfair and deceptive in its use of AI (Harwell, 2019). In late 2019, the Electronic Privacy Information Center filed a complaint with the FTC that alleged the company's facial recognition algorithms produced biased results and that insufficient information about how the algorithms operated had been offered to create transparency. More recently, an applicant screened out of more than 100 candidate pools by Workday's AI tools is seeking to create a class action suit based on age discrimination (Tornone, 2025).

<sup>1</sup> By talent management, we refer to the strategies and tactics managers use to do workforce planning, design jobs, recruit, onboard and offboard employees, and to design and administer compensation and benefits (Makarius, Dachner, Paluch, & Pedde, 2024). When it comes to today's talent management applications, machine learning (ML) is the form of AI most often deployed. Machine learning involves training algorithms on extensive datasets of employee information. By analyzing text-based data, such as those on resumes and in social media posts, natural language processing algorithms can extract relevant skills, qualifications, and experience. Through training on historical data, an algorithm can identify correlations and trends within the data to offer predictions of an applicant's or employee's future performance.

Ideally, deliberations around talent management will be more productive and decisions free of bias when informed by the vast amounts of data AI can process. Information that previously was difficult to summarize — as is typically found on resumes and in social media activity - might now be viewed by managers as useful. As employers become more enthralled with the potential upsides offered by AI, there is every reason to expect employees will express elevated concerns. After all, much of what they know about the technology is influenced by notorious examples of algorithms demonstrating bias. Further, issues around the tool's transparency and where accountability lies for addressing AI-generated errors will also be paramount. Any employer concerned with being perceived as fair in the treatment of employees will need to anticipate and attend to these employee concerns (Gilliland, 1993).

There is no doubt but that organizations already have a trove of data that could inform decisions around human resources (Russell & Bennett, 2014), but only about a quarter of employees express confidence in their employers' strategies for leveraging AI (Quantam Workplace, 2024) and managers are uncertain how AI will transform human resource management practices (Stone, et al., 2024). This lack of confidence is a strong motive for efforts to understand how employees will likely react to the further incursion of AI into talent management. Research on employee reactions to technological innovations for performance monitoring has shown that perceptions of fairness matter whenever machines are involved as decision agents (cf. Kidwell & Bennett, 1994; Otting & Maier, 2018). It has also been found that employees weigh heavily their perceptions of the fairness of algorithms used in recruiting (Ochmann, Michels, Tiefenbeck, Maier, & Laumer, 2024).

To this point, most researchers and pundits have chosen to focus on the ethical issues the technology raises (Andrieux, Johnson, Sarabadani, & Van Slyke, 2024; see Hunkenschroer & Luetge, 2022 for a review). Certainly, ethical considerations are foundational to an appreciation of what AI means in a workplace, but the focus on the implications of such deliberations on actual employee treatment – the focus of organizational justice research – offers more actionable advice to managers. Here, we draw on the well-established literature on organizational justice to highlight several concerns business leaders should carefully study in their search for AI solutions to talent management challenges.

### 1.2. Organizational justice theory

Organizational justice theory holds that employees are concerned with receiving fair outcomes and experiencing fair treatment in interactions with their employers. Decades of research show episodes of treatment perceived as unfair lead both the target of and witnesses to the treatment to feel justified in displaying a range of undesirable behaviors—from passive disengagement to active protest (Konovsky & Freeman, 2000). Conversely, treatment perceived as fair has been shown to build trust, heighten organizational commitment, and, in the aggregate, positively impact individual and organizational performance (Bennett, Martin, Bies, & Brockner, 1995). Just recently, researchers have begun to explore the implications of AI implementation for employee perceptions of organizational justice (e.g., Narayanan, Nagpal, McGuire, Schweitzer, & De Cremer, 2024; Qin, Jia, Luo, Xiao, & Huang, 2023).

Artificial intelligence in talent management decision-making at work could be deemed as unfair by employees for two major reasons (Kaplan & Haenlein, 2019; Breidbach, 2024). First, AI can

be used to create, aide, or replace processes already in place for making decisions. Characteristics of AI algorithms make it difficult for employees to determine if the new process is, in fact, any more worthy of trust than the previous process. Employees may not be satisfied with the quality of communication as to how results produced by AI lead decision-makers to their conclusions. Take, for example, Amazon's experience with its tool that produced bias against women applicants (Kim-Schmid & Ravendhran, 2022). Employees may need clarification on the factors the AI tool considers and how human decision-makers weigh its results. Because AI may make it difficult for employees to understand either a decision-making process or its resulting decision, it might be difficult for them to mount an appeal to unfairness. Relatedly, AI algorithms can perpetuate biases learned from the data used in training. For example, research has found training data leads large language models (LLM) to discriminate through assigning speakers of African American English to low status jobs (Hoffmann, Kallur, Jurafsky, & King, 2024). If employees or applicants are concerned about the integrity of the training data, they will feel unfairly judged. Effectively, any decision influenced by what takes place inside the black box of an AI algorithm becomes fruit of a poisoned tree. Finally, in the context of the employer-employee relationship, the impersonal nature of AI-driven decision-making could feel disrespectful or dismissive. Ironically, in its own job advertisements the AI firm Anthropic requests job seekers not to use AI throughout the application process because the company wants to understand them as a person (Wigglesworth, 2025). When algorithms appear to own decisions that previously were driven with a human touch, employees may feel a sense of disconnect or alienation. The second major way AI affects talent management is through the impact it has on the number and nature of jobs needing to be performed. In assessing their justice concerns surrounding an AI tool, employees will speculate on employer intent. Is the goal to reduce the number of jobs available? Is it to deskill positions and lower levels of compensation? Is it to empower employees to leverage AI tools in ways that make them more valuable to the employer?

Given the speed and scale with which employers and vendors (Slater, 2024; Bersin, 2024) are rolling out what they intend as AI 'solutions' to talent management challenges, it would be naïve not to expect some missteps. Sometimes, these missteps are technical, causing leaders to search for a technical solution. We argue, however, that missteps will have other, perhaps less obvious effects. Here, we focus mainly on whether those who are subjects of AI-influenced talent management tools accept the resulting management decisions. It is just as important for leaders to identify, understand, and respond constructively to employee perceptions of the fairness of an AI deployment as it is in the case of technical failings. Organizational justice theory offers a perspective managers can rely on to predict, prevent, and respond to the human consequences of AI-influenced talent management decisions. Throughout history, employees have felt unjust treatment from managers. The challenge as AI emerges as a talent management tool is to seize the opportunity to understand how AI-assisted decisions can instead create a sense of fair treatment among a workforce.

### 1.3. Justice dimensions

Four dimensions of justice have been identified as important to employees. Distributive justice refers to perceptions about the fairness of outcomes received. Interpersonal justice focuses on whether people feel others have treated them with politeness, dignity, and respect. Informational justice considers the quality of explanations provided about the spectrum of decisions and

actions taken by an authority. Finally, procedural justice involves evaluating the fairness of processes used to arrive at and, if necessary, revisit outcomes. Through interactions with employers and their agents, individuals continually evaluate how they feel about the treatment they and others receive. These evaluations form conclusions around the perceived fairness of treatment received.

Below, each of the four justice lenses is explained and a discussion offered as to how the deployment of AI tools may threaten or support leaders' contentions that the company is indeed concerned with the fair treatment of employees. Then, suggested best practices for using AI to increase the likelihood that those impacted will see their treatment as fair are presented. These efforts are also summarized in Table 1. In that table, we offer illustrations as to how employees with justice concerns might respond and how managers might attend to those concerns. When it comes to employee reactions to decisions based on AI, every perceived feature or flaw of its deployment is fair game, and employees will choose which justice lens or lenses to consider. It is not the case that a single characteristic of an AI tool – for example, hallucination – has only bearing on one justice dimension. One employee might be particularly sensitive to issues around voice in algorithm design, another around the distribution it generates. Finally, we offer in Table 2 a series of questions focused on justice considerations that managers can use to evaluate the appropriateness of their efforts to leverage AI in talent management.

[Insert Tables 1 & 2 About Here]

### 1.4. Distributive justice and AI

Distributive justice focuses on employee perceptions of fairness in how employers allocate work outcomes—rewards, resources, and opportunities—among employees. Outcomes include salary, bonuses, promotions, performance feedback, and workload assignments. Employees believe they have been treated fairly when these outcomes are distributed equitably, considering factors like each employee's job performance, experience, and contributions. Because it creates a sense that merit leads to deserved outcomes, distributive justice fosters a positive work environment, motivates employees, and reduces feelings of resentment or dissatisfaction.

When considering the impact of AI on employees through a distributive justice lens, job loss comes quickly to mind. Contemporary headlines describe how entire professions or industries will soon be replaced (imagine ghost call centers) and how companies seek cost savings by reducing headcount (though these statements are curiously mum when disclosing the AI-related costs of achieving these savings). Many still cite a McKinsey report from 2018 that forecasted AI, depending on how quickly it becomes trusted with a company's most sensitive information, could eliminate 15-30% of today's jobs. In April 2024, Forbes reported that AI adoptions had eliminated over 60,000 jobs that month alone (Roeloffs, 2024). The threat of job loss—and concerns about how leadership plans to arrive at decisions about who stays and who goes—always raise substantial concerns about fairness. How an AI algorithm creates the opportunity for the employer to do more with fewer employees and how it informs decisions about who goes and who remains will be of great interest to all stakeholders to the decision.

Sometimes, these claims about job reduction are followed in the next breath by what is supposed to be good news—that employees will be freed up to spend more time on the 'creative' parts of

their work. In this perspective, advocates argue the real victim of AI implementation is mundane work, not employment rates. No doubt this will sometimes be true, but again, the way this is enacted will be evaluated by employees through a distributive justice lens. When AI changes the content of a job, employee concerns about distributive justice will be activated. For example, using AI to automate tasks and make decisions employees have made in the past – routine or not – will engender a feeling the employee’s role is being diminished. Should they conclude that their knowledge, experience, and judgment are no longer valued, concerns about their ability to be considered contributors will arise. Employees may similarly experience a loss of autonomy if they feel pressured to follow AI recommendations without questioning or critically evaluating them. These examples reflect potential concerns regarding distributive justice.

Issues around the fairness of AI in the employee selection process should also be expected. Already, evidence is emerging that its use is problematic in ways that violate distributive justice (Lyton, 2024). Underrepresented groups fear AI tools will likely disparately impact their efforts to land jobs (Gupta, 2023). A recent EEOC settlement with iTutor revealed the company’s recruiting software systematically rejected older job seekers (Olavsrud, 2024). Because of these concerns, legal and regulatory bodies are trying to understand how to control the use of AI (Matambandadzo, 2024). These approaches will undoubtedly affect the conclusions employees and applicants make about the distributive justice they experience at the hands of AI and the employer (Parliament News, 2024).

Several actions come to mind for leaders who want to be sensitive to distributive justice concerns. First, decisions around designing and adopting an AI solution should consider how tools will alter job outcomes in both positive and negative ways. Managers should consider applications and use policies that balance AI capabilities with human judgment to combat potential adverse outcomes (Jarahi, 2018). As a start, consider the AI governance policies recommended by IBM on their website (<https://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples>). Interestingly, the American Bookssellers Association offers on its website an AI policy written by AI (<https://www.bookweb.org/news/example-ai-policy-written-ai-1629793>). One way to create good governance is to ensure employees retain control over critical decisions in designing and implementing AI. Involving employees during implementation can foster a sense of agency and ownership. In addition, providing ongoing training to adapt to AI tools can reinforce competence and autonomy (Bannon, 2023). Companies like Marriott International, KPMG, and Ally Financial have each made investments in training programs that help employees understand not just how the technology is deployed at work, but how it can be leveraged in their jobs to improve their own effectiveness (Kitterman, 2024). When it is clear the technology will deskill positions, concerned employers would be wise to invest in reskilling the workforce to build faith in their abilities and the organization’s commitment to providing fair outcomes in the future.

Interestingly, employees also compute distributive justice assessments when AI creates more complex work assignments, boosts individual productivity, and produces higher profits. To the degree employees feel they created the value, they will see additional compensation as fair. To the degree managers feel an employer-provided tool created the value, employers will not. This divergence in understanding where value was created and how it should be reimbursed can create significant tension between employees and employers.

### 1.5. Interpersonal justice and AI

Interpersonal justice regards the perceived fairness employees experience during interactions, decisions, and everyday work experiences. It focuses on the dignity and respect shown throughout interactions with supervisors and, more generally, the employer. Employees report high levels of interpersonal justice when they are treated with courtesy, listened to attentively, and feel their opinions are valued. This includes clear and respectful communication, even when delivering negative feedback. A workplace that prioritizes interpersonal justice fosters a positive work environment, hopes to increase employee engagement, and reduces feelings of disrespect or alienation (Bies, 2015). This expectation has been found to be even stronger among more senior-level executives (Martin & Nagao, 1989).

We have already seen employers make missteps in using AI to create a reputation for interpersonal justice. In one thoughtless episode, Vanderbilt University officials relied on AI to write an email to its community as part of its official response to a shooting at another university (Perotta, 2023). Delegating communication about such a tragedy is not a way to build a sense of concern regarding interpersonal justice. Ironically, the note — written with ChatGPT — echoed many justice themes. But the very choice to use AI in this context can be viewed as a reflection of a cold, uncaring, disrespectful culture. On one hand, AI tools that correct spelling and grammar to improve communication have been largely non-controversial. Conversely, content creation is an entirely different task; it is easy to see how some would conclude that using generative AI for that purpose is disrespectful. Research suggests that AI-based information is considered less procedurally and impersonally just (Acikgoz, Davison, Compagnone, & Laske, 2020). In the employee selection context, interpersonal justice has also been found to be a prominent employee concern (Ochmann, et al., 2024).

This is comparable to the use of generative AI in talent management tasks such as performance reviews. Because performance reviews involve mundane, repetitive, and time-consuming tasks, they seem like a ripe AI use case. However, when considered through an interpersonal justice lens, how will employees perceive a review generated not by a thoughtful manager but by an impersonal algorithm? Indeed, employees might prefer an AI-based review to one conducted by a lazy or hostile manager. It is also possible the AI tool can identify positive elements of performance that even a conscientious manager might miss. What is most important to acknowledge here is the objective quality of the review is not necessarily what concerns an employee making a judgment about the interpersonal justice of their treatment. By their very nature, performance reviews should build trust and communication and guide and support employee development. They rely on managerial judgment and care. Without the norm of the human-in-the-loop, odds of reaching a conclusion the process was impersonally fair may be reduced. In this case, relying on AI's capabilities sends a strong negative message to employees about what their manager values.

A different sort of interpersonal justice risk has been created by the decision made by leaders at Walmart, Delta, and Starbucks to leverage AI to search through the troves of employee communication stored on company servers (Field, 2024). Precisely what is being searched for is unclear; what is clear is that some employees might be sleepless after hearing this news. It is reasonable for an employer to argue that such activity is within their rights, that the employees

have no expectation of privacy regarding email residing on an employer's server. That conclusion, however, will not stop employees from feeling a violation of norms when it comes to their evaluations of interpersonal justice. Of course, the review of emails was possible before the introduction of AI. However, because the task would have been so labor-intensive without these new tools, it would make sense only in extreme circumstances. Now, the ease with which all this data can be processed might create a temptation to engage in an effort that could be dramatically off-putting to employees.

### 1.6. Informational justice and AI

Informational justice in the workplace focuses on the fairness with which news is communicated to employees. Employees feel justly treated when they promptly receive explanations about decisions, policies, or situations with truthful and comprehensive information. This transparency builds trust between employees and management, fosters a sense of security and control among employees, and allows them to make informed decisions about their work. Conversely, lacking informational justice can lead to confusion, frustration, and feeling out of the loop (Ochmann, et al., 2024). Ultimately, such a situation impacts morale and productivity (Yoch & Spector, 2001).

One important concern regarding AI and informational justice is grounded squarely in the principle of transparency. The simple fact that the term 'black box' is widely used to describe 'where' AI results come from makes the transparency issue blatant. The challenge that results when it comes to building a sense of informational justice around AI is formidable. Those impacted by the outcome will undoubtedly question how fairly the situation was represented in the information fed to the black box and how that information was manipulated during its time in the black box. Understanding the transparency problem inherent in black box algorithms, coders are at work creating 'white box' solutions. White box AI is designed to provide explicit transparency around the process that leads to results. Though such models should provide a greater sense of informational justice, they are currently limited in power (Sciforce, 2020).

There have been examples of companies successfully managing informational justice perceptions during an AI implementation. A few years ago, Hilton Hotels implemented AI-powered automation for housekeeping tasks in some hotels. Before the rollout, they openly communicated the changes to employees, emphasizing improving efficiency and the guest experience. More importantly, they offered extensive reskilling and retraining programs to help displaced workers transition to new roles within the company. Other hotels have taken notice. Recent research reports that AI can increase housekeeping efficiency by 20% and guest satisfaction by 15% (Karagiannis, 2024). These performance gains will be difficult for an employer to ignore. At the same time, achieving them in a manner that is perceived as unjust may have a negative impact on the ledger, as increased costs associated with on-the-job absence and turnover, among other unproductive employee behaviors, are experienced.

Creating stronger impressions of informational justice requires managers to establish clear communication channels to explain how AI systems work, the data they use, and the decisions they lead managers to make (Vincent, 2021). Further, employers must be thoughtful in determining how to best educate employees on the nature of the influential AI tools in talent management decisions. If employees are forming justice judgments based on their suspicions about AI, the risk of developing a false sense of unfairness is significant. Like other practices

that impact employees, feedback mechanisms should be established, allowing employees to ask questions, provide input, and continually engage in open dialogue.

### 1.7. Procedural justice and AI

Procedural justice in the workplace centers on the fairness of the processes used to make decisions that affect employees. An employee's focus here is not the outcome itself but rather their judgment as to whether it was produced using fair, transparent, and consistent procedures. This includes aspects like having a voice in decision-making processes, knowing relevant information was considered, and having the opportunity to appeal decisions. Employees who perceive procedural justice feel respected and believe they have been treated fairly, even when receiving an undesirable outcome. This sense of justice leads to higher trust in leadership, increased acceptance of decisions, and, ultimately, greater employee satisfaction and motivation (Konovsky, 2000). Employees will make conclusions about procedural justice for the decisions an AI algorithm informs, just as they already do in considering strictly human decisions (Marr, 2024).

Research has identified six justice rules that contribute to evaluating a procedure's fairness. Each of these rules is discussed below. The first three rules, consistency, bias suppression, and accuracy, focus on an employee's evaluation of the steps taken to decide. The next three, correctability, representativeness, and ethicality, are elements of the process that become active for employees after a decision has been announced.

Consistency refers to the similarity of treatment and outcomes across people or time. The second rule, bias suppression, involves the ability of a procedure to prevent favoritism or influence from other external biases. The third rule concerns a procedure's accuracy: the ability to offer objectively high-quality solutions. AI offers no guarantees regarding these three elements of procedural justice. Take, for example, bias suppression. AI can leverage vast amounts of data while theoretically eliminating many human biases in recruitment, performance appraisal, and other HR systems. However, biases can also be "baked-in" to the training data used to evolve an algorithm, thus actually perpetuating biases. In perpetuating biases with a tool that some see as infallible, considerable damage can be done. Such was the case when in 2022 the Equal Employment Opportunity Commission sued iTutorGroup for age discrimination because it held the recruitment algorithm for tutors filtered out older applicants. The company agreed to settle the suit in 2023 (Wiessner, 2023).

For a second example, accuracy is at risk because we understand AI hallucinates and can misinterpret critical data and contextual factors (Thorbecke, 2023). Improving employee perceptions of accuracy requires managers to monitor and evaluate output to incorporate continuously or even rely on human oversight based on the risk of each use case. Regarding AI recommendations, employees will likely go no further than "trust but verify." In the foreseeable future, the easiest way to verify will be to let employees know that a reliable human eye has ensured the algorithm's work is trustworthy. This may change with time and increasing familiarity with AI tools. After all, it is unlikely anyone checks the math today after asking Excel to compute a result.

Next are the justice rules that focus on how an outcome is put into play. It becomes evident quickly that here the key guarantor of a sense of fair treatment are the steps the organization has put in place as a check on implementation problems. Correctability, the fourth procedural justice rule, refers to the presence of opportunities to reverse or modify unfair or inaccurate decisions. Any decision based on an AI-produced result needs to be supported by a process that includes the ability to appeal to a higher (and human) authority. The opacityness of AI systems often makes the grounds for an appeal challenging to ascertain. And if warranted, one outcome of the appeal should be to train the AI system further so that similar errors are not repeated. Not surprisingly, correctability is being - and will continue to be - addressed via regulation (Dupre, 2023).

The fifth rule, representativeness, describes the degree to which parties affected by a decision have been involved in the decision-making process. AI that fails to consider the diversity of perspectives or the disparate populations impacted by a decision can quickly be seen as containing bias. Without representation, stakeholders may lose trust - first in the decision and, over time, in the organization. To avoid perceptions of a lack of representativeness, leaders should proactively identify stakeholders, understand potential impacts on each, and regularly solicit their input. To impact perceptions of justice, the gathering and full consideration of this input need to be visible to employees from the design through the implementation and the use of the AI tool (Bastian, 2021).

Finally, the sixth rule, ethicality, focuses on the degree to which the decision-making process accords with general standards of fairness and morality. In recent months, considerable evidence of flawed or harmful decisions arising from the application of AI has accumulated. Each publicized ethical violation increases suspicion and decreases trust among those impacted by AI-driven decisions (West & Allen, 2023). The ethicality dimension also encompasses privacy concerns. Employees will only be reassured about data protection if the organization develops and obeys a robust set of thoughtful policies. Designing AI governance policy without sufficient attention to justice concerns creates considerable risk to decision-makers. Managers will struggle to balance AI performance monitoring with employees' dignity, respect, and privacy needs. Yet only when managers visibly strive to ensure AI tools are continuously trained to remove real and perceived bias will employee justice judgments be enhanced. Human supervision over AI-generated results should always be present, and mechanisms should ensure employees understand that feedback regarding the system is solicited, heard, and acted upon (The Economist, 2022). Resources to help managers consider responsible ways to use AI are beginning to appear (e.g., Neely, 2023). One example of how organizations are responding to this need is through the creation of an AI fairness charter (Yousif, 2021). Such a charter addresses explicitly, for example, the individuals who are responsible for the development of AI tools, an acknowledgement of the groups that might be adversely impacted through the use of the tools, and the specific practices – and parties responsible – for ensuring fair outcomes.

## 2. Managing fairly in the age of AI

Overall, it is essential to remember that particularly in talent management we are early in the evolution of AI. In terms the Gartner Group popularized, we are likely in the 'peak of inflated expectations,' stage 2 of a five-stage 'hype cycle' they offer to plot our experience with a new technology (Fenn & Raskino, 2008). In this stage, publicity around a small number of successful

use cases overshadow reporting of failure. What is evident in these early days is that AI technologies offer many ways to disrupt the familiar methods workplaces use to manage talent. In many cases, that disruption is precisely what we value. We are eager to reap the benefits AI offers. It is likely we will learn, though, that some of the disruption may be only that – disruptive. And, as McKinsey & Company opined in their 2023 report, in an era characterized by great resignation and quiet quitting, can employers afford to anger employees and applicants again? To this point, efforts to provide managers with a framework that contextualizes how employees might perceive the insertion of AI into processes that previously had been more personal have been incomplete (Kaplan & Haenlein, 2020). Our aim here has been to offer a theoretically grounded way for managers to proactively address what will inevitably be employee suspicions regarding the fairness of AI as a talent management tool.

Employees, uneasy about surrendering control to AI's opaque "black box," may call for humans to remain in the decision-making loop. At the same time, AI's purpose precisely is to counteract the subjective biases and inconsistencies humans naturally introduce. The irony is apparent the insistence on human involvement to oversee AI might reintroduce the issues AI is designed to resolve. This highlights a more profound tension between trust in technology and comfort with human imperfection. It is critical managers acknowledge this paradox and proceed thoughtfully as AI is more aggressively used to improve talent management outcomes.

For an organization to realize the full benefit of its investment in AI technology, employees must accept its use as appropriate. That judgment will be grounded to no small degree in each employee's perception of the fairness that characterizes its use. Positive perception is complex to guarantee because perceptions of fairness wholly lie in the eye of the beholder. How effectively this seemingly unmanageable perception is managed requires a focus on each of the dimensions of justice discussed herein: distributive, interpersonal, informational, and procedural. When leaders consistently pay attention to justice considerations, employee confidence in AI and the safety with which its products are used to inform decisions will be enhanced. Of course, a daunting challenge leaders face here is that many of the elements of AI that might lead employees to sense unfair treatment are unobservable or not understood. In response, leaders throughout organizations will need to be educated on how AI tools work. They must develop the language necessary to communicate with employees about their concerns effectively.

## References

Acikgoz, Y., Davison, K.H., Compagnone, M., & Laske, M. (2020). Justice perceptions of artificial intelligence in selection. *International Journal of Selection and Assessment*, 399-416.

Andrieux, P., Johnson, R.D., Sarabadani, J., & Van Slyke, C. (2024). Ethical considerations of generative AI-enabled human resource management. *Organizational Dynamics*. <https://doi.org/10.1016/j.orgdyn.2024.101032>.

Bannon, L. (2023). AI in the workplace is already here. The first battleground? Call centers. *Wall Street Journal*, February 18. Retrieved August 16, 2024, Retrieved from <https://www.wsj.com/articles/ai-chatgpt-chatbot-workplace-call-centers-5cd2142a>.

Barney, J. (1991). Firm resources and sustained competitive advantage. *Journal of Management*, 17(1), 99-120.

Bastian, R. (2021). Why representation matters when building AI. *Forbes*, March 28. Retrieved August 16, 2024, Retrieved from <https://futurism.com/the-byte/ai-deceive-creators>.

Bennett, N., Lemoine, G.J., & Molnar, P. (2024). AI enabled VUCA. *European Business Review*, November/December.

Bennett, N., Martin, C.L., Bies, R.J., and Brockner, J. (1995). Coping with a layoff: A longitudinal study of victims. *Journal of Management*, 21 (6), 1025-1040.

Bersin, J. (2024). Josh Bershin Company announces 30 AI HR tech trailblazer vendors the CHRO Really Needs to know about. Retrieved January 3, 2025, Retrieved from <https://www.prnewswire.com/news-releases/josh-bersin-company-announces-30-ai-hr-tech-trailblazer-vendors-the-chro-really-needs-to-know-about-302258511.html>.

Bies, R. J. (2015). Interactional justice: Looking backward, looking forward. In R. S. Cropanzano & M. L. Ambrose (Eds.), *The Oxford handbook of justice in the workplace* (pp. 89–107). Oxford University Press.

Breidbach, C.F. (2024). Responsible algorithmic decision-making. *Organizational Dynamics*, 53. <https://doi.org/10.1016/j.orgdyn.2024.101031>.

Dungan, R. (2024). Automated Rejections. Employers Increasingly Turning to AI to Handle Recruitment Process. HR Grapevine, October 29. Retrieved December 7, 2024, Retrieved from <https://www.hrgrapevine.com/us/content/article/2024-10-29-employers-increasingly-turning-to-artificial-intelligence-to-handle-recruitment-process>.

Dupre, M.H. (2023). Scientists train AI to be evil, find they can't reverse it. *The Byte*. January 16. Retrieved August 16, 2024, Retrieved from <https://futurism.com/the-byte/ai-deceive-creators>.

The Economist. (2022). Welcome to the era of the hyper-surveilled office. May 14. Retrieved August 16, 2024. Retrieved from <https://www.economist.com/business/welcome-to-the-era-of-the-hyper-surveilled-office/21809219>

Fenn, J., & Raskino, M. (2008). Mastering the hype cycle: How to choose the right innovation at the right time. *Harvard Business Press*, Boston, MA.

Field, H. (2024). How Walmart, Delta, Chevron, and Starbucks are using AI to monitor employee messages. CNBC. Retrieved August 16, 2024, Retrieved from <https://www.cnbc.com/2024/02/09/ai-might-be-reading-your-slack-teams-messages-using-tech-from-aware.html>.

Gelinas, D., Sadreddin, A., & Vahidov, R. (2022). Artificial intelligence in human resource management: A review and research agenda. *Pacific Asia Journal of the Association for Information Systems*, 1-42.

Gilliland, S. W. (1993). The perceived fairness of selection systems: An organizational justice perspective. *Academy of Management Review*, 18(4), 694 - 734. <https://doi.org/10.2307/258595>.

Gupta, S. (2023). Underrepresented groups in countries around the world are worried about AI being a threat to jobs. *Fast Company*, October 31.

Handfield-Jones, H., Michaels, E., & Axelrod, B. (2001). *The war for talent*. Boston: Harvard Business School Publishing.

Harragan, Jr., P. (2022). How Amazon uses AI to dominate e-commerce. *GoDataFeed*. Retrieved August 16, 2024, Retrieved from <https://www.godatafeed.com/blog/how-amazon-uses-ai-to-dominate-e-commerce#:~:text=AI%2C%20therefore%2C%20allows%20Amazon%20to,35%25%20of%20purchases%20on%20Amazon>.

Harwell, D. (2019). Rights group files federal complaint against AI-hiring firm HireVue, citing 'unfair and deceptive' practices. *Washington Post*, November 6.

Hofmann, V., Kalluri, P.R., Jurafsky, D. & King, S. (2024). AI generates covertly racist decisions about people based on their dialect. *Nature* 633, 147-154. <https://doi.org/10.1038/s41586-024-07856-5>.

Holloway, C., & Hand, H.H. (1988). Who's running the store anyway? Artificial intelligence!!! *Business Horizons*, 31(2), 70-76.

Hunkenschroer, A.L., & Luetge, C. (2022). Ethics of AI-Enabled Recruiting and Selection: A Review and Research Agenda. *Journal of Business Ethics*, 977-1007.

Jarrahi, M.H. (2018). Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making. *61, Business Horizons*, 577-586.

Kaplan, A., Haenlein, M. (2019). Siri, Siri in my hand: Who's the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence. *Business Horizons*, 62, 15-25.

Kaplan, A., & Haenlein, M. (2020). Rulers of the world, unite! The challenges and opportunities of artificial intelligence. *Business Horizons*, 63, 37-50.

Karagiannis, M. (2024). How AI is transforming hotel housekeeping into a guest's dream. *CNBC*. Retrieved August 16, 2024. Retrieved from <https://www.cnbc.com/2024/02/09/ai-might-be-reading-your-slack-teams-messages-using-tech-from-aware.html>.

Kidwell, R.E. and Bennett, N. (1994). Electronic surveillance as employee control: A procedural justice interpretation. *Journal of High Technology Management Research*, 5, (1): 39-57.

Kim-Schmid, J., & Raveendhran, R. (2022). Where AI Can – and Can't – Help Talent Management. *Harvard Business Review*, October 13.

Kitterman, T. (2024). How the 100 best companies are training their workforce for AI. *Great Places to Work*, June 17. Retrieved January 5, 2025. Retrieved from <https://www.greatplacetowork.com/resources/blog/100-best-training-workforce-ai>.

Konovsky, M. A., & Freeman, A. B. (2000). Understanding procedural justice and its impact on business organizations. *Journal of Management*, 26(3), 489-511.

Krysik, A. (2024). Netflix algorithm: How Netflix uses AI to improve personalization. *Stratoflow*. Retrieved August 16, 2024. Retrieved from <https://stratoflow.com/how-netflix-recommendation-algorithm-work/>.

Leyer, M., & Schneider, S. (2021). Decision augmentation and automation with artificial intelligence: Threat or opportunity for managers? *Business Horizons*, 64, 711-724.

Lyton, C. (2024). AI hiring tools may be filtering out the best job applicants. Retrieved August 16, 2024. Retrieved from <https://www.bbc.com/worklife/article/20240214-ai-recruiting-hiring-software-bias-discrimination>.

Makarius, E.E., Dachner, A.M., Paluch, R.M., & Pedde, J. (2024). Feel the churn: Exercising talent management practices to support a climate for career mobility. *Business Horizons*, 67, 55-69.

Marr, B. (2024). The Biggest Challenges and Pitfalls of Data-Driven, AI-Enabled HR. Forbes.com, January 12. Retrieved September 13, 2024. Retrieved from <https://bernardmarr.com/the-biggest-challenges-and-pitfalls-of-data-driven-ai-enabled-hr/>.

Martin, C. L., & Nagao, D. H. (1989). Some effects of computerized interviewing on job applicant responses. *Journal of Applied Psychology, 74*(1), 72-80.

Matambandazo, S. (2024). The Impact of Artificial Intelligence on HR Processes. *Tulane University Law School*, August 12. Retrieved October 29, 2024, Retrieved from <https://online.law.tulane.edu/blog/artificial-intelligence-on-hr-processes>.

Mortenson, M. (2023). Tension is rising around remote work. *Harvard Business Review*, July 18.

Narayanan, D., Nagpal, M., McGuire, J., Schweitzer, S., & De Cremer, D. (2024). Fairness perceptions of artificial intelligence: A review and path forward. *International Journal of Human-Computer Interaction, 40*(1), 4-23.

Neely, T. (2023). 8 questions about using AI responsibly, answered. *HBR.org*, May 9. Retrieved February 16, 2025, Retrieved from <https://hbr.org/2023/05/8-questions-about-using-ai-responsibly-answered>.

Ochmann, J., Michels, L., Tiefenbeck, V., Maier, C., & Laumer, S. (2024). Perceived algorithmic fairness: An empirical study of transparency and anthropomorphism in algorithmic recruiting. *Information Systems Journal, 34*, 2, 384-414.

Olavsrud, T. (2024). 12 Famous AI disasters. *CIO.com*, October 2. Retrieved January 4, 2025, Retrieved from <https://www.cio.com/article/190888/5-famous-analytics-and-ai-disasters.html>.

Parliament News (2024). Artificial intelligence act: MEPs adopt landmark law. Retrieved August 16, 2024, Retrieved from <https://www.europarl.europa.eu/news/en/press-room/20240308IPR19015/artificial-intelligence-act-meps-adopt-landmark-law>.

Perotta, R. (2023). Peabody EDI office responds to MSU shooting with email written using ChatGPT. *The Vanderbilt Hustler*, February 17. Retrieved May 1, 2024, Retrieved from <https://vanderbilthustler.com/2023/02/17/peabody-edi-office-responds-to-msu-shooting-with-email-written-using-chatgpt/>.

Qin, S., Jia, N., Luo, X., Liao, C., & Huang, Z. (2023). Perceived Fairness of human managers compared with artificial intelligence in employee performance evaluation. *Journal of Management Information Systems, 40*(4), 1039-1070.

Quantum Workplace (2024). Organizations still are not ready for AI. Retrieved January 14, 2025, Retrieved from <https://www.quantumworkplace.com/employee-engagement-trends-report/emerging-intelligence>.

Rainie, L., Anderson, M., McClain, C., Vogels, E.A., & Gelles-Watnick, R. (2023). AI in hiring and evaluating workers: What Americans think. *Pew Research Center*. Retrieved March 11, 2024, Retrieved from [https://www.pewresearch.org/wp-content/uploads/sites/20/2023/04/PI\\_2023.04.20\\_AI-in-Hiring\\_FINAL.pdf](https://www.pewresearch.org/wp-content/uploads/sites/20/2023/04/PI_2023.04.20_AI-in-Hiring_FINAL.pdf).

Roeloffs, M.W. (2024). Almost 65,000 job cuts were announced in April—And AI was blamed for the most losses ever. Retrieved August 16, 2024, Retrieved from <https://www.forbes.com/sites/maryroeloffs/2024/05/02/almost-65000-job-cuts-were-announced-in-april-and-ai-was-blamed-for-the-most-losses-ever/>.

Russell, C., & Bennett, N. (2015). Big data and talent management: Using hard data to make the soft stuff easy. *Business Horizons*, 58, 237-242.

Sciforce. (2020). Introduction to the white box AI: The concept of interpretability. Retrieved August 16, 2024, Retrieved from <https://medium.com/sciforce/introduction-to-the-white-box-ai-the-concept-of-interpretability-5a31e1058611>.

Slater, B. (2024). Beamery Announces TalentGPT, the World's First Generative AI for HR. PRNewsire, Retrieved January 3, 2025, Retrieved from <https://beamery.com/resources/news/beamery-announces-suite-of-new-products>.

Stone, D.L., Lukaszewski, K.M., & Johnson, R.D. (2024). Will artificial intelligence radically change human resource management processes? *Organizational Dynamics*, 53.

Thorbecke, C. (2023). AI tools make things up a lot, and that is a huge problem. *CNN Business*. Retrieved August 16, 2024, Retrieved from <https://www.cnn.com/2023/08/29/tech/ai-chatbot-hallucinations/index.html>.

Torone, K. (2025). Applicant says Workday AI bias lawsuit should be a nationwide collective action. *HR Dive*. Retrieved February 15, 2025, Retrieved from <https://www.hrdive.com/news/workday-ai-bias-lawsuit-nationwide-class-action/739719/>.

United Tires (2024). Study reveals where drivers are most dependent on their GPS. Retrieved August 16, 2024, Retrieved from <https://www.utires.com/articles/where-drivers-need-gps-the-most/>.

Vincent, V. (2021). Integrating intuition and artificial intelligence in organizational decision making. *Business Horizons*, 64, 425-438.

West, I., & Allen, L. (2023). Embedding an ethical AI culture. *KPMG*. Retrieved August 16, 2024, Retrieved from <https://kpmg.com/uk/en/blogs/home/posts/2023/01/embedding-an-ethical-ai-culture.html>.

Wiesner, D. (2023). Tutoring firm settles US agency's first bias lawsuit involving AI software. *Reuters*, August 10. Retrieved October 23, 2024, Retrieved from <https://www.reuters.com/legal/tutoring-firm-settles-us-agencies-first-bias-lawsuit-involving-ai-software-2023-08-10/>.

Wigglesworth, R. (2025). Anthropic: 'Please don't use AI.' *Financial Times*, February 4. Retrieved February 9, 2025, Retrieved from <https://www.ft.com/content/9b1e6af4-94f2-41c6-bb91-96a74b9b2da1?utm>.

Yoch, C., & Spector, P.E. (2001). The role of justice in organizations: A meta-analysis. *Organizational Behavior and Human Decision Processes*, 86(2), 278-321.

Yousif, N. (2021). 10 steps to educate your company on AI fairness. *World Economic Forum*, June 9. Retrieved February 16, 2025, Retrieved from <https://www.weforum.org/stories/2021/06/10-steps-to-educate-your-company-on-ai-fairness/>.

Table 1. Examples of how employees would see AI through justice lenses

| Lens                                                                                                                                                        | Potential pitfalls                                                                                                                                                                                                          | Recommendations to managers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <strong>Distributive</strong><br>Perceived fairness of outcomes received (rewards, resources, job content, growth opportunities) as influenced by use of AI | Employees may suspect algorithms favor metrics leading to biased allocations of outcomes.                                                                                                                                   | <ul><li>Audit AI Decisions regularly to identify and correct biases</li><li>Revisit compensation and adjust for employees taking on more complex tasks and/or increased responsibilities</li><li>Provide alternative roles and upskilling opportunities or severance packages to assist workers displaced through AI deployment.</li></ul>                                                                                                                                                                                                                                               |
| <strong>Interpersonal</strong><br>Focuses on the respect, dignity, and empathy displayed during decision-making                                             | Employees may not recognize any human element in AI-driven decisions leading them to perceive the organization's culture as impersonal and uncaring.                                                                        | <ul><li>Supplement AI-driven decisions with empathetic and respectful personal communication, especially in sensitive areas like layoffs, promotions, and performance feedback.</li><li>Maintain regular check-ins with employees to preserve a human connection.</li><li>Emphasize that when AI supports decisions, human judgment and respectful personal interaction remain central to workplace culture.</li><li>In developing a use case for AI in talent management take into consideration where the AI may not be appropriate if viewed through an interpersonal lens,</li></ul> |
| <strong>Informational</strong><br>The timeliness, clarity, and justification of talent management decisions                                                 | The 'black box' nature of AI might create a void for employees trying to ascertain informational fairness, leading to an erosion of trust in leadership.                                                                    | <ul><li>Demystify AI's decision-making process through employee training and exposure.</li><li>Proactively communicate the role of AI in decision-making, including the data, criteria, and logic used.</li><li>Ensure employees receive detailed explanations and feedback on AI-driven decisions.</li><li>Encourage open dialogue around company use of AI, addressing any concerns.</li></ul>                                                                                                                                                                                         |
| <strong>Procedural</strong><br>Perceived fairness of processes and procedures in talent management decisions                                                | AI-driven decision-making processes may not provide visible cues employees could use to evaluate its procedural fairness. The absence of a case for its fairness provides an opportunity for employees to assume the worst. | <ul><li>Involve employees in the design and review of AI processes, allowing for feedback and input.</li><li>Establish an appeal processes for AI-driven decisions.</li><li>Widely communicate instances where employee concerns around procedures have been heard and the system's fairness improved.</li></ul>                                                                                                                                                                                                                                                                         |

**Table 2. Questions managers should address when developing AI for talent management**

### **Distributive**

- Do employees understand the nature and quality of the data considered by AI?
- Do employees know the role that human decision makers play as intermediaries between an AI result and a company action?
- If employees understood how AI would change job roles, salaries, and workloads, would they agree that the outcomes are fair?
- Who are the likely winners and losers if this AI system is implemented?
- If AI increases productivity or revenue, how will those gains be reinvested? In other words, how will financial rewards be equitably distributed?
- Do I have a strategy to monitor and adjust AI-driven decisions if they create unintended distributive fairness issues?

### **Interpersonal**

- Have we been thoughtful about how decisions produced with AI are explained for employees?
- Are managers and decision makers trained to handle algorithm decisions with empathy and professionalism?
- Will the use of AI in this case send the wrong message to our key stakeholders as to our organizational culture or values?

### **Informational**

- Have we been thoughtful about how decisions produced with AI are contextualized for employees?
- Would I be comfortable explaining AI-driven resource allocations to all employees?
- Have I provided clear, comprehensive, and accurate information about why, where, and how AI decisions will be made?
- If an algorithm's decision is challenged, is relevant and understandable information available to justify its outcome?
- Have I disclosed the potential risks or limitations of AI implementation and not just the benefits?

### **Procedural**

- Have we taken steps to provide for employee input in design of AI tools?
- Have we built in and widely communicated procedures for appealing AI-influenced decisions?
- Have I established clear, repeatable criteria for how AI-related decisions (e.g., job role changes, task automation) will be made?
- Have I involved a diverse group of stakeholders (employees, managers, HR, legal teams) in shaping AI-related policies?
- Based on risk, are there checks and balances to ensure that AI recommendations are not final without human validation where risk of a serious mistake will be detrimental to the organization?