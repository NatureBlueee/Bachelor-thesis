

![Elsevier logo](935eed7aa61f7777f62cfc032e11bee9_img.jpg)

Elsevier logo

ELSEVIER

Contents lists available at ScienceDirect

## Computers in Human Behavior

journal homepage: [www.elsevier.com/locate/comphumb](http://www.elsevier.com/locate/comphumb)![Cover image of Computers in Human Behavior journal](0538daaa5583c23e17db3a12f2281a55_img.jpg)

Cover image of Computers in Human Behavior journal

![Check for updates icon](4f4b52340aaccb1bcf733468dca9ee03_img.jpg)

Check for updates icon

# The roles of outcome and race on applicant reactions to AI systems

Rewina Bedemariam<sup>\*</sup>, Jennifer L. Wessel

Social, Decision and Organizational Science Department (SDOS), University of Maryland, College Park, USA

## ARTICLE INFO

Handling editor: Bjørn de Koning

### Keywords:

Applicant reactions

AI

Fairness perceptions

Race

Selection

## ABSTRACT

Practitioners have embraced the use of Artificial Intelligence (AI) systems for employee recruitment and selection. However, studies examining applicant reactions to AI have been exclusively vignette-based with no perceived outcome associated with the decision and also have not considered demographic differences in AI evaluator perceptions. We employed an experimental design wherein type of evaluator (AI vs human) and the selection decision (acceptance vs rejection) were manipulated and participants were led to believe they would receive different outcomes based on the selection decision. The results showed more negative interactional justice perceptions for AI evaluators. Further, interaction analyses revealed that being rejected by AI had a negative impact on certain procedural and general justice perceptions. We compared Black and White applicants on these perceptions, finding that the negative impact of being rejected by AI was particularly strong for Black applicants in terms of their general justice perceptions. Theoretical and practical implications are discussed.

## 1. The roles of outcome and race on applicant reactions to artificial intelligence selection systems

There has been a marked growth in the use of technology in recruitment and selection, with 74% of large U.S. organizations using some form of electronic selection tools to help with the hiring process (Stone, Deadrick, Lukaszewski, & Johnson, 2015). Organizations are now leveraging the use of Artificial Intelligence (AI) and Machine Learning (ML) to automate selection decisions (EEOC, 2023; Stephan, Brown, & Erickson, 2017). AI-based tools can offer numerous benefits in the job application and hiring decision-making process, including more efficient decision-making, standardized selection processes and accuracy (e.g., EEOC, 2023; Johnson & Verdicchio, 2017).

Doubts concerning the utility of these algorithms, however, arise at the inner workings of the algorithm itself (often termed the “black box”). Specifically, there are concerns that surround the use of datasets that lack information or disproportionately represent certain populations, as well as the application of poorly designed models that lack sound theoretical and scientific ground (Bash & Melchers, 2019). AI-based selection tools might maintain or even amplify systematic discrimination perpetuated by humans since such systems will “learn” to replicate human biases embedded in the datasets used (Calisian, Bryson, & Narayanan, 2017; Mann & O’Neil, 2016).

Although the goal of AI tools is to increase the accuracy and efficiency of the hiring process, the perceptions, and attitudes that people

have of them affect their viability in the workplace and thus should be explored. This is because negative applicant reactions lead to outcomes such as withdrawal from the selection process, which dampens the utility of the selection system and potentially curbs potential employees from joining the company (Ryan, Sacco, McFarland, & Kriska, 2000). Negative perceptions of a selection system can also make the organization less attractive to top candidates, decrease acceptance intentions, and may increase intentions to litigate (Gemen, Proost, Van Dijke, De Witte, & Von Grumbkow, 2012). Candidates with negative reactions to a company’s selection system may also dissuade other potential employees from applying to the company (Hausknecht, Day, & Thomas, 2004). Since managing a positive image is important for an organization during the selection process, our study focusing on applicant reactions to AI can assist HR managers in weighing the advantages and disadvantages of using computer-mediated selection systems versus traditional selection procedures (Lee, 2018). Additionally, the exploration of potential differences in applicant reaction in terms of minority status could bring to the forefront issues that are relevant for companies seeking to promote inclusion.

Although psychological interest in decision-making and human perception in recruiting and employment has endured for a long time in I/O psychology research, only recently have researchers started to look into the function that AI plays in the context of applicant reactions. Few studies have begun to examine whether traditional and algorithmic job application screening elicit distinct reactions in terms of procedural and

<sup>\*</sup> Corresponding author. 9500 Fairfax Blvd. Apt 2214, Fairfax, VA, 22031, USA.

E-mail address: [rewinab@umd.edu](mailto:rewinab@umd.edu) (R. Bedemariam).

interpersonal justice. For instance, a two-part vignette research by Acikgoz, Davison, Compagnone, and Laske (2020) indicated that traditional human-based interviewing was perceived as more fair in both procedural and interactional dimensions than AI-based interviewing. Another recent vignette study by Noble, Foster, and Craig (2021) found that while automated screening was rated higher on consistency, it elicited lower perceptions on job relatedness-predictive, job relatedness-content, opportunity to perform, opportunity for reconsideration, treatment, and two-way communication. The interaction between screening medium (HR vs automated) and outcome favorability showed mixed results with opportunity to perform and reconsideration opportunity demonstrating an interaction (Noble et al., 2021).

It is important to note that both of these past studies used hypothetical scenarios in which the individual did not believe they were actually being evaluated and there was no personal consequence tied to the AI selection tool. In our study, participants will believe their information is being screened by a person or an AI selection tool and they will also believe there is an actual outcome associated with their acceptance or rejection by the decision-maker. Utilizing a higher fidelity technique in an experimental paradigm will allow us to better understand how AI is perceived when there are actual stakes in its decisions.

A second contribution of this study is the focus on racial identity as a factor in reactions to AI-based selection tools. Very few studies have explored individual differences/person characteristics in applicant reactions to AI, despite calls to include individual differences in applicant reactions research (Ryan & Ployhart, 2000). Because racial minorities have experiences that can influence their perceptions of fairness such as past experiences of prejudice, discrimination, and stereotyping, we focus on applicant race as a potential personal factor affecting perceptions of fairness in the selection process. Specifically, given increased public awareness in the ability of AI tools to exhibit anti-Black bias (e.g., HBR, 2019; Johnson & Johnson, 2023), we posit it is important to understand race as a factor in reactions to AI selection tools.

It is becoming increasingly crucial to make sure that applicants have a positive reaction to the way the organization treats them during the recruitment process (Walker et al., 2013). Fairness perceptions are important even for applicants who organizations reject, as negative fairness perceptions increase intentions to pursue litigation (Bauer, Truxillo, Paronto, Weekley, & Campion, 2004a, 2004b) and actual legal challenges (Goldman, 2001). Therefore, from a strategic standpoint, recruiting needs to address not only growing the quantity and caliber of the initial applicant pool, but also keeping applicants engaged all the way through the hiring process (Walker, Helmuth, Feild, & Bauer, 2015; Acikgoz et al., 2020).

### 1.1. Organizational justice theory and applicant reactions

Organizational justice has emerged as a popular conceptualization of fairness perceptions in work-related contexts (Greenberg & Cropanzano, 2001). One key type of fairness perception is *procedural justice*, which is the extent to which the process used to decide the outcome is deemed fair (Gilliland, 1993). Gilliland further identified several procedural justice rules, including: job-relatedness, opportunity to perform, reconsideration opportunity, consistency of administration, feedback, and selection information (Gilliland, 1993). *Interactional justice* is another component of organizational justice theory. It describes the perceptions of fairness related to interpersonal interactions between the individual and the person or entity that distributes a certain outcome (Gilliland, 1993). It underlies an individual's belief that persons or entities involved in rendering a decision/outcome treated them fairly. *Distributive justice* is concerned with the fair distribution of particular outcomes of a process (Greenberg & Cropanzano, 2001).

In the context of applicant reactions, justice is important because it can affect the way that applicants perceive and respond to the hiring process. *Applicant reactions* refer to “attitudes, affect, or cognitions an individual might have about the hiring process” (Ryan & Ployhart, 2000,

p. 566). Applicant reactions are integral to a hiring procedure, because applicants' perceptions of fairness directly influence subsequent attitudes and behaviors both during and after hiring (Hausknecht et al., 2004; Konradt, Warszta, & Ellwart, 2013; Truxillo, Bauer, Campion, & Paronto, 2002). For example, if applicants perceive the process as unjust or unfair, they may become frustrated or discouraged, which could lead to negative reactions and potentially harm the company's reputation (Geenen et al., 2012; Hausknecht et al., 2004).

Each procedural justice rule serves an important role in maintaining the fairness perception of an application process. Job-relatedness, face validity, and opportunity to perform are especially important in determining positive reactions to selection processes (Bauer, Maertz, Dolen, & Campion, 1998; Hausknecht et al., 2004; Schleicher, Venkataramani, Morgeson, & Campion, 2006). Out of the procedural justice dimensions, there are some factors that are likely to be more relevant to understanding reactions to human versus AI decision-makers than other factors. For example, *opportunity to perform*, which refers to one's ability to demonstrate knowledge, skills, and abilities in a selection process would likely vary with an AI vs human decision-maker. Bauer et al. (2006) suggested that technological advances in the medium of assessment administration may impact reactions of opportunity to perform. This has been found to be the case with other technology-mediated hiring mediums such as asynchronous videos (Bauer et al., 2004a, 2004b; Chapman, Uggerslev, & Webster, 2003a, 2003b). Studies on computer-based testing have also found that consistency, treatment of the applicants, and opportunity to perform were the strongest predictors of fairness perceptions (Dineen, NOE, & Wang, 2004, pp. 127–145; Konradt et al., 2013).

From the perspective of signaling theory (Chapman et al., 2003a, 2003b), the perception of applicants' inability to ask questions or discuss their choices during the selection process may affect their fairness heuristics and send a message that the organization does not value them. This means that traditional HR methods with potential for human interaction may be preferred over AI-based selection due to their greater interpersonal contact and capacity for two-way communication (Gilliland, 1993). Therefore, there may be differences between the conditions for numerous dimensions such as *reconsideration opportunity*, since it could be perceived that they can reach out to an HR personnel for a reconsideration opportunity than in an AI-mediated selection process. *Feedback*, another procedural justice dimension, denotes the information given to applicants about their performance. It emphasizes both timing at which the feedback is given and the content of that feedback (Gilliland, 1993). For this study, we posit that applicants whose decisions were made by an AI would perceive their chances of getting feedback as minimal (compared to a human decision-maker) and thus their perception of this dimension would be lower.

Overall, we posit that more advanced technology such as AI could elicit greater negative reactions than a human decision-maker on perceptions of opportunity to perform, opportunity for reconsideration, and feedback.

**H1.** Applicants who believe they are being reviewed by an AI/ML procedure will have more negative perceptions of opportunity to perform, feedback and reconsideration opportunity and overall lower general procedural justice perceptions than those applicants who believe they are being reviewed by human recruiters.

There is some existing empirical evidence that interactional justice perceptions are more negative for AI decision-makers. Gonzalez, Capman, Oswald, Theys, and Tomczak (2019a) found that participants generally reacted less favorably on the interactional justice dimension to AI decision-makers (compared to HR). The comments they received from the participants expressed concerns of its “impersonal nature and inaccuracy” (p.37). They went on to show that most of the participants particularly expressed stronger interpersonal concerns (e.g., dignified

treatment, communication) than procedural concerns (e.g., consistency, accuracy).

Fairness heuristic theory (Lind, 1992; Lind, Kray, & Thompson, 2001) notes that people create a “fairness heuristic” to help them make decisions and form expectations in the face of ambiguity. As such in regards to interactional justice, AI systems could be perceived as less fair because of the perceived lack of human interaction, which may serve as a heuristic indicating unfairness since the interpersonal dynamics of a selection process may be considered missing and may also send a message to the applicant that they are not valued as a potential employee (Van Iddekinghe, Raymark, & Roth, 2005; Acikgoz et al., 2020). As such, it was also hypothesized that AI would have a significant effect interpersonal justice perceptions in addition to having lower procedural justice fairness perceptions.

**H2.** Applicants who believe they are being reviewed by an AI/ML procedure will have more negative interactional justice perceptions than those applicants who believe they are being reviewed by human recruiters

Focusing on distributive justice, applicant reactions research has demonstrated that applicants who are not hired display negative fairness perceptions in comparison to those who were hired (Bauer et al., 1998; Hausknecht et al., 2004; Truxillo & Bauer, 2002. Gilliland (1993) finds that when distributive rules are violated (negative outcomes), procedural justice rules have the biggest impact on fairness perceptions. For example, being rejected by an AI tool may be more likely to elicit negative fairness reactions than being rejected by HR, as the difference of evaluation mode (i.e., process) becomes more salient when one is not hired. We posit that individuals will have a particularly negative reaction to being rejected by AI (instead of HR), due to the tendency to attribute rejection to the inadequacies of the process instead of their own performance (Schinkel, Vianen, & Dierendonck, 2013). Specifically, Schinkel, van Vianen, and van Dierendonck (2013) found that procedural fairness affected organizational perceptions when applicants were rejected. They further found that applicants who were hired appeared indifferent to procedural fairness.

We hypothesize that the outcome will have an impact on the perception of both procedural and interactional justice.

**H3.** Outcome favorability will moderate the relationship between decision-maker condition and justice perception, such that the negative effect of having an AI decision-maker will be stronger when receiving an unfavorable outcome (rejection).

### 1.2. The influence of racial identity on AI fairness perceptions

Job candidates have experiences that can influence their interpretation of the organization's selection processes and outcomes (Derous, Born, & Witte, 2004). For example, an individual with prior experiences of discrimination may attribute a rejection decision to discrimination (Ryan & Ployhart, 2000). Therefore, the perceptions they have of an application procedure can at least partly – be determined by their characteristics and background experiences. In the context of AI and selection, we predict differences between Black and White applicants based on a) past work on identity management opportunities and b) the role of outcome in low-trust or ambivalent situations.

Often, to cope with stereotypes and prejudicial assumptions, stigmatized individuals will enact certain interpersonal strategies to manage their identities and hopefully be viewed more favorably (Roberts, Settles, & Jellison, 2008). For example, Miller and Kaiser (2001) discussed “compensation” as a primary-control coping strategy that is sometimes used by stigmatized individuals to lessen prejudiced reactions. For individuals from racial/ethnic minority groups, in particular, the impact of stigma and negative stereotypes associated with their group identity will have a notable impact on career advancement prospects (Morgan, 2002). Thus, some applicants from marginalized

groups employ strategies to exert control over how they are viewed and affect their social and career standing during job interviews and selection procedures for work. Roberts (2005) produced a comprehensive theory highlighting the process by which those in the minority will actively construct their professional image in attempts to thwart stereotyping and categorization, labeled “social identity-based impression management”. Given these concerns over being stigmatized and seen as unprofessional, those who belong to a minority racial group may want more opportunity to manage their image, receive feedback, and overall, exert a sense of control over the application process. Supporting this theory, one empirical study on applicant reactions by Hiemstra, Derous, Serlie, and Born (2012) found that ethnic minority applicants are more likely to prefer a personalized way of applying to jobs (i.e., video versus paper resume), which they posited was due to a desire to overcome initial stereotyping through having an opportunity to elaborate on their qualifications. Elaboration opportunities, however, may be found lacking when evaluated by AI tools.

In addition to the limited opportunity to manage one's image actively, another potential reason for the discrepancy in fairness perceptions between minority groups and majority groups is increasing awareness in the public domain that AI can exhibit anti-Black bias in facial recognition systems, recruitment, and other areas (e.g., HBR, 2019; Johnson & Johnson, 2023). Past research suggests that the influence of outcome favorability on perceptions of said outcome is greater when the decision-maker's reputation for bias is high (Stähl, Vermunt, & Elemers, 2008). Put another way, when applicants are more ambivalent and/or wary about a decision-maker, we would expect a stronger negative reaction when rejected by that decision-maker. Human evaluators obviously can have racial biases and AI may be seen as more consistent and objective in some contexts, but given recent public news stories of AI racial biases (Johnson & Johnson, 2023; Verma, 2022; Wolf, 2023), we posit it is likely that opinions among Black applicants may be more ambivalent toward AI and thus more reliant on the outcome itself.

In this study, we hypothesize that AI-generated rejections will invoke different reactions for minority and majority racial groups, due to wariness of potential anti-Black bias and lack of opportunity to construct, elaborate, and/or manage one's professional image.

**H4.** Black participants will report more negative fairness perceptions when decision is made by an AI evaluator and when rejected by AI.

## 2. Method

### 2.1. Participants

We sampled working adults that resided in the USA through Mturk. The first phase had a total of 550 participants. Due to attrition, we had a total of 377 participants who completed both the first and second phase. Ninety-five participants were removed from analysis because they failed attention checks and/or did not complete the surveys (had lower than 70% completion rate or did not complete all major scales for analysis). A total of 282 participants comprised our final sample (162 female, 175 Black, and 107 White participants) were sampled. By condition, 150 were randomly assigned in the AI condition and 132 in the HR condition.

### 2.2. Procedure

We conducted a 2 (Condition)  $\times$  2 (Outcome)  $\times$  2 (Race) between-subjects experimental design. All aspects of the procedure and materials were approved by the Institutional Review Board of the authors' institution. Data was gathered through MTurk (Turk Prime) to simulate the typical job application process that occurs through online portals. Participants applied for a HIT (Human Intelligence Task), which are online tasks that individuals can sign up for in order to get paid. The HIT

job was advertised as paying \$5 for those who got accepted, so that those rejected would get a sense that they incur a loss by being rejected. The study consisted of two stages: 1) the application and 2) the post-application survey.

During the application phase participants applying for the fake HIT job post answered application blanks. The application blank items allowed participants to manage impressions by asking them to provide their strengths, weaknesses, and skills. Gilmore and Ferris (1989) noted that “this prior information (i.e., application blank or resume) available to the interviewer is typically strategic impression management” (p. 202). Moreover, application blanks are designed to collect information that an organization deems important for suitable hiring, thus maintaining fidelity. All participants were also asked to provide their work experiences, as well as demographic information. After completing the application blank, all participants were told they would receive a decision within 24 h. They were compensated \$1.00 for filling out the application blank.

For the post-application phase, applicants were sent links to the second phase of the study (after 24 h), where they were randomly assigned to a condition (AI vs HR) and outcome (rejection vs acceptance) groups. They got a message that either read:

“Your application was reviewed by [Human Resources personnel/ an Artificial Intelligence system]. We would like to inform you that you have been accepted to complete the HIT job you have applied for. Before proceeding to the task, there are surveys to be filled about the job application process. Please press next to continue.”

Or

“Your application was reviewed by [Human Resources personnel/ an Artificial Intelligence system]. We regret to inform you that you have been rejected to complete the HIT job you have applied for. However, you will be paid \$1.00 for completing the surveys about the job application. Please press next to continue.”

After this message, all participants in both conditions filled out surveys concerning their perceptions of the justice dimensions, got debriefed and were compensated \$1.00 for completion.

### 2.3. Measures

#### 2.3.1. General applicant reactions

The General Applicant Reaction Scale ( $\alpha = 0.75$ ) measures the overall perception of fairness of the job application process. It contains nine items concerning the perceived fairness of the evaluator and their decision (e.g., “I trust the factors used by a [AI vs HR] to make the employment decision”. Participants responded on a 1–5 scale ranging from Strongly Disagree to Strongly Agree. This scale was adopted from Gonzalez, Capman, Justenhoven and Preuss (2019b).

#### 2.3.2. Procedural Justice

The Selection Procedural Justice scale (SPJS) is composed of procedural justice subscales such as opportunity to perform ( $\alpha = 0.94$ ), reconsideration opportunity ( $\alpha = 0.89$ ), job-relatedness ( $\alpha = 0.83$ ), information known ( $\alpha = 0.92$ ) and feedback ( $\alpha = 0.82$ ). Participants responded on a 1–5 scale ranging from Strongly Disagree to Strongly Agree. The scale is adopted from Bauer et al. (2001).

#### 2.3.3. Interactional Justice

The interactional justice subscale of the SPJS was used. It measures how participants assessed the interactional component of the job application process ( $\alpha = 0.92$ ) and contain three items (e.g., “I think I am treated fairly in the decision process via the decisions made by [Alvs HR]”). [YA3] Participants responded on a 1–5 scale ranging from Strongly Disagree to Strongly Agree.

#### 2.3.4. Familiarity with artificial intelligence

Past work on AI-based systems have discussed how perceived or actual familiarity with AI can influence perceptions (Gonzalez et al., 2019a; Johnson & Verdichchio, 2017). We decided to include familiarity with AI as a control variable in analyses, as it was not the focus of our study but may have influence. To measure participant’s familiarity with AI, we utilized an instrument adapted from Gonzalez et al. (2019b) that probes the extent of familiarity of participants towards AI. It is a two-item measure ( $\alpha = 0.85$ ) with questions like “How much experience do you have with using artificial intelligence?”. Response options were on a 1–5 point likert scale from highly experienced to no experience at all.

## 3. Results

Table 1 provides the means, standard deviations, and correlations for all key variables. See our supplemental file for means by conditions for all six outcome variables. A post hoc sensitivity analysis found that given power of .80, three predictors and a sample size of 282, we were able to detect an effect size of at minimum  $\hat{\beta}^2 = 0.029$ . In all analyses below, we also included two outcome variables focused on emotions (positive and negative). These findings were exploratory and outside of the scope of our focus on fairness perceptions. We include those findings in the supplemental file (see Table 2).

### Hypothesis 1. Procedural Justice

We conducted a between-subjects multivariate analysis of covariance examining the main effects of condition, controlling for familiarity with AI and outcome and race, for the following outcome variables: procedural fairness subscales (feedback, reconsideration opportunity, chance to perform, job relatedness, information known), general procedural fairness reactions, and interactional fairness (Pillai’s  $V = 0.038$ ,  $F(9, 269) = 1.19, p = .301$ , partial  $\eta^2 = 0.038$ ). Condition by itself (being in the AI or HR condition) did not have a main direct effect on any of the procedural justice dimensions such as Job relatedness  $F(1, 277) = 0.903, p = .343$ ; Chance to Perform  $F(1, 277) = 0.048, p = .826$ ; Information known  $F(1, 277) = 2.650, p = .105$ ; Reconsideration Opportunity  $F(1, 277) = 0.571, p = .451$ ; Feedback  $F(1, 277) = 1.664, p = .198$ , and General Fairness Reactions  $F(1, 277) = 1.975, p = .161$ . Thus, H1 was not supported.

### Hypothesis 2. Interactional Justice

For our second hypothesis (H2), there was a significant main effect of condition (AI vs HR) on interactional justice,  $F(1, 277) = 5.682, p = .018$ , partial  $\eta^2 = 0.020$ , such that marginal means (i.e., controlling for familiarity) of interactional justice for participants in the AI condition were lower ( $M = 3.38$ ) than those in the HR condition ( $M = 3.66, d = 0.29$ ). Therefore, H2 was supported.

### Hypothesis 3. Condition \* Outcome

We conducted a 2 (Condition)  $\times$  2 (Outcome) between-subjects multivariate analysis of covariance, controlling for familiarity with AI and race, for all outcome variables. The interaction between condition (AI vs HR) and outcome (rejection vs acceptance) was significant for some of the procedural justice outcomes, including: chance to perform, reconsideration opportunity, feedback, general procedural reactions and interactional justice perceptions.

The interaction between condition and outcome (accepted or rejected) had a significant effect on the chance to perform dimension of procedural justice,  $F(1, 276) = 4.015, p = .046$ , partial  $\eta^2 = 0.014$ . A pairwise comparison of the marginal means indicates that participants in the AI condition who were rejected had lower perceptions of chance to perform ( $M = 2.89$ ) than those rejected by the HR condition ( $M = 3.19, p = .114$ ), though not significant. Those who were accepted in the AI condition had a mean of 3.38 compared to those who were accepted by HR ( $M = 3.14, p = .81$ ), also not significant. Additional post-hoc

Table 1

Table of means, standard deviations and intercorrelations.

| Variable                      | M    | SD   | 1                 | 2                 | 3                 | 4                 | 5                 | 6                 | 7                 | 8    | 9 |
|-------------------------------|------|------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|------|---|
| 1 Procedural Justice          | 3.25 | .85  | 1                 |                   |                   |                   |                   |                   |                   |      |   |
| 2 Interactional Justice       | 3.52 | .99  | .614 <sup>a</sup> | 1                 |                   |                   |                   |                   |                   |      |   |
| 3 General fairness reactions  | 2.99 | .59  | .655 <sup>a</sup> | .693 <sup>a</sup> | 1                 |                   |                   |                   |                   |      |   |
| 4 Job Relatedness             | 3.71 | .94  | .661 <sup>a</sup> | .484 <sup>a</sup> | .466 <sup>a</sup> | 1                 |                   |                   |                   |      |   |
| 5 Information Known           | 3.39 | 1.08 | .762 <sup>a</sup> | .368 <sup>a</sup> | .359 <sup>a</sup> | .476 <sup>a</sup> | 1                 |                   |                   |      |   |
| 6 Chance to perform           | 3.18 | 1.12 | .871 <sup>a</sup> | .527 <sup>a</sup> | .634 <sup>a</sup> | .570 <sup>a</sup> | .565 <sup>a</sup> | 1                 |                   |      |   |
| 7 Reconsideration Opportunity | 2.82 | 1.05 | .880 <sup>a</sup> | .533 <sup>a</sup> | .589 <sup>a</sup> | .461 <sup>a</sup> | .534 <sup>a</sup> | .699 <sup>a</sup> | 1                 |      |   |
| 8 Feedback                    | 3.56 | .98  | .746 <sup>a</sup> | .514 <sup>a</sup> | .477 <sup>a</sup> | .374 <sup>a</sup> | .542 <sup>a</sup> | .502 <sup>a</sup> | .588 <sup>a</sup> | 1    |   |
| 9 Familiarity with AI         | 2.42 | .70  | .104              | .174 <sup>a</sup> | .122 <sup>b</sup> | .067              | .039              | .126 <sup>b</sup> | .065              | .056 | 1 |

<sup>a</sup> Correlation is significant at the 0.01 level (2-tailed).

<sup>b</sup> Correlation is significant at the 0.05 level (2-tailed).

Table 2

Outcome \* Condition effect on dependent variables.

| Independent Variable | Dependent Variable          | Sum of Squares | df | Mean Square | F      | Sig  |
|----------------------|-----------------------------|----------------|----|-------------|--------|------|
| Outcome * Condition  | Interactional Justice       | 11.849         | 1  | 11.849      | 13.628 | .000 |
|                      | General Reactions           | 2.332          | 1  | 2.332       | 7.233  | .008 |
|                      | Job Relatedness             | 5.808          | 1  | 5.808       | 6.619  | .011 |
|                      | Information                 | .257           | 1  | .257        | .217   | .642 |
|                      | Chance to Perform           | 4.979          | 1  | 4.979       | 4.015  | .046 |
|                      | Reconsideration Opportunity | 5.714          | 1  | 5.714       | 5.276  | .022 |
|                      | Feedback                    | 10.784         | 1  | 10.784      | 11.641 | .001 |

analyses show that participants perceived a significantly greater chance to perform when accepted (versus rejected) by AI ( $p = .008$ ), which is likely driving the interaction.

The interaction between condition (AI vs HR) and outcome (rejection vs acceptance) had a significant effect on reconsideration opportunity,  $F(1, 276) = 5.276$ ,  $p = .022$ , partial  $\eta^2 = 0.019$ . A pairwise comparison of the marginal means indicates that participants in the AI condition who were rejected had lower perceptions of reconsideration opportunity ( $M = 2.48$ ) than those rejected by the HR condition ( $M = 2.91$ ,  $p = .031$ ,  $d = .039$ ). Those who were accepted in the AI condition had a mean of 3.03 compared to those who were accepted by HR ( $M = 2.84$ ,  $p = .291$ ), which was not a significant difference.

Perceptions of feedback was also significantly affected by this interaction,  $F(1, 276) = 11.849$ ,  $p < .001$ , partial  $\eta^2 = 0.040$ . Participants in the AI condition who were rejected had lower feedback perceptions ( $M = 3.17$ ) than those rejected by the HR condition ( $M = 3.78$ ,  $p < .001$ ,  $d = 0.59$ ). Those who were accepted in the AI condition had a mean of 3.78 compared to those who were accepted by HR ( $M = 3.55$ ,  $p = .151$ ), showing that this effect was only significant when rejection occurs in the AI condition.

General procedural justice was also significantly affected by this interaction,  $F(1, 276) = 2.332$ ,  $p = .008$ , partial  $\eta^2 = 0.026$ . Participants in the AI condition who were rejected had general procedural fairness perceptions of  $M = 2.74$ , which is significantly lower than those rejected by the HR condition ( $M = 3.02$ ,  $p = .004$ ,  $d = 0.46$ ). Those who were accepted in the AI condition had a mean of 3.10 compared to those who were accepted by HR ( $M = 3.02$ ,  $p = .391$ ), showing that this effect was only significant when applicants were rejected.

There was also a significant interaction effect between condition and outcome on interactional justice ( $F(1, 277) = 7.69$ ,  $p = .003$ , partial  $\eta^2 = 0.047$ ), qualifying the main effect of condition. Participants in the AI condition who were rejected had lower interactional justice scores ( $M = 3.03$ ) than those rejected by the HR condition ( $M = 3.72$ ,  $p < .001$ ,  $d = 0.75$ ). Those who were accepted in the AI condition had a mean of 3.73 compared to those who were accepted by HR ( $M = 3.60$ ,  $p = .406$ ),

which was not significant. Overall,  $H3$  received partial support. Rejection seems to amplify the differences in fairness reactions between AI and HR evaluators.

### Hypothesis 4. Condition \* Outcome \* Race

We conducted a 2 (Condition)  $\times$  2 (Outcome)  $\times$  2 (Race) between-subjects multivariate analysis of covariance, controlling for familiarity with AI, for all outcome variables. There was a significant interaction found between Condition, Outcome and Race at (Pillai's  $V = 0.095$ ,  $F(9, 265) = 3.08$ ,  $p = .002$ , partial  $\eta^2 = 0.095$ ). A three-way interaction effect was significant for general procedural justice reactions,  $F(1, 273) = 3.89$ ,  $p < .001$ , partial  $\eta^2 = 0.045$ . A pairwise comparison of the marginal means indicates that Black applicants in the AI condition who were rejected reported more negative procedural justice reactions ( $M = 2.62$ ) than those who were rejected in the HR condition ( $M = 3.09$ ,  $p < .001$ ,  $d = 0.874$ ). A pairwise comparison also showed that Black participants that were accepted in the AI condition reported higher procedural justice reactions ( $M = 3.25$ ) than those who were accepted in the HR condition ( $M = 3.01$ ,  $p = .037$ ,  $d = 0.218$ ).

There was no significant effect of condition (AI vs HR) on general fairness reactions for White applicants for either outcome (acceptance vs rejection). In the AI condition White participants who were rejected had a mean of 3.00, whereas in the HR condition and rejected, they had a mean of 2.95 ( $p = .734$ ). White participants accepted by AI had a mean of 2.83, which was not significantly different from White participants accepted by HR ( $M = 3.05$ ,  $p = .157$ ). Overall,  $H4$  was partially supported, as the three-way interaction was only significant for general fairness perceptions. See Figs. 1 and 2 for an illustration of this interaction.

## 4. Exploratory analyses

### 4.1. Familiarity with AI

Although familiarity with AI was not a primary focus of our work, we explored the possibility that this variable will interact with our independent variables (i.e., AI vs HR evaluator; accepted vs rejected; Black vs White applicants). Familiarity with AI has been shown to interact with type of decision-maker in predicting attitudes towards organizations and decisions (Gonzalez et al., 2019a, 2022), necessitating that we test for any interactions that might qualify our main findings. As familiarity is a continuous measure, we conducted several regression models with familiarity entered as a main effect variable, along with our three original independent variables (outcome, race, and condition). We then modeled two-way interactions with each of these four variables, three-way interactions, and a four-way interaction between outcome, race, condition, and familiarity. We ran regressions for every dependent variable in our main analysis.

We found no evidence to support that familiarity with AI qualified any of our hypothesized relationships. Specifically, for all outcome variables, we did not find significant interactions between condition and

![](55d2bfe1c3d04e86df8d7a104d802172_img.jpg)

General Justice Reactions at Race=Black

Y-axis: General Reactions Marginal Means

X-axis: Condition (AI, HR)

Outcome: Rejection (dotted line), Acceptance (solid line)

Covariates appearing in the model are evaluated at the following values: CompositeFamiliarity = 2.4131

Fig. 1. General justice reactions by condition and outcome at Race = Black.

![](ac99eff233b8fe51d30f499e7413c345_img.jpg)

General Justice Reactions at Race=White

Y-axis: General Reactions Marginal Means

X-axis: Condition (AI, HR)

Outcome: Rejection (dotted line), Acceptance (solid line)

Covariates appearing in the model are evaluated at the following values: CompositeFamiliarity = 2.4131

Fig. 2. General justice reactions by condition and outcome at Race = White.

familiarity (which would have qualified H1 and H2), condition and outcome and familiarity (i.e., did not qualify H3), nor condition and outcome and race and familiarity (i.e., did not qualify H4). We did find a significant three-way interaction between condition, race, and familiarity for three of our outcome variables: general fairness reactions, reconsideration opportunity, and feedback (general fairness:  $b = -0.32$ ,  $p = .005$ ; feedback:  $b = -0.81$ ,  $p = .012$ ; reconsideration opportunity:  $b = -0.94$ ,  $p = .007$ ). For all three outcomes, simple slopes tests revealed that Black applicants reporting high familiarity (i.e., +1 sd above the mean) had significantly more positive perceptions of HR compared to AI (general fairness:  $b = 0.62$ ,  $p < .001$ ; feedback:  $b = 0.86$ ,  $p = .003$ ; reconsideration opportunity:  $b = 0.88$ ,  $p = .005$ ) and there was no relationship between condition (AI vs HR) and outcome for White applicants with high familiarity (general fairness:  $b = -0.34$ ,  $p = .068$ ; feedback:  $b = -0.11$ ,  $p = .723$ ; reconsideration opportunity:  $b = -0.39$ ,

$p = .286$ ). For feedback, White applicants with low familiarity also viewed AI more negatively ( $b = 0.63$ ,  $p = .037$ ). For general fairness reactions, Black applicants with low familiarity also viewed AI more negatively ( $b = 0.36$ ,  $p = .028$ ).

We also found a significant three-way interaction between outcome, race, and familiarity for opportunity to perform ( $b = -0.93$ ,  $p = .013$ ). Specifically, rejected Black applicants had more positive perceptions about the opportunity to perform than did accepted Black applicants ( $b = 0.856$ ,  $p = .005$ ). This relationship was not found for any other groups. Overall, exploratory results: 1) do not qualify our main results, 2) could suggest some awareness of potential for anti-Black bias in AI as highly familiar Black applicants perceive AI more negatively, and 3) should be interpreted very tentatively as they are post-hoc analyses. Figures for these significant interactions can be found in our supplemental file.

### 4.2. Open-ended comments

At the end of the experiment, we asked all participants two open-ended questions. The first open-ended question asked participants if they would choose to be evaluated by (AI vs HR) and why. The second question asked what factors led to the comfort or discomfort of participants in regards to being evaluated by an AI. To analyze the open-ended comments, we followed thematic analysis guidelines suggested by [Braun and Clarke \(2006\)](#). First, the lead author familiarized themselves with the data and assessed frequently used words and sentiments. The lead author then generated an initial coding scheme based on those observations and formed 6 content themes. Then, the lead researcher and three undergraduate research assistants coded the qualitative answers in terms of the extent to which each answer fit a given theme. Coders used a 5 point likert scale indicating the strength of that theme's presence in a given answer (ranging from non-existent, to highly present for each of the six themes). ICC values indicated sufficient levels of consistency across raters for most codes. Themes with low inter-rater reliability ( $>0.6$ ) were dropped from the list of codes. Of the remaining codes, ICC values ranged from 0.622 to 0.937, with an average ICC value of 0.772.

People overwhelmingly chose to be evaluated by human resources personnel (HR = 216, AI = 69). A chi-square test comparing choices of Black (22.41% chose AI, 77.59% chose HR) and White (25.77% chose AI, 74.23% chose HR) applicants did not find any significant differences ( $\chi^2(1, N = 271) = 0.3896; p = .532$ ). We organized their reasons around six themes that emerged from the data and also had sufficient inter-rater reliability: personhood (e.g., “a human can detect intangible qualities”), communication ability (e.g., “... AI cannot be manipulated or impressed”), bias (e.g., “... I think [AI] would be a lot more fair and unbiased”), and the formulaic/“techy” nature of AI (e.g., “... AI will make a calculated choice, but will be missing many factors”). There were no racial differences in the presence of any of these themes.

When asked what influenced how comfortable or not they were with AI evaluators specifically, six themes emerged with adequate inter-rater reliability: personhood (e.g., “... heart, determination, devotion, desire, conscientiousness are qualities that an AI may not recognize”), trust (e.g., “privacy would be of concern”), bias - both positive (e.g., “I know that AI will not have prejudice because of my age, gender, skin color, tattoos ...”) and negative (e.g., “a lot of AIs have a bias against people of color when using facial recognition software”), communication (e.g., “no way to sway an AI”), formulaic/“techy nature (e.g., “an AI only knows what it is programmed to do”), and technology progress (e.g., “I don’t think we are at a point that we can program an AI to make the intuitive leaps that a human is capable of”). There were two significant racial group differences. Black applicant responses were rated higher in discussing the (lack of) personhood ( $M = 2.25, SD = 1.71$ ) and the techy/formulaic nature of AI ( $M = 1.89, SD = 1.33$ ) than were White applicants ( $M = 1.89, SD = 1.45, p = .051; M = 1.50, SD = 1.18, p = .010$ ).

Overall, we see very few differences between Black and White applicants in terms of how they view AI and HR in general, which matches our main quantitative analyses that found no interactions of condition and race predicting fairness perceptions. Where we did find differences (i.e., personhood and “techy” perceptions), we see that Black applicants may be more likely to focus on ways that AI lacks nuance, although it is important to note that the means for these perceptions were low for both Black and White applicants, indicating these were not widespread reasons for discomfort with AI for either group.

## 5. Discussion

This study aimed to assess how applicants react to being evaluated by artificial intelligence (AI) versus human resource personnel (HR) using a realistic application scenario. Consistent with [Gonzalez, Capman, Martin, Justenhoven, and Preuss's \(2019a, April\)](#) study, we did not find significant main effects by condition on procedural justice but did find a main effect for interactional justice. This supports [Gonzalez's](#) reasoning

that AI takes away the interpersonal element from the application process more so than procedural justice elements.

However, our work moves beyond past research in that we found evaluation by AI can have negative effects on some dimensions of procedural fairness (chance to perform, reconsideration opportunity, and feedback) and general fairness reactions, but only when the individual is rejected by AI. Differences in perceptions of reconsideration opportunity and feedback perceptions suggest that those rejected by an AI see the procedure as lacking a way to communicate and that their chances of appealing the process and decision (reconsideration opportunity) are low. These findings underline the importance of using experimentation with personally-relevant outcomes when examining reactions to AI. Our results indicate that individuals are more likely to doubt the fairness of AI when AI evaluation results in a tangible loss.

This relates to past research that shows that outcome favorability moderates between justice rules (such as procedural justice) and the individual's reaction to a hiring decision ([Gilliland, 1993](#)). The reason why people have a particularly negative reaction to being rejected by AI (instead of HR) could be many. For one, there may be a self-serving bias where people attribute rejection to the inadequacies of the process instead of their own performance ([Ployhart and Harold, 2004; Ryan & Ployhart, 2000; Schinkel, van Vianen, A. E., & Ryan, 2016](#)). That is, people's sense-making mechanisms of negative outcomes come to play ([Brockner & Wiesenfeld, 1996](#)). Additionally, for events that are unexpected and negative (such as rejection) external cues and processes are weighed more heavily. [Gilliland \(1993\)](#) finds that when distributive rules are violated (negative outcomes), procedural justice rules have the biggest impact on fairness perceptions. However, this would not explain why we did not find the same effects of outcome favorability in the HR condition.

Another explanation for this finding could be that applicants have cognitive shortcuts or heuristics about justice perceptions in uncertain situations ([Croppanzano, Byrne, Bobocel, & Rupp, 2001; Lind et al., 2001](#)). Given the novelty and people's relative unfamiliarity towards AI (compared to human decision makers), this would likely heighten the feeling of uncertainty and thus their reliance on cognitive shortcuts which negatively impacts their fairness perceptions. ([McCarthy et al., 2017](#)). Thus, negative fairness perceptions were amplified for those who were in the AI condition and who were rejected.

Results from the study also indicated that there are some differences in fairness perception based on participant race. Specifically, for general fairness perceptions, only rejected Black participants showed more negative perceptions of AI versus HR. Put together with the finding that accepted Black applicants showed more positive perceptions of AI versus HR, we see some evidence for a heightened outcome favorability effect for Black participants being evaluated by AI. This seems to align with past findings that show a reputation for bias against a personally relevant group can make the outcome more important to perceptions of fairness ([Stähli, Vermunt, & Ellemers, 2008](#)). Although not widespread, we did see some qualitative comments that spoke to knowledge of AI racial bias (e.g., “AI is only as effective as who trained it or programmed it – accordingly there are lots of opportunities to introduce bias into an AI program without you realizing that is what you just did”). Our findings do not seem to support the idea that AI is generally not preferred by minority applicants because it is less personalized, as AI was preferred by Black applicants when there was a positive outcome. However, we encourage future research to explicitly test mechanisms underlying racial differences in reaction to AI selection decisions.

These findings have practical implications for companies considering using AI in selection. Negative applicant reactions lead to outcomes such as withdrawal from the selection process, which dampens the utility of the selection system and potentially curbs potential employees from joining the company ([Ryan et al., 2000](#)). Further, negative justice perceptions can result in the organization seeming unattractive to top candidates, decreased acceptance intentions, increased intentions to litigate, and actual legal actions ([Bauer et al., 2001; Geenen et al., 2012](#);

**Goldman, 2001**). Since managing a positive image is important for an organization during the selection process, our findings can assist HR managers in weighing the advantages and disadvantages of using computer-mediated selection systems versus traditional selection procedures. Particularly when considering the link between applicant reactions and legal challenges, it is important to consider any disparate impact of using AI in selection. If a selection system is desirable for certain features (e.g., cost-friendly, resource-efficient) but also shows a differential impact by race (particularly for those who are rejected and thus most likely to be aggrieved), then organizations would benefit from efforts to humanize the AI and/or educate their applicant pool on how the AI is used. Specifically, organizations should work on developing AI selection systems that can lessen this fairness perception gap between racial groups. It is also important to note that the potential for racial bias in AI is documented and has significant negative potential for minority applicants, warranting a response from organizations in terms of how they are using it, and how they are buffering against group-based biases.

Researchers have suggested solutions for those who see the benefit in AI, but worry about the potential downsides. **Burton, Stein, and Jensen's (2020)** suggest that a mix of a human-algorithm decision would outperform a single system decision. They suggest that this mix of human and AI decision making would mitigate mistrust and aversions to AI.

**Gonzalez et al. (2019b)** suggests that humans could be involved throughout the AI based selection process such as by emphasizing that humans' decision makers were involved during the training of the algorithms and reviewing process. The use of explanations could lessen the negative fairness perceptions (**Greenberg, 1990; Ployhart, Ehrhart, & Hayes, 2005**) by providing information that help guide applicants' attributions. Furthermore, supplying applicants with information regarding how AI collects and uses data could increase favorable perceptions (**Langer, König, & Fitili, 2018**).

To specifically address potential wariness toward AI by racial minority applicants, organizations can go beyond managing perceptions by using some of the suggestions above, and actually work toward reducing any potential biases in their algorithms. Strategies put forth for mitigating bias include evaluating input data for historical bias, incorporating unique and diverse cases into the data on which the AI is being trained, auditing your AI selection tool through external reviews, and having AI provide options to a human decision maker who makes the ultimate decision (**Diakopoulos & Friedler, 2016; Roselli, Matthews, & Talagala, 2019; Silberg & Manyika, 2019**). However, the use of any of these mitigation strategies could backfire (e.g., **Langer et al., 2018**), warranting future research examining high-fidelity hiring situations where individuals are rejected by AI evaluators systems under different mitigating conditions.

### 5.1. Limitations and future directions

This study examined the role of racial identity in reactions to AI selection systems, an important question for our modern diverse workforce that was lacking in the current literature. Moreover, our experimental paradigm had the advantage of testing applicants' reactions to AI in a higher fidelity context than past work using hypothetical vignettes. Hypothetical scenario studies may not "fully capture the elements of reality under study" (**Hughes & Huby, 2004**, p. 45). That is, there is a difference between how one "would ought to do" when imagining a scenario versus what one "would do" if they actually experienced it first-hand. Thus, by introducing real stakes, our findings contribute to a greater understanding of AI applicant perceptions.

These strengths notwithstanding, there are some limitations to the current study. For one, there was no direct manipulation of procedural justice rules (e.g., feedback, reconsideration opportunity). For example, the lack of substantial feedback provided to the participants may limit the study's ability to fully assess the effectiveness of the selection process. Without feedback, participants may not have the opportunity to

reflect on their performance and understand the areas in which they could improve. This could impact the validity of the study's results, as it may not accurately capture how participants would respond to an AI evaluator where greater feedback was present. Our study focused on perceptions that applicants have of AI and Human mediated selection systems, as perceptions can have a significant impact on applicants' attitudes and behaviors. We encourage direct manipulation of these dimensions in further studies to better understand how AI may be viewed differently with different levels of feedback, reconsideration opportunity, etc.

Secondly, the brief job posting used in the study does not reflect more in-depth job postings that are typical for certain positions. Further, in prioritizing keeping things brief for participant retention purposes, the application itself may not have allowed for the full range of impression management behaviors from applicants. This could limit the generalizability of the study's findings to other selection contexts, as participants may respond differently to a more detailed and lengthy job posting and application.

Thirdly, the study's context of applying for a HIT is distinct from a high stakes personnel selection context for a long-term position. The consequences of being rejected from a HIT is not as severe as being rejected from a job that has long-term career implications. Although this may limit the generalizability of the study's findings to other selection contexts, our findings do suggest that even in low-stakes selection, there are implications to fairness perceptions for being rejected, and specifically for being rejected by an AI evaluator.

Lastly, we did not provide a detailed explanation of how the AI and selection process work to the applicants. We told applicants in that condition that their application was being evaluated by Artificial Intelligence. It is possible that greater information and transparency about how the AI evaluator was making decisions would have minimized certain negative effects that we found or made them worse (see **Langer et al., 2018**). Future research that manipulates information given about the AI decision-making tool will elucidate this point.

In conclusion, this study provides evidence that applicants react negatively to being rejected by AI evaluators in a way that is not replicated with human evaluators. Further, we found that for general fairness reactions, Black applicants were uniquely and negatively affected by being rejected by an AI evaluator. We believe organizations need to carefully weigh the pros and cons of using AI and come up with ways to mitigate some of these negative perceptions (such as using mixed systems and using explanations).

## Credit author statement

Rewina Bedemariam: Conceptualization, Methodology, Investigation and Original draft preparation. Jennifer Wessel: Data Curation, Writing - Review & Editing and Supervision.

## Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## Data availability

Data will be made available on request.

## Appendix A. Supplementary data

Supplementary data to this article can be found online at <https://doi.org/10.1016/j.chb.2023.107869>.

## References

Acikgoz, Y., Davison, K. H., Compagnone, M., & Laske, M. (2020). Justice perceptions of artificial intelligence in selection. *International Journal of Selection and Assessment, 28* (4), 399–416.

Basch, J. M., & Melchers, K. G. (2019). Fair and flexible?! Explanations can improve applicant reactions toward asynchronous video interviews. *Personnel Assessment and Decisions, 5*(3), 2. <https://doi.org/10.25035/pad.2019.03.002>

Bauer, T. N., Maertz, C. P., Jr., Dolen, M. R., & Campion, M. A. (1998). Longitudinal assessment of applicant reactions to employment testing and test outcome feedback. *Journal of Applied Psychology, 83*(6), 892. <https://doi.org/10.1037/0021-9010.83.6.892>

Bauer, T. N., Truxillo, D. M., Paronto, M. E., Weekley, J. A., & Campion, M. A. (2004a). Applicant reactions to different selection technology: Face-to-face, interactive voice response, and computer-assisted telephone screening interviews. *International Journal of Selection and Assessment, 12*(1–2), 135–148.

Bauer, T. N., Truxillo, D. M., Paronto, M. E., Weekley, J. A., & Campion, M. A. (2004b). Applicant reactions to different selection technology: Face-to-face, interactive voice response, and computer-assisted telephone screening interviews. *International Journal of Selection and Assessment, 12*, 135e148. <https://doi.org/10.1111/j.10965-075X.2004.00269.x>

Bauer, T. N., Truxillo, D. M., Sanchez, R. J., Craig, J. M., Ferrara, P., & Campion, M. A. (2001). Applicant reactions to selection: Development of the selection procedural justice scale (SPJS). *Personnel Psychology, 54*(2), 387–419.

Bauer, T. N., Truxillo, D. M., Tucker, J. S., Weathers, V., Bertolino, M., Erdogan, B., et al. (2006). Selection in the information age: The impact of privacy concerns and computer experience on applicant reactions. *Journal of Management, 32*(5), 601–621. <https://doi.org/10.1177/0149206306289829>

Braun, V., & Clarke, V. (2006). Using thematic analysis in psychology. *Qualitative Research in Psychology, 3*(2), 77–101.

Brockner, J., & Wiesenfeld, B. M. (1996). An integrative framework for explaining reactions to decisions: Interactive effects of outcomes and procedures. *Psychological Bulletin, 120*(2), 189. <https://doi.org/10.1037/0033-2909.120.2.189>

Burton, J. W., Stein, M. K., & Jensen, T. B. (2020). A systematic review of algorithm aversion in augmented decision making. *Journal of Behavioral Decision Making, 33*(2), 220–239.

Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. *Science, 356*(6334), 183–186. <https://doi.org/10.1126/science.aal4230>

Chapman, D. S., Uggerslev, K. L., & Webster, J. (2003a). Applicant reactions to face-to-face and technology-mediated interviews: A field investigation. *Journal of Applied Psychology, 88*(5), 944. <https://doi.org/10.1037/0021-9010.88.5.944>

Chapman, D. S., Uggerslev, K. L., & Webster, J. (2003b). Applicant reactions to face-to-face and technology-mediated interviews: A field investigation. *Journal of Applied Psychology, 88*(5), 944. <https://doi.org/10.1037/0021-9010.88.5.944>

Cropanzano, R., Byrne, Z. S., Bobocel, D. R., & Rupp, D. E. (2001). Moral virtues, fairness, virtues, social entities, and other denizens of organizational justice. *Journal of Vocational Behavior, 58*(2), 164–209. <https://doi.org/10.1006/jvbe.2001.1791>

Derous, E., Born, M. P., & Witte, K. D. (2004). How applicants want and expect to be treated: Applicants' selection treatment beliefs and the development of the social process questionnaire on selection. *International Journal of Selection and Assessment, 12*(1–2), 99–119. <https://doi.org/10.1111/j.10965-075X.2004.00267.x>

Diakopoulos, N., & Friedler, S. (2016). How to hold algorithms accountable. Nov 17 MIT Technology Review, 17 (11). <https://www.technologyreview.com/2016/11/17/55957/>

Dineen, B. R., Noe, R. A., & Wang, C. (2004). Perceived fairness of web-based applicant screening procedures: Weighing the rules of justice and the role of individual differences. In *The university of Michigan and in alliance with the society of human resources management* (Vol. 43). Human Resource Management: Published in Cooperation with the School of Business Administration. <https://doi.org/10.1002/hrm.20011>, 2–3.

EEOC. (2023). releases new resource on Artificial Intelligence and title VII. US EEOC. (n. d. <https://www.eeoc.gov/newsroom/eeoc-releases-new-resource-artificial-intelligence-and-title-vii>

Geenen, B., Proost, K., Van Dijke, M., De Witte, K., & Von Grumbkow, J. (2012). The role of affect in the relationship between distributive justice expectations and applicants' recommendation and litigation intentions. *International Journal of Selection and Assessment, 20*(4), 404–413. <https://doi.org/10.1111/j.1203.2003.00405>

Gilliland, S. W. (1993). The perceived fairness of selection systems: An organizational justice perspective. *Academy of Management Review, 18*(4), 694–734. <https://doi.org/10.5465/amer.1993.9402210155>

Gilmore, D. C., & Ferris, G. R. (1989). The effects of applicant impression management tactics on interviewer judgments. *Journal of Management, 15*(4), 557–564. <https://doi.org/10.1177/014920638901550405>

Goldman, B. M. (2001). Toward an understanding of employment discrimination claiming: An integration of organizational justice and social information processing theories. *Personnel Psychology, 54*(2), 361–386.

Gonzalez, M. F., Capman, J. F., Martin, N. R., Justenhoven, R., & Preuss, A. (2019a). *Rage against the machine: Reactions to artificial intelligence in selection systems*. April.

National Harbor, MD: Poster presented at the 34th annual meeting of the Society for Industrial and Organizational Psychology.

Gonzalez, M. F., Capman, J. F., Oswald, F. L., Theyes, E. R., & Tomczak, D. L. (2019b). "Where's the IO?" Artificial intelligence and machine learning in talent management systems. *Personnel Assessment and Decisions, 5*(3), 5. <https://doi.org/10.25035/pad.2019.03.005>

Gonzalez, M. F., Liu, W., Shirase, L., Tomczak, D. L., Lobbe, C. E., Justenhoven, R., et al. (2022). Allaying with AI? Reactions toward human-based, AI/ML-based, and augmented hiring processes. *Computers in Human Behavior, 130*, 1–16.

Greenberg, J. (1990). Organizational justice: Yesterday, today, and tomorrow. *Journal of Management, 16*(2), 399–432.

Greenberg, J., & Cropanzano, R. (2001). *Advances in organizational justice*. Stanford, CA: Stanford University Press. ISBN: 9780804764582.

Hausknecht, J. P., Day, D. V., & Thomas, S. C. (2004). Applicant reactions to selection procedures: An updated model and meta-analysis. *Personnel Psychology, 57*(3), 639–683. <https://doi.org/10.1111/j.1744-6570.2004.00003.x>

Hiemstra, A. M., Derous, E., Serlie, A. W., & Born, M. P. (2012). Fairness perceptions of video resumes among ethnically diverse applicants. *International Journal of Selection and Assessment, 20*(4), 423–433.

Hughes, R., & Huby, M. (2004). The construction and interpretation of vignettes in social research. *Social Work and Social Sciences Review, 11*(1), 36–51.

Iddelinge, Van, C. H., Raymark, P. H., & Roth, P. L. (2005). Assessing personality with a structured employment interview: Construct-related validity and susceptibility to response inflation. *Journal of Applied Psychology, 90*(3), 536.

Johnson, T. L., & Johnson, N. N. (2023). Police facial recognition technology can't tell Black People Apart. May 18 *Scientific American*.

Johnson, D. G., & Verdichio, M. (2017). Reframing AI discourse. *Minds and Machines, 27* (4), 575–590. <https://doi.org/10.1007/s11023-017-9417-6>

Konradt, U., Warsza, T., & Ellwart, T. (2013). Fairness perceptions in web-based selection: Impact on applicants' pursuit intentions, recommendation intentions, and intentions to reapply. *International Journal of Selection and Assessment, 21*(2), 155–169. <https://doi.org/10.1111/j.1203.2003.00405>

Langer, M., König, C. J., & Fitill, A. (2018). Information as a double-edged sword: The role of computer experience and information on applicant reactions towards novel technologies for personnel selection. *Computers in Human Behavior, 81*, 19–30.

Lee, M. K. (2018). Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management. *Big Data & Society, 5*(1), Article 2053951718756684. <https://doi.org/10.1177/2053951718756684>

Lind, E. A. (1992). The fairness heuristic: Rationality and 'relativity' in procedural evaluations. March, Irvine, CA: Paper presented at the Fourth International Conference of the Society for the Advancement of Socio-Economics.

Lind, E. A., Kray, L., & Thompson, L. (2001). Primacy effects in justice judgments: Testing predictions from fairness heuristic theory. *Organizational Behavior and Human Decision Processes, 85*(2), 189–210. <https://doi.org/10.1006/obhd.2000.2937>

Mann, G., & O'Neil, C. (2016). Hiring algorithms are not neutral. Retrieved June 24, 2022, from *Harvard Business Review*, 9. <https://hbr.org/2016/12/hiring-algorithms-are-not-neutral>

McCarthy, J. M., Bauer, T. N., Truxillo, D. M., Anderson, N. R., Costa, A. C., & Ahmed, S. M. (2017). Applicant perspectives during selection: A review addressing "so what?" "What's new?", and "where to next?". *Journal of Management, 43*(6), 1693–1725. <https://doi.org/10.1111/j.1744-6570.2016.00618.x>

Miller, C. T., & Kaiser, C. R. (2001). A theoretical perspective on coping with stigma. *Journal of Social Issues, 57*(1), 73–92.

Morgan, L. M. (2002). The nature, antecedents, and consequences of social identity-based impression management: Uncovering strategies for professional image construction in cases of negative stereotyping. *Unpublished doctoral dissertation*. Ann Arbor, MI: University of Michigan.

Noble, S. M., Foster, L. L., & Craig, S. B. (2021). The procedural and interpersonal justice of automated application and resume screening. *International Journal of Selection and Assessment, 29*(2), 139–153.

Ployhart, R. E., Ehrhart, K. H., & Hayes, S. C. (2005). Using attributions to understand the effects of explanations on applicant reactions: Are reactions consistent with the covariation principle? 1. *Journal of Applied Social Psychology, 35*(2), 259–296.

Ployhart, R. E., & Harold, C. M. (2004). The applicant attribution-reaction theory (AART): An integrative theory of applicant attributional processing. *International Journal of Selection and Assessment, 12*(1–2), 84–98. <https://doi.org/10.1111/j.10965-075X.2004.00266.x>

Roberts, L. M. (2005). Changing faces: Professional image construction in diverse organizational settings. *Academy of Management Review, 30*(4), 685–711.

Roberts, L. M., Settles, I. H., & Jellison, W. A. (2008). Predicting the strategic identity management of gender and race. *Identity: An International Journal of Theory and Research, 8*(4), 269–306. <https://doi.org/10.1080/152834080082365270>

Roselli, D., Matthews, J., & Talagala, N. (2019). Managing bias in AI. May in *Companion proceedings of the 2019 World Wide Web conference* (pp. 539–544).

Ryan, A. M., & Ployhart, R. E. (2000). Applicants' perceptions of selection procedures and decisions: A critical review and agenda for the future. *Journal of Management, 26* (3), 565–566. <https://doi.org/10.1177/014920630002600308>

Ryan, A. M., Sacco, J. M., McFarland, L. A., & Kriska, S. D. (2000). Applicant self-selection: Correlates of withdrawal from a multiple hurdle process. *Journal of Applied Psychology, 85*, 163–179. <https://doi.org/10.1037/0021-9010.85.2.163>

Ryan, A. M., Sacco, J. M., McFarland, L. A., & Kriska, S. D. (2000). Applicant self-selection: Correlates of withdrawal from a multiple hurdle process. *Journal of Applied Psychology, 85*, 163.

Schinkel, S., van Vianen, A. E., & Ryan, A. M. (2016). Applicant reactions to selection events: Four studies into the role of attributional style and fairness perceptions. *International Journal of Selection and Assessment, 24*(2), 107–118. <https://doi.org/10.1111/j.1203.2003.00405>

Schinkel, S., van Vianen, A. E., & Dierendonck, D. (2013). Selection fairness and outcomes: A field study of interactive effects on applicant reactions. *International Journal of Selection and Assessment, 21*(1), 22–31. <https://doi.org/10.1111/j.1203.2003.00405>

Schleicher, D. J., Venkataramani, V., Morgeson, F. P., & Campion, M. A. (2006). So you didn't get the job... now what do you think? Examining opportunity-to-perform

fairness perceptions. *Personnel Psychology*, 59(3), 559–590. <https://doi.org/10.1111/j.1744-6570.2006.00047.x>

Silberg, J., & Manyika, J. (2019). Notes from the AI frontier: Tackling bias in AI (and in humans). *McKinsey Global Institute*, 1(6).

Stähl, T., Vermunt, R., & Ellemers, N. (2008). Reactions to outgroup authorities' decisions: The role of expected bias, procedural fairness and outcome favorability. *Group Processes & Intergroup Relations*, 11(3), 281–299.

Stephan, M., Brown, D., & Erickson, R. (2017). Talent acquisition: Enter the cognitive recruiter. Rewriting the rules for the digital age, eds. B. Pelster & J. Schwartz.

Stone, D. L., Deadrick, D. L., Lukaszewski, K. M., & Johnson, R. (2015). The influence of technology on the future of human resource management. *Human Resource Management Review*, 25(2), 216–231. <https://doi.org/10.1016/j.hrmr.2015.01.002>

Truxillo, D. M., Bauer, T. N., Campion, M. A., & Paronto, M. E. (2002). Selection fairness information and applicant reactions: A longitudinal field study. *Journal of Applied Psychology*, 87(6), 1020–1031.

Verma, P. (2022). These robots were trained on AI. They became racist and sexist. The Washington Post. July 16 <https://www.washingtonpost.com/technology/2022/07/16/racist-robots-ai/>.

Walker, H. J., Bauer, T. N., Cole, M. S., Bernerth, J. B., Feild, H. S., & Short, J. C. (2013). Is this how I will be treated? Reducing uncertainty through recruitment interactions. *Academy of Management Journal*, 56(5), 1325–1347.

Walker, H. J., Helmuth, C. A., Feild, H. S., & Bauer, T. N. (2015). Watch what you say: Job applicants' justice perceptions from initial organizational correspondence. *Human Resource Management*, 54(6), 999–1011.

Wolf, Z. B. (2023). AI can be racist, sexist and creepy. What should we do about it? | CNN politics. CNN. March 18 <https://www.cnn.com/2023/03/18/politics/ai-chatgpt-rcist-what-matters/index.htm>.